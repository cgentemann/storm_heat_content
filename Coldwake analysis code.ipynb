{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting and data analysis for global cold wakes\n",
    "#from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input directories\n",
    "dir_storm_info='f:/data/tc_wakes/database/info/'\n",
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "dir_flux = 'F:/data/model_data/oaflux/data_v3/daily/turbulence/'\n",
    "dir_cmc = 'F:/data/sst/cmc/CMC0.2deg/v2/'\n",
    "dir_ccmp='F:/data/sat_data/ccmp/v02.0/Y'\n",
    "##where to get the data through opendap, use these directories instead\n",
    "#dir_cmc = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/CMC/CMC0.1deg/v3/'\n",
    "#dir_flux = 'http://apdrc.soest.hawaii.edu:80/dods/public_data/WHOI_OAFlux/version3/daily/lh_oaflux/'\n",
    "#the latest ccmp is from www.remss.com but they do not have an opendap server so you can use this instead:\n",
    "#dir_ccmp='https://podaac-opendap.jpl.nasa.gov/opendap/allData/ccmp/L3.0/flk/'\n",
    "\n",
    "#################################################################################\n",
    "import geopy.distance\n",
    "from math import sin, pi\n",
    "from scipy import interpolate\n",
    "\n",
    "#functions for running storm data\n",
    "import sys\n",
    "#sys.path.append('C:/Users/gentemann/Google Drive/d_drive/python/storm_heat_content/subroutines/')\n",
    "#from storm_masking_routines import interpolate_storm_path\n",
    "#from storm_masking_routines import get_dist_grid\n",
    "#from storm_masking_routines import closest_dist\n",
    "#from storm_masking_routines import calculate_storm_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plot output data and mask\n",
    "\n",
    "import os\n",
    "dir_out_figures = 'F:/data/tc_wakes/database/figs/check_storm_dist/'\n",
    "iyr_storm,inum_storm = 2003,27\n",
    "for inum_storm in range(1,58): #100):\n",
    "    filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "    exists = os.path.isfile(filename)\n",
    "    if not exists:\n",
    "        continue\n",
    "    print(filename)\n",
    "    ds_storm=xr.open_dataset(filename)\n",
    "    ds_storm = ds_storm.sel(j2=0)\n",
    "    ds_storm.close()\n",
    "#    ds_storm['lon'] = (ds_storm.lon + 180) % 360 - 180\n",
    "    filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "    ds_all = xr.open_dataset(filename)\n",
    "    ds_all.close()\n",
    "    #ds_all = ds_all.sortby('lon')\n",
    "    print(ds_all.lon[0].values,ds_all.lon[-1].values)\n",
    "    if abs(ds_all.lon[-1]-ds_all.lon[0])>180:\n",
    "        ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "        #ds_all = ds_all.sortby(ds_all.lon)\n",
    "        ds_storm['lon'] = np.mod(ds_storm['lon'], 360)\n",
    "        #ds_storm = ds_storm.sortby(ds_storm.lon)\n",
    "    plt.figure(figsize=(15,3.5))\n",
    "    plt.subplot(131)\n",
    "    ds_all.side_of_storm.plot()\n",
    "    plt.plot(ds_storm.lon,ds_storm.lat,'w')\n",
    "    plt.plot(ds_storm.lon[0],ds_storm.lat[0],'w*')\n",
    "    plt.subplot(132)\n",
    "    ds_all.dist_from_storm_km.plot(vmin=0,vmax=3000)\n",
    "    plt.plot(ds_storm.lon,ds_storm.lat,'w')\n",
    "    max_lat = ds_storm.lat.max()\n",
    "    if max_lat<0:\n",
    "        cond = ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm<=0)) |  ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm>0))\n",
    "    else:\n",
    "        cond = ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm<0)) |  ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm>=0))\n",
    "    subset = ds_all.where(cond)\n",
    "    plt.subplot(133)\n",
    "    subset.dist_from_storm_km.plot(vmin=0,vmax=3000)\n",
    "    plt.plot(ds_storm.lon,ds_storm.lat,'w')\n",
    "    filename = dir_out_figures + str(iyr_storm) + str(inum_storm).zfill(3) + '_interpolated_track.png'\n",
    "    plt.savefig(filename,dpi=100,bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_lats=np.arange(-90,90,.25)\n",
    "map_lats[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "#trying to rewrite so faster using numpy historgram\n",
    "#plotting and data analysis for global cold wakes\n",
    "#from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import geopy.distance\n",
    "from math import sin, pi\n",
    "from scipy import interpolate\n",
    "from scipy import stats\n",
    "\n",
    "#functions for running storm data\n",
    "import sys\n",
    "\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input directories\n",
    "dir_storm_info='f:/data/tc_wakes/database/info/'\n",
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "#################################################################################\n",
    "\n",
    "#start to look at data and make some pdfs\n",
    "\n",
    "date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "\n",
    "map_lats=np.arange(-90,90,.25)\n",
    "map_lons=np.arange(-180,180,.25)\n",
    "imap_lats = map_lats.size\n",
    "imap_lons = map_lons.size\n",
    "  \n",
    "for iyr_storm in range(2002,2017):\n",
    "    #init arrays\n",
    "    init_data=0\n",
    "    map_sum,map_cnt,map_max = np.zeros([imap_lats,imap_lons]),np.zeros([imap_lats,imap_lons]),np.zeros([imap_lats,imap_lons])\n",
    "    for inum_storm in range(0,100): #(0,100): #100):\n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "        exists = os.path.isfile(filename)\n",
    "        if not exists:\n",
    "            continue\n",
    "        print(filename)\n",
    "        ds_storm_info=xr.open_dataset(filename)\n",
    "        ds_storm_info = ds_storm_info.sel(j2=0)\n",
    "        ds_storm_info.close()\n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "        ds_all = xr.open_dataset(filename)\n",
    "        ds_all['spd']=np.sqrt(ds_all.uwnd**2+ds_all.vwnd**2)\n",
    "        ds_all.close()\n",
    "        if abs(ds_all.lon[-1]-ds_all.lon[0])>180:\n",
    "            ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "            ds_storm_info['lon'] = np.mod(ds_storm_info['lon'], 360)\n",
    "        max_lat = ds_storm_info.lat.max()\n",
    "        #remove all data outsice 100km/800km or cold wake >0 or <-10\n",
    "        if max_lat<0:\n",
    "            cond = ((((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm<=0)) | \n",
    "            ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm>0))) \n",
    "            & (ds_all.coldwake_max<=-.1) & (ds_all.coldwake_max>=-10))\n",
    "        else:\n",
    "            cond = ((((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm<0)) | \n",
    "            ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm>=0))) \n",
    "            & (ds_all.coldwake_max<=-.1) & (ds_all.coldwake_max>=-10))\n",
    "        subset = ds_all.where(cond)\n",
    "\n",
    "        #create coldwake anomaly with nan for all values before wmo storm time\n",
    "        subset['sst_anomaly']=subset.analysed_sst-subset.sst_prestorm\n",
    "    #    for i in range(0,xdim):\n",
    "    #        for j in range (0,ydim):\n",
    "    #            if np.isnan(subset.closest_storm_index[j,i]):\n",
    "    #                continue\n",
    "    #            iend = subset.closest_storm_index[j,i].data.astype(int)\n",
    "    #            subset.sst_anomaly[:iend,j,i]=np.nan\n",
    "\n",
    "        #create array with day.frac since closest storm passage\n",
    "        tdif_dy = (subset.time-subset.closest_storm_time_np64)/np.timedelta64(1, 'D')\n",
    "    #    tdif_dy = tdif_dy.where(tdif_dy>=0,np.nan)\n",
    "        subset['tdif_dy']=tdif_dy\n",
    "\n",
    "        xdim,ydim,tdim = ds_all.lon.shape[0],ds_all.lat.shape[0],ds_all.time.shape[0]\n",
    "\n",
    "        pdim=xdim*ydim\n",
    "        pdim3=tdim*xdim*ydim\n",
    "        print(xdim*ydim)\n",
    "#        data = subset.coldwake_max\n",
    "#        cbin1 = np.arange(-10, 0, 0.1)  #cold wake bins\n",
    "#        bins=cbin1\n",
    "#        hist1,mids = np.histogram(data,bins)[0],0.5*(bins[1:]+bins[:-1])\n",
    "#        sum1 = np.cumsum(mids*hist1) \n",
    "\n",
    "        cbin1 = np.arange(-10, 0, 0.1)  #cold wake bins\n",
    "        bins=cbin1\n",
    "        x= np.reshape(subset.coldwake_max.data,(pdim))\n",
    "        v = np.reshape(subset.coldwake_max.data,(pdim))\n",
    "        hist1,mids1=stats.binned_statistic(x,v,'count', bins)[0],0.5*(bins[1:]+bins[:-1])\n",
    "        sum1=stats.binned_statistic(x,v, 'sum', bins)[0]\n",
    "\n",
    "        cbin2 = np.arange(0,8)  #day to max, plot histogram of when cold wake max happens\n",
    "        bins=cbin2\n",
    "        x= np.reshape(subset.coldwake_hrtomaxcold.data/24,(pdim))\n",
    "        v = np.reshape(subset.coldwake_max.data,(pdim))\n",
    "        hist2,mids2=stats.binned_statistic(x,v,'count', bins)[0],0.5*(bins[1:]+bins[:-1])\n",
    "        sum2=stats.binned_statistic(x,v, 'sum', bins)[0]\n",
    "\n",
    "        cbin3 = np.arange(0,50)  #dy to recovery\n",
    "        bins=cbin3\n",
    "        x= np.reshape(subset.coldwake_dytorecovery.data,(pdim))\n",
    "        v = np.reshape(subset.coldwake_max.data,(pdim))\n",
    "        hist3,mids3=stats.binned_statistic(x,v,'count', bins)[0],0.5*(bins[1:]+bins[:-1])\n",
    "        sum3=stats.binned_statistic(x,v, 'sum', bins)[0]\n",
    "\n",
    "        cbin4 = np.arange(0,600,3)  #max cold wake as function of MLD at start of storm\n",
    "        bins=cbin4\n",
    "        x= np.reshape(subset.dbss_obml[0,:,:].data,(pdim))\n",
    "        v = np.reshape(subset.coldwake_max.data,(pdim))\n",
    "        hist4,mids4=stats.binned_statistic(x,v,'count', bins)[0],0.5*(bins[1:]+bins[:-1])\n",
    "        sum4=stats.binned_statistic(x,v, 'sum', bins)[0]\n",
    "\n",
    "        cbin5 = np.arange(0,200)  #max cold wake as function of wmo max storm wind speed\n",
    "        bins=cbin5\n",
    "        x= np.reshape(subset.wmo_storm_wind.data,(pdim))\n",
    "        v = np.reshape(subset.coldwake_max.data,(pdim))\n",
    "        hist5,mids5=stats.binned_statistic(x,v,'count', bins)[0],0.5*(bins[1:]+bins[:-1])\n",
    "        sum5=stats.binned_statistic(x,v, 'sum', bins)[0]\n",
    "\n",
    "        cbin6 = np.arange(0,400,2)  #max cold wake as function of wmo max storm translation speed\n",
    "        bins=cbin6\n",
    "        x= np.reshape(subset.wmo_storm_speed.data,(pdim))\n",
    "        v = np.reshape(subset.coldwake_max.data,(pdim))\n",
    "        hist6,mids6=stats.binned_statistic(x,v,'count', bins)[0],0.5*(bins[1:]+bins[:-1])\n",
    "        sum6=stats.binned_statistic(x,v, 'sum', bins)[0]\n",
    "\n",
    "        cbin7 = np.arange(-10,50,1)  #cold wake recovery as function of time\n",
    "        bins = cbin7\n",
    "        x= np.reshape(subset.tdif_dy.data,(pdim3))\n",
    "        v = np.reshape(subset.sst_anomaly.data,(pdim3))\n",
    "        hist7,mids7=stats.binned_statistic(x,v,'count', bins)[0],0.5*(bins[1:]+bins[:-1])\n",
    "        sum7=stats.binned_statistic(x,v, 'sum', bins)[0]\n",
    "\n",
    "        x1= np.reshape(subset.wmo_storm_speed.data,(pdim))\n",
    "        x3= np.reshape(subset.wmo_storm_wind.data,(pdim))\n",
    "        b1= cbin6 #np.arange(0,200,1)\n",
    "        b3= cbin5 #np.arange(0,200,1)\n",
    "        dbins=np.vstack((b1,b3)).T\n",
    "        v = np.reshape(subset.coldwake_max.data,(pdim))\n",
    "        hist8=stats.binned_statistic_2d(x1,x3,v,'count', bins=dbins.T)[0]\n",
    "        sum8=stats.binned_statistic_2d(x1,x3,v, 'sum', bins=dbins.T)[0]\n",
    "\n",
    "        x1= np.reshape(subset.wmo_storm_speed.data,(pdim))\n",
    "        x2= np.reshape(subset.dbss_obml[0,:,:].data,(pdim))\n",
    "        x3= np.reshape(subset.wmo_storm_wind.data,(pdim))\n",
    "        x=np.vstack((x1,x2,x3))\n",
    "        b1= cbin6 #np.arange(0,200,1)\n",
    "        b2= cbin4 #np.arange(0,600,3)\n",
    "        b3= cbin5 #np.arange(0,200,1)\n",
    "        dbins=np.vstack((b1,b2,b3)).T\n",
    "        v = np.reshape(subset.coldwake_max.data,(pdim))\n",
    "        hist9=stats.binned_statistic_dd(x.T,v,'count', bins=dbins.T)[0]\n",
    "        sum9=stats.binned_statistic_dd(x.T,v, 'sum', bins=dbins.T)[0]    \n",
    "\n",
    "        if init_data == 0:\n",
    "            sv_sum1,sv_cnt1,sv_bin1 = sum1,hist1,cbin1\n",
    "            sv_sum2,sv_cnt2,sv_bin2 = sum2,hist2,cbin2\n",
    "            sv_sum3,sv_cnt3,sv_bin3 = sum3,hist3,cbin3\n",
    "            sv_sum4,sv_cnt4,sv_bin4 = sum4,hist4,cbin4\n",
    "            sv_sum5,sv_cnt5,sv_bin5 = sum5,hist5,cbin5\n",
    "            sv_sum6,sv_cnt6,sv_bin6 = sum6,hist6,cbin6\n",
    "            sv_sum7,sv_cnt7,sv_bin7 = sum7,hist7,cbin7\n",
    "            sv_sum8,sv_cnt8 = sum8,hist8\n",
    "            sv_sum9,sv_cnt9 = sum9,hist9\n",
    "            init_data=1\n",
    "        else:\n",
    "            sv_sum1+= sum1\n",
    "            sv_cnt1+= hist1\n",
    "            sv_sum2+= sum2\n",
    "            sv_cnt2+= hist2\n",
    "            sv_sum3+= sum3\n",
    "            sv_cnt3+= hist3\n",
    "            sv_sum4+= sum4\n",
    "            sv_cnt4+= hist4\n",
    "            sv_sum5+= sum5\n",
    "            sv_cnt5+= hist5      \n",
    "            sv_sum6+= sum6\n",
    "            sv_cnt6+= hist6           \n",
    "            sv_sum7+= sum7\n",
    "            sv_cnt7+= hist7           \n",
    "            sv_sum8+= sum8\n",
    "            sv_cnt8+= hist8           \n",
    "            sv_sum9+= sum9\n",
    "            sv_cnt9+= hist9           \n",
    "\n",
    "        #put on global map\n",
    "        tem = subset.coldwake_max.interp(lat=map_lats,lon=map_lons)\n",
    "        tem=tem.fillna(0)\n",
    "        temc=(tem/tem).fillna(0)\n",
    "        map_sum+=tem\n",
    "        map_cnt+=temc\n",
    "        map_max=np.where(tem.data < map_max, tem,map_max)  #where tem<max put tem value in otherwise leave max\n",
    "\n",
    "    m1=xr.DataArray(map_sum, coords={'lat': map_lats, 'lon':map_lons}, dims=('lat', 'lon'))        \n",
    "    m2=xr.DataArray(map_cnt, coords={'lat': map_lats, 'lon':map_lons}, dims=('lat', 'lon'))        \n",
    "    m3=xr.DataArray(map_max, coords={'lat': map_lats, 'lon':map_lons}, dims=('lat', 'lon'))        \n",
    "    ds=xr.Dataset(data_vars={'sum1': (('coldw'),sv_sum1),\n",
    "                             'cnt1': (('coldw'),sv_cnt1),\n",
    "                             'sum2': (('dymax'),sv_sum2),\n",
    "                             'cnt2': (('dymax'),sv_cnt2),\n",
    "                             'sum3': (('dyrec'),sv_sum3),\n",
    "                             'cnt3': (('dyrec'),sv_cnt3),\n",
    "                             'sum4': (('mld'),sv_sum4),\n",
    "                             'cnt4': (('mld'),sv_cnt4),\n",
    "                             'sum5': (('wnd'),sv_sum5),\n",
    "                             'cnt5': (('wnd'),sv_cnt5),\n",
    "                             'sum6': (('tspd'),sv_sum6),\n",
    "                             'cnt6': (('tspd'),sv_cnt6),\n",
    "                             'sum7': (('dtime'),sv_sum7),\n",
    "                             'cnt7': (('dtime'),sv_cnt7),\n",
    "                             'sum8': (('tspd','wnd'),sv_sum8),\n",
    "                             'cnt8': (('tspd','wnd'),sv_cnt8),\n",
    "                             'sum9': (('tspd','mld','wnd'),sv_sum9),\n",
    "                             'cnt9': (('tspd','mld','wnd'),sv_cnt9),\n",
    "                                 'map_sum': (('lat','lon'),m1),\n",
    "                                'map_cnt': (('lat','lon'),m2),\n",
    "                                'map_max': (('lat','lon'),m3)\n",
    "                               },\n",
    "                                 coords={'coldw':cbin1[0:-1],\n",
    "                                         'dymax':cbin2[0:-1],\n",
    "                                         'dyrec':cbin3[0:-1],\n",
    "                                         'mld':cbin4[0:-1],\n",
    "                                         'wnd':cbin5[0:-1],\n",
    "                                         'tspd':cbin6[0:-1],\n",
    "                                         'dtime':cbin7[0:-1],\n",
    "                                         'lat':map_lats,'lon':map_lons})\n",
    "\n",
    "    filename='f:/data/tc_wakes/database/results/hist_sum_'+str(iyr_storm)+'.nc'\n",
    "    ds.to_netcdf(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for iyr_storm in range(2002,2017):\n",
    "    #print(iyr_storm)\n",
    "    filename='f:/data/tc_wakes/database/results/hist_sum_'+str(iyr_storm)+'.nc'\n",
    "    ds=xr.open_dataset(filename)\n",
    "    ds.close()\n",
    "    if iyr_storm == 2002:\n",
    "        ds_all = ds\n",
    "    else:\n",
    "        ds_all+= ds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_figs = 'f:/data/tc_wakes/database/figs/hist/'\n",
    "plt.figure(1, figsize=(12, 2))\n",
    "plt.subplot(121)\n",
    "plt.plot(cbin1[0:-1],ds_all.cnt1)\n",
    "plt.xlabel('Cold wake maximum (K)')\n",
    "plt.ylabel('Number of occurances')\n",
    "plt.subplot(122)\n",
    "plt.plot(cbin2[0:-1],ds_all.cnt2/10000)\n",
    "plt.xlabel('Post-storm day of cold wake maximum (dys)')\n",
    "plt.ylabel('Count (10,000s)')\n",
    "plt.savefig(dir_figs+'coldwake_max.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_figs = 'f:/data/tc_wakes/database/figs/hist/'\n",
    "plt.figure(1, figsize=(12, 2))\n",
    "plt.subplot(121)\n",
    "plt.plot(cbin3[0:-1],ds_all.sum3/ds_all.cnt3)\n",
    "plt.xlabel('Days until SST recovery')\n",
    "plt.ylabel('$\\Delta$SST (K)')\n",
    "plt.subplot(122)\n",
    "plt.plot(cbin4[0:-1],ds_all.sum4/ds_all.cnt4)\n",
    "plt.xlabel('MLD (m)')\n",
    "plt.ylabel('$\\Delta$SST (K)')\n",
    "plt.savefig(dir_figs+'coldwake_max.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_all.sum6/ds_all.cnt6).plot()  #storm translational speed versus cold wake anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_all.sum7/ds_all.cnt7).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_all.sum8/ds_all.cnt8).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_all.sum9/ds_all.cnt9).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.pcolor(bins1,bins2,H.transpose())\n",
    "plt.grid()\n",
    "plt.xlabel('v1')\n",
    "plt.ylabel('v2')\n",
    "plt.colorbar()\n",
    "plt.savefig(dir_figs+'coldwake_max.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "        #start saving this data\n",
    "        \n",
    "    for i in range(0,xdim):\n",
    "        for j in range(0,ydim):\n",
    "            if np.isnan(subset.analysed_sst[0,j,i]):  #don't process masked values\n",
    "                continue\n",
    "            storm_date64 = np.datetime64(date_1858 + dt.timedelta(days=float(ds_all.closest_storm_time[j,i])))\n",
    "            time_diff = subset.time-storm_date64\n",
    "            storm_index = np.argmin(abs(time_diff)).data\n",
    "\n",
    "#            map_sum[j,i]+=subset.coldwake_max[j,i]\n",
    "#            map_cnt[j,i]+=1\n",
    "#            if  map_max[j,i]>subset.coldwake_max[j,i]:\n",
    "#                map_max[j,i]=subset.coldwake_max[j,i]\n",
    "            \n",
    "            if subset.coldwake_max[j,i]<-0.1:  #cold wake larger than -0.1 \n",
    "                icold=int(np.round(subset.coldwake_max[j,i].data*10.))+10\n",
    "                icoldhr=int(np.round(subset.coldwake_hrtomaxcold[j,i].data))\n",
    "                icolddy2=int(np.round(subset.coldwake_hrtorecovery[j,i].data/24.))\n",
    "                iswnd=int(np.round(subset.wmo_storm_wind[j,i].data*.5))\n",
    "                iwnd=int(np.round(subset.spd[storm_index,j,i].data))\n",
    "                isspd=int(abs(np.round(subset.wmo_storm_speed[j,i].data)))\n",
    "\n",
    "            #PROBLEM MLD interpolation pre 1/1 for each year need to fix\n",
    "            #for now just setting first data to 1/1 for each year\n",
    "                im=0\n",
    "                if np.isnan(subset.dbss_obml[im,j,i].data):\n",
    "                    while np.isnan(subset.dbss_obml[im,j,i].data):\n",
    "                        im+=1\n",
    "                        if im>=tdim:\n",
    "                            break\n",
    "                if im>=tdim:\n",
    "                    continue\n",
    "                if np.isnan(subset.dbss_obml[im,j,i].data):\n",
    "                    continue\n",
    "                imld=int(np.round(subset.dbss_obml[im,j,i].data))\n",
    "\n",
    "                if (icold<0) | (icold>100):  #cold wake max between 0,10 K\n",
    "                    continue\n",
    "                if (icold>=0) & (icold<=100) & (isspd>=0) & (isspd<=200) & (imld>=0) & (imld<=100):                \n",
    "                    if (iswnd>=0) & (iswnd<=100):             \n",
    "                        sum1[icold,iswnd,isspd,imld]=sum1[icold,iswnd,isspd,imld]+subset.coldwake_max[j,i].data  #uses storm wind from wmo track\n",
    "                        num1[icold,iswnd,isspd,imld]=num1[icold,iswnd,isspd,imld]+1  #uses storm wind from wmo track\n",
    "                    if (iwnd>=0) & (iwnd<=100):                  \n",
    "                        sum2[icold,iwnd,isspd,imld]=sum2[icold,iwnd,isspd,imld]+subset.coldwake_max[j,i].data    #uses collocated ccmp wind on storm day\n",
    "                        num2[icold,iwnd,isspd,imld]=num2[icold,iwnd,isspd,imld]+1    #uses collocated ccmp wind on storm day\n",
    " #               if (icoldhr>=0) & (icoldhr<100):\n",
    " #                   num3[icoldhr]=num3[icoldhr]+1   #pdf of when max cold wake occurs\n",
    " #               if (icolddy2>=0) & (icolddy2<100):\n",
    " #                   num4[icolddy2]=num4[icolddy2]+1   #pdf of when recovery occurs\n",
    "    #            num4[icold] = num4[icold]+1                     \n",
    " #               sum5[icold]=sum5[icold]+subset.coldwake_hrtorecovery[j,i].data  #mean of recorvery time as f(max coldwake)\n",
    " #               num5[icold]=num5[icold]+1  #mean of recorvery time as f(max coldwake)\n",
    " #               if (icoldhr>=0) & (icoldhr<100):\n",
    " #                   sum6[icoldhr]=sum6[icoldhr]+subset.coldwake_max[j,i].data  #mean of max cold wake as f(hr from storm)\n",
    " #                   num6[icoldhr]=num6[icoldhr]+1  #mean of max cold wake as f(hr from storm)\n",
    "\n",
    "                #caluclate the mean sst anomaly as f(days after storm)\n",
    " #               for k in range(storm_index,tdim):\n",
    " #                   data_time = subset.time[k]\n",
    " #                   dtime = data_time-storm_date64\n",
    " #                   ddy = dtime / np.timedelta64(1,'D')\n",
    " #                   iddy = int(np.round(ddy))\n",
    " #                   if (iddy<0) | (iddy>50):\n",
    " #                       continue\n",
    " #                   change_in_sst = subset.sst_prestorm[j,i].data-subset.analysed_sst[k,j,i].data\n",
    " #                   sum7[iddy]=sum7[iddy]+change_in_sst  \n",
    " #                   num7[iddy]=num7[iddy]+1  \n",
    "\n",
    "    for i in range(0,100):\n",
    "        dim1[i]=(i-10)/10\n",
    "        dim2[i]=i\n",
    "        dim3[i]=i*24\n",
    "        dim4[i]=i/.5\n",
    "        dim5[i]=i\n",
    "        dim7[i]=i\n",
    "        dim8[i]=i\n",
    "    for i in range(0,200):\n",
    "        dim6[i]=i\n",
    "#                                'num3': (('coldhr'),num3),\n",
    "#                                'num4': (('colddy'),num4),\n",
    "#                                'sum5': (('cold'),sum5),\n",
    "#                                'num5': (('cold'),num5),\n",
    "#                                'sum6': (('coldhr'),sum6),\n",
    "#                                'num6': (('coldhr'),num6),\n",
    "#                               'sum7': (('dyfrom'),sum7),\n",
    "#                                'num7': (('dyfrom'),num7)\n",
    " \n",
    "    m1=xr.DataArray(map_sum, coords={'lat': map_lats, 'lon':map_lons}, dims=('lat', 'lon'))        \n",
    "    m2=xr.DataArray(map_cnt, coords={'lat': map_lats, 'lon':map_lons}, dims=('lat', 'lon'))        \n",
    "    m3=xr.DataArray(map_max, coords={'lat': map_lats, 'lon':map_lons}, dims=('lat', 'lon'))        \n",
    "\n",
    "    ds=xr.Dataset(data_vars={'sum1': (('cold','swnd','sspd','mld'),sum1),\n",
    "                                'num1': (('cold','swnd','sspd','mld'),num1),\n",
    "                                'sum2': (('cold','cwnd','sspd','mld'),sum2),\n",
    "                                'num2': (('cold','cwnd','sspd','mld'),num2),\n",
    "                                'map_sum': (('lat','lon'),m1),\n",
    "                                'map_cnt': (('lat','lon'),m2),\n",
    "                                'map_max': (('lat','lon'),m3)\n",
    "                               },\n",
    "                                 coords={'cold':dim1,'coldhr':dim2,'colddy':dim3,'swnd':dim4,\n",
    "                                         'wnd':dim5,'sspd':dim6,'mld':dim7,'dyfrom':dim8,\n",
    "                                         'lat':map_lats,'lon':map_lons})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#plotting and data analysis for global cold wakes\n",
    "#from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import geopy.distance\n",
    "from math import sin, pi\n",
    "from scipy import interpolate\n",
    "\n",
    "#functions for running storm data\n",
    "import sys\n",
    "\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input directories\n",
    "dir_storm_info='f:/data/tc_wakes/database/info/'\n",
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "#################################################################################\n",
    "\n",
    "#start to look at data and make some pdfs\n",
    "\n",
    "date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "\n",
    "#init arrays\n",
    "init_data=0\n",
    "num1=np.zeros([101,101,201,101])\n",
    "sum1=np.zeros([101,101,201,101])\n",
    "num2=np.zeros([101,101,201,101])\n",
    "sum2=np.zeros([101,101,201,101])\n",
    "num3=np.zeros(101)\n",
    "num4=np.zeros(101)\n",
    "num5=np.zeros(101)\n",
    "sum5=np.zeros(101)\n",
    "num6=np.zeros(101)\n",
    "sum6=np.zeros(101)\n",
    "sum7=np.zeros(101)\n",
    "num7=np.zeros(101)\n",
    "dim1,dim2,dim3,dim4,dim5,dim6,dim7,dim8=np.zeros(101),np.zeros(101),np.zeros(101),np.zeros(101),np.zeros(101),np.zeros(201),np.zeros(101),np.zeros(101)\n",
    "\n",
    "map_lats=np.arange(-90,90,.25)\n",
    "map_lons=np.arange(-180,180,.25)\n",
    "imap_lats = map_lats.size\n",
    "imap_lons = map_lons.size\n",
    "map_sum,map_cnt,map_max = np.zeros([imap_lats,imap_lons]),np.zeros([imap_lats,imap_lons]),np.zeros([imap_lats,imap_lons])\n",
    "  \n",
    "iyr_storm = 2003\n",
    "for inum_storm in range(65,66): #(0,100): #100):\n",
    "    filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "    exists = os.path.isfile(filename)\n",
    "    if not exists:\n",
    "        continue\n",
    "    print(filename)\n",
    "    ds_storm_info=xr.open_dataset(filename)\n",
    "    ds_storm_info = ds_storm_info.sel(j2=0)\n",
    "    ds_storm_info.close()\n",
    "    filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data_all.nc'\n",
    "    ds_all = xr.open_dataset(filename)\n",
    "    ds_all['spd']=np.sqrt(ds_all.uwnd**2+ds_all.vwnd**2)\n",
    "    ds_all.close()\n",
    "    if abs(ds_all.lon[-1]-ds_all.lon[0])>180:\n",
    "        ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "        ds_storm_info['lon'] = np.mod(ds_storm_info['lon'], 360)\n",
    "    max_lat = ds_storm_info.lat.max()\n",
    "    #remove all data outsice 100km/800km or cold wake >0 or <-10\n",
    "    if max_lat<0:\n",
    "        cond = ((((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm<=0)) | \n",
    "        ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm>0))) \n",
    "        & (ds_all.coldwake_max<=0) & (ds_all.coldwake_max>=-10))\n",
    "    else:\n",
    "        cond = ((((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm<0)) | \n",
    "        ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm>=0))) \n",
    "        & (ds_all.coldwake_max<=0) & (ds_all.coldwake_max>=-10))\n",
    "    subset = ds_all.where(cond)\n",
    "\n",
    "    xdim,ydim,tdim = ds_all.lon.shape[0],ds_all.lat.shape[0],ds_all.time.shape[0]\n",
    "    \n",
    "    #change hr to recovery to day to recovery\n",
    "    subset['coldwake_dytorecovery'] = subset.coldwake_hrtorecovery/24.\n",
    "    \n",
    "    #calculate the historgram and mean for the cold wakes\n",
    "    cbin1 = np.arange(-10, 0, 0.1)  #cold wake bins\n",
    "    subset_sum1=subset.groupby_bins('coldwake_max',cbin1).sum()  #sums number\n",
    "    subset_cnt1=subset.groupby_bins('coldwake_max',cbin1).count()    #sums variable  \n",
    "    cbin2 = np.arange(0,24*6)  #hour to max\n",
    "    subset_sum2=subset.groupby_bins('coldwake_hrtomaxcold',cbin2).sum()  #sums number\n",
    "    subset_cnt2=subset.groupby_bins('coldwake_hrtomaxcold',cbin2).count()    #sums variable\n",
    "    cbin3 = np.arange(0,50)  #dy to recovery\n",
    "    subset_sum3=subset.groupby_bins('coldwake_dytorecovery',cbin3).sum()  #sums number\n",
    "    subset_cnt3=subset.groupby_bins('coldwake_dytorecovery',cbin3).count()    #sums variable\n",
    "    cbin4 = np.arange(0,500)  #mld\n",
    "    subset_sum4=subset.groupby_bins('dbss_obml',cbin4).sum()  #sums number\n",
    "    subset_cnt4=subset.groupby_bins('dbss_obml',cbin4).count()    #sums variable\n",
    "    cbin5 = np.arange(0,200)  #mld\n",
    "    subset_sum5=subset.groupby_bins('spd',cbin5).sum()  #sums number\n",
    "    subset_cnt5=subset.groupby_bins('spd',cbin5).count()    #sums variable\n",
    "\n",
    "    sdate = np.empty([ydim,xdim], dtype=dt.datetime)    \n",
    "    for i in range(0,xdim):\n",
    "        for j in range(0,ydim):\n",
    "            sdate[j,i] = np.datetime64(date_1858 + dt.timedelta(days=float(ds_all.closest_storm_time[j,i])))\n",
    "    #xsdate=xr.DataArray(sdate, coords={'lat': ds_data.lat.values, 'lon':ds_data.lon.values}, dims=('lat', 'lon'))        \n",
    "    sdate2 = np.empty([tdim,ydim,xdim])    \n",
    "    for i in range(0,xdim):\n",
    "        for j in range(0,ydim):\n",
    "            if np.isnan(subset.analysed_sst[0,j,i]):  #don't process masked values\n",
    "                continue\n",
    "            for k in range(0,tdim):\n",
    "                sdate2[k,j,i] = (ds_all.time[k] - sdate[j,i]) / np.timedelta64(1,'D')\n",
    "    xsdate2=xr.DataArray(sdate2, coords={'time':ds_all.time, 'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('time','lat', 'lon'))        \n",
    "    ds_all['dys_from_storm2']=xsdate2\n",
    "    cbin6 = np.arange(-10,50,1)  #cold wake bins\n",
    "    subset_sum6=ds_all.groupby_bins('dys_from_storm2',cbin6).sum()  #sums number\n",
    "    subset_cnt6=ds_all.groupby_bins('dys_from_storm2',cbin6).count()    #sums variable  \n",
    "\n",
    "        #start saving this data\n",
    "    if init_data == 0:\n",
    "        sv_sum1,sv_cnt1 = subset_sum1,subset_cnt1\n",
    "        sv_sum2,sv_cnt2 = subset_sum2,subset_cnt2\n",
    "        sv_sum3,sv_cnt3 = subset_sum3,subset_cnt3\n",
    "        sv_sum4,sv_cnt4 = subset_sum4,subset_cnt4\n",
    "        sv_sum5,sv_cnt5 = subset_sum5,subset_cnt5\n",
    "        sv_sum6,sv_cnt6 = subset_sum6,subset_cnt6\n",
    "        init_data=1\n",
    "    else:\n",
    "        sv_sum1+= subset_sum1\n",
    "        sv_cnt1+= subset_cnt1\n",
    "        sv_sum2+= subset_sum2\n",
    "        sv_cnt2+= subset_cnt2\n",
    "        sv_sum3+= subset_sum3\n",
    "        sv_cnt3+= subset_cnt3\n",
    "        sv_sum4+= subset_sum4\n",
    "        sv_cnt4+= subset_cnt4\n",
    "        sv_sum5+= subset_sum5\n",
    "        sv_cnt5+= subset_cnt5      \n",
    "        sv_sum6+= subset_sum6\n",
    "        sv_cnt6+= subset_cnt6           \n",
    "\n",
    "        #put on global map\n",
    "    tem = subset.coldwake_max.interp(lat=map_lats,lon=map_lons)\n",
    "    tem=tem.fillna(0)\n",
    "    temc=(tem/tem).fillna(0)\n",
    "    map_sum+=tem\n",
    "    map_cnt+=temc\n",
    "    map_max=np.where(tem.data < map_max, tem,map_max)  #where tem<max put tem value in otherwise leave max\n",
    "        \n",
    "    for i in range(0,xdim):\n",
    "        for j in range(0,ydim):\n",
    "            if np.isnan(subset.analysed_sst[0,j,i]):  #don't process masked values\n",
    "                continue\n",
    "            storm_date64 = np.datetime64(date_1858 + dt.timedelta(days=float(ds_all.closest_storm_time[j,i])))\n",
    "            time_diff = subset.time-storm_date64\n",
    "            storm_index = np.argmin(abs(time_diff)).data\n",
    "\n",
    "#            map_sum[j,i]+=subset.coldwake_max[j,i]\n",
    "#            map_cnt[j,i]+=1\n",
    "#            if  map_max[j,i]>subset.coldwake_max[j,i]:\n",
    "#                map_max[j,i]=subset.coldwake_max[j,i]\n",
    "            \n",
    "            if subset.coldwake_max[j,i]<-0.1:  #cold wake larger than -0.1 \n",
    "                icold=int(np.round(subset.coldwake_max[j,i].data*10.))+10\n",
    "                icoldhr=int(np.round(subset.coldwake_hrtomaxcold[j,i].data))\n",
    "                icolddy2=int(np.round(subset.coldwake_hrtorecovery[j,i].data/24.))\n",
    "                iswnd=int(np.round(subset.wmo_storm_wind[j,i].data*.5))\n",
    "                iwnd=int(np.round(subset.spd[storm_index,j,i].data))\n",
    "                isspd=int(abs(np.round(subset.wmo_storm_speed[j,i].data)))\n",
    "\n",
    "            #PROBLEM MLD interpolation pre 1/1 for each year need to fix\n",
    "            #for now just setting first data to 1/1 for each year\n",
    "                im=0\n",
    "                if np.isnan(subset.dbss_obml[im,j,i].data):\n",
    "                    while np.isnan(subset.dbss_obml[im,j,i].data):\n",
    "                        im+=1\n",
    "                        if im>=tdim:\n",
    "                            break\n",
    "                if im>=tdim:\n",
    "                    continue\n",
    "                if np.isnan(subset.dbss_obml[im,j,i].data):\n",
    "                    continue\n",
    "                imld=int(np.round(subset.dbss_obml[im,j,i].data))\n",
    "\n",
    "                if (icold<0) | (icold>100):  #cold wake max between 0,10 K\n",
    "                    continue\n",
    "                if (icold>=0) & (icold<=100) & (isspd>=0) & (isspd<=200) & (imld>=0) & (imld<=100):                \n",
    "                    if (iswnd>=0) & (iswnd<=100):             \n",
    "                        sum1[icold,iswnd,isspd,imld]=sum1[icold,iswnd,isspd,imld]+subset.coldwake_max[j,i].data  #uses storm wind from wmo track\n",
    "                        num1[icold,iswnd,isspd,imld]=num1[icold,iswnd,isspd,imld]+1  #uses storm wind from wmo track\n",
    "                    if (iwnd>=0) & (iwnd<=100):                  \n",
    "                        sum2[icold,iwnd,isspd,imld]=sum2[icold,iwnd,isspd,imld]+subset.coldwake_max[j,i].data    #uses collocated ccmp wind on storm day\n",
    "                        num2[icold,iwnd,isspd,imld]=num2[icold,iwnd,isspd,imld]+1    #uses collocated ccmp wind on storm day\n",
    " #               if (icoldhr>=0) & (icoldhr<100):\n",
    " #                   num3[icoldhr]=num3[icoldhr]+1   #pdf of when max cold wake occurs\n",
    " #               if (icolddy2>=0) & (icolddy2<100):\n",
    " #                   num4[icolddy2]=num4[icolddy2]+1   #pdf of when recovery occurs\n",
    "    #            num4[icold] = num4[icold]+1                     \n",
    " #               sum5[icold]=sum5[icold]+subset.coldwake_hrtorecovery[j,i].data  #mean of recorvery time as f(max coldwake)\n",
    " #               num5[icold]=num5[icold]+1  #mean of recorvery time as f(max coldwake)\n",
    " #               if (icoldhr>=0) & (icoldhr<100):\n",
    " #                   sum6[icoldhr]=sum6[icoldhr]+subset.coldwake_max[j,i].data  #mean of max cold wake as f(hr from storm)\n",
    " #                   num6[icoldhr]=num6[icoldhr]+1  #mean of max cold wake as f(hr from storm)\n",
    "\n",
    "                #caluclate the mean sst anomaly as f(days after storm)\n",
    " #               for k in range(storm_index,tdim):\n",
    " #                   data_time = subset.time[k]\n",
    " #                   dtime = data_time-storm_date64\n",
    " #                   ddy = dtime / np.timedelta64(1,'D')\n",
    " #                   iddy = int(np.round(ddy))\n",
    " #                   if (iddy<0) | (iddy>50):\n",
    " #                       continue\n",
    " #                   change_in_sst = subset.sst_prestorm[j,i].data-subset.analysed_sst[k,j,i].data\n",
    " #                   sum7[iddy]=sum7[iddy]+change_in_sst  \n",
    " #                   num7[iddy]=num7[iddy]+1  \n",
    "\n",
    "    for i in range(0,100):\n",
    "        dim1[i]=(i-10)/10\n",
    "        dim2[i]=i\n",
    "        dim3[i]=i*24\n",
    "        dim4[i]=i/.5\n",
    "        dim5[i]=i\n",
    "        dim7[i]=i\n",
    "        dim8[i]=i\n",
    "    for i in range(0,200):\n",
    "        dim6[i]=i\n",
    "#                                'num3': (('coldhr'),num3),\n",
    "#                                'num4': (('colddy'),num4),\n",
    "#                                'sum5': (('cold'),sum5),\n",
    "#                                'num5': (('cold'),num5),\n",
    "#                                'sum6': (('coldhr'),sum6),\n",
    "#                                'num6': (('coldhr'),num6),\n",
    "#                               'sum7': (('dyfrom'),sum7),\n",
    "#                                'num7': (('dyfrom'),num7)\n",
    " \n",
    "    m1=xr.DataArray(map_sum, coords={'lat': map_lats, 'lon':map_lons}, dims=('lat', 'lon'))        \n",
    "    m2=xr.DataArray(map_cnt, coords={'lat': map_lats, 'lon':map_lons}, dims=('lat', 'lon'))        \n",
    "    m3=xr.DataArray(map_max, coords={'lat': map_lats, 'lon':map_lons}, dims=('lat', 'lon'))        \n",
    "\n",
    "    ds=xr.Dataset(data_vars={'sum1': (('cold','swnd','sspd','mld'),sum1),\n",
    "                                'num1': (('cold','swnd','sspd','mld'),num1),\n",
    "                                'sum2': (('cold','cwnd','sspd','mld'),sum2),\n",
    "                                'num2': (('cold','cwnd','sspd','mld'),num2),\n",
    "                                'map_sum': (('lat','lon'),m1),\n",
    "                                'map_cnt': (('lat','lon'),m2),\n",
    "                                'map_max': (('lat','lon'),m3)\n",
    "                               },\n",
    "                                 coords={'cold':dim1,'coldhr':dim2,'colddy':dim3,'swnd':dim4,\n",
    "                                         'wnd':dim5,'sspd':dim6,'mld':dim7,'dyfrom':dim8,\n",
    "                                         'lat':map_lats,'lon':map_lons})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset.spd.max('time').plot()\n",
    "#subset.coldwake_max.plot()\n",
    "subset.dbss_obml[20,:,:].plot()\n",
    "#subset.hum2m[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.sst_prestorm[40,100].data\n",
    "i,j = 100,40\n",
    "storm_date = date_1858 + dt.timedelta(days=float(ds_all.closest_storm_time[j,i]))  #create new time array that can be queried for year etc\n",
    "storm_date64 = np.datetime64(storm_date)\n",
    "#print(subset.time)\n",
    "time_diff = subset.time-storm_date64\n",
    "storm_index = np.argmin(abs(time_diff)).data\n",
    "istart,iend = int(storm_index)-1,int(storm_index)+5\n",
    "if istart<0:\n",
    "    istart=0\n",
    "if iend>tdim:\n",
    "    iend=tdim\n",
    "coldwake_max[j,i] = (subset.sst_prestorm[j,i]-subset.analysed_sst[istart:iend,j,i]).min()\n",
    "itmp = np.argmin(subset.sst_prestorm[j,i]-subset.analysed_sst[istart:iend,j,i])\n",
    "print(itmp)\n",
    "print(subset.sst_prestorm[j,i].values,subset.analysed_sst[istart:iend,j,i].values)\n",
    "print(istart,iend)\n",
    "print(coldwake_max[j,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(subset.sst_prestorm[j,i]-subset.analysed_sst[istart:iend,j,i]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### create a dataset\n",
    "nxc, nyc = 1000,1000\n",
    "nx, ny = nxc*8*7, nyc*8*7\n",
    "v = da.random.normal(0., .5, size=(nx,ny), chunks=(nxc, nyc))\n",
    "v1 = v\n",
    "v2 = v + da.random.normal(1., .5, size=(nx,ny), chunks=(nxc, nyc))\n",
    "\n",
    "v1min, v1max, dv1= -2., 2., .1\n",
    "v2min, v2max, dv2= -1., 3., .05\n",
    "i1max = int(np.rint((v1max-v1min)/dv1))+1\n",
    "i2max = int(np.rint((v2max-v2min)/dv2))+1\n",
    "def get_index(v1,v2):\n",
    "    ''' This function provides the index of (v1,v2) coupled value position\n",
    "    in the 2D histogram array\n",
    "    '''\n",
    "    i1 = np.maximum(np.rint((v1-v1min)/dv1)+1,0)\n",
    "    i1 = np.minimum(i1,i1max)\n",
    "    i2 = np.maximum(np.rint((v2-v2min)/dv2)+1,0)\n",
    "    i2 = np.minimum(i2,i2max)\n",
    "    return i1+i2*(i1max+1)\n",
    "# apply get_index to v1 and v2 arrays\n",
    "v12 = da.map_blocks(get_index, v1, v2, dtype='float')\n",
    "# compute histogram of linear indices\n",
    "h, lbins = da.histogram(v12, bins=np.arange(-.5,(i1max+1)*(i2max+1)+0.5,1.))\n",
    "# normalize and reshape\n",
    "H = h.compute()\n",
    "H = H*1./H.sum()\n",
    "H = H.reshape((i1max+1,i2max+1), order='F')\n",
    "\n",
    "# compute bins\n",
    "bins1 = np.arange(v1min-dv1, v1max+dv1, dv1)\n",
    "bins2 = np.arange(v2min-dv2, v2max+dv2, dv2)\n",
    "bins1, bins2 = np.meshgrid(bins1,bins2)\n",
    "\n",
    "#\n",
    "plt.figure()\n",
    "plt.pcolor(bins1,bins2,H.transpose())\n",
    "plt.grid()\n",
    "plt.xlabel('v1')\n",
    "plt.ylabel('v2')\n",
    "plt.colorbar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iyr_storm = 2015\n",
    "inum_storm = 50\n",
    "filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "exists = os.path.isfile(filename)\n",
    "print(filename)\n",
    "ds_storm_info=xr.open_dataset(filename)\n",
    "ds_storm_info = ds_storm_info.sel(j2=0)\n",
    "ds_storm_info.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_storm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_storm_info.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
