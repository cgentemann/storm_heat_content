{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input directories\n",
    "dir_storm_info='f:/data/tc_wakes/database/info/'\n",
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "dir_flux = 'F:/data/model_data/oaflux/data_v3/daily/turbulence/'\n",
    "dir_cmc = 'F:/data/sst/cmc/CMC0.2deg/v2/'\n",
    "dir_ccmp='F:/data/sat_data/ccmp/v02.0/Y'\n",
    "##where to get the data through opendap, use these directories instead\n",
    "#dir_cmc = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/CMC/CMC0.1deg/v3/'\n",
    "#dir_flux = 'http://apdrc.soest.hawaii.edu:80/dods/public_data/WHOI_OAFlux/version3/daily/lh_oaflux/'\n",
    "#the latest ccmp is from www.remss.com but they do not have an opendap server so you can use this instead:\n",
    "#dir_ccmp='https://podaac-opendap.jpl.nasa.gov/opendap/allData/ccmp/L3.0/flk/'\n",
    "\n",
    "#################################################################################\n",
    "import geopy.distance\n",
    "from math import sin, pi\n",
    "from scipy import interpolate\n",
    "\n",
    "#functions for running storm data\n",
    "import sys\n",
    "sys.path.append('C:/Users/gentemann/Google Drive/d_drive/python/storm_heat_content/subroutines/')\n",
    "from storm_masking_routines import interpolate_storm_path\n",
    "from storm_masking_routines import get_dist_grid\n",
    "from storm_masking_routines import closest_dist\n",
    "from storm_masking_routines import calculate_storm_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plot output data and mask\n",
    "\n",
    "import os\n",
    "dir_out_figures = 'F:/data/tc_wakes/database/figs/check_storm_dist/'\n",
    "iyr_storm,inum_storm = 2003,27\n",
    "for inum_storm in range(1,27): #100):\n",
    "    filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "    exists = os.path.isfile(filename)\n",
    "    if not exists:\n",
    "        continue\n",
    "    print(filename)\n",
    "    ds_storm=xr.open_dataset(filename)\n",
    "    ds_storm = ds_storm.sel(j2=0)\n",
    "    ds_storm.close()\n",
    "#    ds_storm['lon'] = (ds_storm.lon + 180) % 360 - 180\n",
    "    filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined__masking_data.nc'\n",
    "    ds_all = xr.open_dataset(filename)\n",
    "    ds_all.close()\n",
    "    #ds_all = ds_all.sortby('lon')\n",
    "    print(ds_all.lon[0].values,ds_all.lon[-1].values)\n",
    "    if abs(ds_all.lon[-1]-ds_all.lon[0])>180:\n",
    "        ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "        #ds_all = ds_all.sortby(ds_all.lon)\n",
    "        ds_storm['lon'] = np.mod(ds_storm['lon'], 360)\n",
    "        #ds_storm = ds_storm.sortby(ds_storm.lon)\n",
    "    plt.figure(figsize=(15,3.5))\n",
    "    plt.subplot(131)\n",
    "    ds_all.side_of_storm.plot()\n",
    "    plt.plot(ds_storm.lon,ds_storm.lat,'w')\n",
    "    plt.plot(ds_storm.lon[0],ds_storm.lat[0],'w*')\n",
    "    plt.subplot(132)\n",
    "    ds_all.dist_from_storm_km.plot(vmin=0,vmax=3000)\n",
    "    plt.plot(ds_storm.lon,ds_storm.lat,'w')\n",
    "    max_lat = ds_storm.lat.max()\n",
    "    if max_lat<0:\n",
    "        cond = ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm<=0)) |  ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm>0))\n",
    "    else:\n",
    "        cond = ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm<0)) |  ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm>=0))\n",
    "    subset = ds_all.where(cond)\n",
    "    plt.subplot(133)\n",
    "    subset.dist_from_storm_km.plot(vmin=0,vmax=3000)\n",
    "    plt.plot(ds_storm.lon,ds_storm.lat,'w')\n",
    "    filename = dir_out_figures + str(iyr_storm) + str(inum_storm).zfill(3) + '_interpolated_track.png'\n",
    "    plt.savefig(filename,dpi=100,bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_out_figures = 'F:/data/tc_wakes/database/figs/check_storm_dist/'\n",
    "iyr_storm,inum_storm = 2003,3\n",
    "filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "exists = os.path.isfile(filename)\n",
    "print(filename)\n",
    "ds_storm=xr.open_dataset(filename)\n",
    "ds_storm = ds_storm.sel(j2=0)\n",
    "ds_storm.close()\n",
    "filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined__masking_data.nc'\n",
    "ds_all = xr.open_dataset(filename)\n",
    "ds_all.close()\n",
    "#ds_all = ds_all.sortby('lon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_storm.lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all.lon-360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isave_mld_year = 0 #init MLD monthly data read flag\n",
    "\n",
    "\n",
    "#        if iyr_storm!=2007: # or iyr_storm<2003:\n",
    "#            continue\n",
    "iyr_storm, inum_storm = 2003,3\n",
    "filename = 'F:/data/tc_wakes/database/info/' + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + 'annual_storm_info.nc'\n",
    "isave_mld_year = 0 #init MLD monthly data read flag\n",
    "ds_storm_info = xr.open_dataset(filename)\n",
    "lats = ds_storm_info.lat[0,:]\n",
    "lons = ds_storm_info.lon[0,:]  #lons goes from 0 to 360\n",
    "lons = (lons + 180) % 360 - 180 #put -180 to 180\n",
    "ds_storm_info['lon'] = (ds_storm_info.lon + 180) % 360 - 180\n",
    "dysince = ds_storm_info.time\n",
    "ds_storm_info.close()\n",
    "\n",
    "#make lat and lon of storm onto 25 km grid for below\n",
    "lons = (((lons - .125)/.25+1).astype(int)-1)*.25+.125\n",
    "lats = (((lats + 89.875)/.25+1).astype(int)-1)*.25-89.875\n",
    "\n",
    "iwrap=0\n",
    "#calculate size of box to get data in\n",
    "minlon,maxlon = min(lons.values)-10, max(lons.values)+10\n",
    "minlat,maxlat = min(lats.values)-10, max(lats.values)+10\n",
    "\n",
    "ydim_storm = round((maxlat - minlat)/.25).astype(int)\n",
    "new_lat_storm = np.linspace(minlat, maxlat, ydim_storm)\n",
    "if (minlon<-90 and maxlon>=90) or (minlon<-180 and maxlon<0):  #this storm wraps  keep everythig 0 to 360 then wrap data at very end\n",
    "    iwrap = 1\n",
    "    lons2 = np.mod(lons, 360)\n",
    "    minlon, maxlon = min(lons2.values)-10, max(lons2.values)+10\n",
    "    xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "    new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "else:\n",
    "    xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "    new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "\n",
    "print(iwrap,minlon,maxlon)\n",
    "print(iwrap,xdim_storm, new_lon_storm[:5],new_lon_storm[-5:])\n",
    "\n",
    "\n",
    "date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "dims=lats.shape\n",
    "tdim=dims[0]\n",
    "tem_date=[0]*tdim #print(dysince.values)\n",
    "for i in range(0,tdim):\n",
    "    tem_date[i]=date_1858+dt.timedelta(days=float(dysince[0,i].values))  #create new time array that can be queried for year etc\n",
    "min_date = min(tem_date)+dt.timedelta(days=-5)\n",
    "max_date = max(tem_date)+dt.timedelta(days=5)\n",
    "minjdy = min_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "minyear =min_date.year #create new time array that can be queried for year etc\n",
    "minmon =min_date.month #create new time array that can be queried for year etc\n",
    "minday =min_date.day #create new time array that can be queried for year etc\n",
    "maxjdy = max_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "maxyear =max_date.year  #create new time array that can be queried for year etc\n",
    "print(minyear,minjdy,maxyear,maxjdy)\n",
    "\n",
    "dif = max(tem_date)-min(tem_date)\n",
    "tdim=int(dif.days)+30             #calculate ssts for 30 days after storm\n",
    "\n",
    "#print(tdim,xdim,ydim)            \n",
    "filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "ds_all = xr.open_dataset(filename,drop_variables=['uwnd','vwnd','dbss_obml','lhtfl','shtfl','tmp2m ','hum2m','analysed_sst_clim'])\n",
    "ds_all.close()\n",
    "\n",
    "#        ds_all = xr.merge([ds_storm_ccmp, ds_storm_mld, ds_storm_lhf, ds_storm_shf, ds_storm_ta, ds_storm_qa, ds_storm_sst])\n",
    "if iwrap==1:\n",
    "    ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "    ds_storm_info['lon'] = np.mod(ds_storm_info['lon'], 360)\n",
    "\n",
    "print(iwrap)\n",
    "print(ds_storm_info.lon.min(),ds_storm_info.lon.max())\n",
    "print(ds_all.lon.min(),ds_all.lon.max())\n",
    "\n",
    "#dist,index,stime,position,ds_storm_interp = closest_dist(ds_all,ds_storm_info)\n",
    "ds_in = ds_all.copy(deep=True)\n",
    "#sx_input = ds_storm_info.copy(deep=True)\n",
    "dsx_input = ds_storm_info.copy(deep=True)\n",
    "ds_storm_new = interpolate_storm_path(dsx_input)       \n",
    "tdim,xdim,ydim=ds_storm_new.lat.shape[1], ds_in.analysed_sst[0,:,0].shape[0], ds_in.analysed_sst[0,0,:].shape[0]\n",
    "dx_save=np.zeros([tdim,xdim,ydim])\n",
    "dx_grid,dy_grid = np.meshgrid(ds_in.lon.values,ds_in.lat.values)\n",
    "lon_grid,lat_grid = np.meshgrid(ds_in.lon.values,ds_in.lat.values)\n",
    "min_dist_save = np.zeros([xdim,ydim])*np.nan\n",
    "min_index_save = np.zeros([xdim,ydim])*np.nan\n",
    "min_time_save = np.zeros([xdim,ydim])*np.nan\n",
    "\n",
    "position = np.zeros([xdim,ydim])*np.nan\n",
    "#for each location of the storm calculate the difference for all values in box\n",
    "for ipt in range(0,ds_storm_new.lat.shape[1]):  # all storm values\n",
    "    dist_tem_grid = get_dist_grid(ds_storm_new.lat[0,ipt].values,ds_storm_new.lon[0,ipt].values,lat_grid,lon_grid)\n",
    "    dx_save[ipt,:,:]=dist_tem_grid       \n",
    "#now go through each value in box and find minimum storm location/day\n",
    "for j in range(150,151): #0,ds_in.lon.shape[0]):\n",
    "    for i in range(0,ds_in.lat.shape[0]):\n",
    "        imin = np.argmin(dx_save[:,i,j])\n",
    "        min_dist_save[i,j]=dx_save[imin,i,j]\n",
    "        min_index_save[i,j]=imin\n",
    "        min_time_save[i,j]=ds_storm_new.time[0,imin]\n",
    "        i1,i2=imin,imin+1\n",
    "        if i2>=ds_storm_new.lat.shape[1]:\n",
    "            i1,i2=imin-1,imin\n",
    "        lonx,laty=ds_in.lon[j],ds_in.lat[i]\n",
    "#                sign((Bx - Ax) * (Y - Ay) - (By - Ay) * (X - Ax))\n",
    "        position[i,j] = np.sign((ds_storm_new.lon[0,i2] - ds_storm_new.lon[0,i1]) * (laty - ds_storm_new.lat[0,i1]) \n",
    "                             - (ds_storm_new.lat[0,i2] - ds_storm_new.lat[0,i1]) * (lonx - ds_storm_new.lon[0,i1]))\n",
    "        print(i,j,position[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tem = ds_in.copy(deep=True)\n",
    "for j in range(0,ds_in.lon.shape[0]):\n",
    "    for i in range(0,ds_in.lat.shape[0]):\n",
    "        imin = np.argmin(dx_save[:,i,j])\n",
    "        min_dist_save[i,j]=dx_save[imin,i,j]\n",
    "        min_index_save[i,j]=imin\n",
    "        min_time_save[i,j]=ds_storm_new.time[0,imin]\n",
    "        i1,i2=imin,imin+1\n",
    "        if i2>=ds_storm_new.lat.shape[1]:\n",
    "            i1,i2=imin-1,imin\n",
    "        lonx,laty=ds_tem.lon[j],ds_tem.lat[i]\n",
    "        if (ds_storm_new.lon[0,i2]<0) and (lonx>180):\n",
    "            lonx=lonx-360\n",
    "#                sign((Bx - Ax) * (Y - Ay) - (By - Ay) * (X - Ax))\n",
    "        position[i,j] = np.sign((ds_storm_new.lon[0,i2] - ds_storm_new.lon[0,i1]) * (laty - ds_storm_new.lat[0,i1]) \n",
    "                             - (ds_storm_new.lat[0,i2] - ds_storm_new.lat[0,i1]) * (lonx - ds_storm_new.lon[0,i1]))\n",
    "        #print(i,j,position[i,j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(position)\n",
    "plt.plot(150,150,'r.')\n",
    "plt.plot(150,10,'r.')\n",
    "#plt.plot(ds_storm_new.lon[0,:],ds_storm_new.lat[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j = 10,150\n",
    "imin = np.argmin(dx_save[:,i,j])\n",
    "print('storm',imin,ds_storm_new.lon[0,imin-1].values,ds_storm_new.lat[0,imin-1].values)\n",
    "print('storm',imin,ds_storm_new.lon[0,imin].values,ds_storm_new.lat[0,imin].values)\n",
    "i1,i2=imin,imin+1\n",
    "if i2>=ds_storm_new.lat.shape[1]:\n",
    "    i1,i2=imin-1,imin\n",
    "lonx,laty=ds_in.lon[j]-360,ds_in.lat[i]\n",
    "#                sign((Bx - Ax) * (Y - Ay) - (By - Ay) * (X - Ax))\n",
    "position[i,j] = np.sign((ds_storm_new.lon[0,i2] - ds_storm_new.lon[0,i1]) * (laty - ds_storm_new.lat[0,i1]) \n",
    "                        - (ds_storm_new.lat[0,i2] - ds_storm_new.lat[0,i1]) * (lonx - ds_storm_new.lon[0,i1]))\n",
    "print(i1,i2,imin)\n",
    "print(lonx.values,laty.values)\n",
    "print(position[i,j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds_in.lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(ds_all.lon,ds_all.lat,ds_all.dist_from_storm_km)\n",
    "plt.plot(ds_storm_info.lon[0,:],ds_storm_info.lat[0,:],'r')\n",
    "plt.plot(ds_storm_info.lon[0,0],ds_storm_info.lat[0,0],'w*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate mask\n",
    "print('caluculating mask')\n",
    "ds_mask = calculate_storm_mask(ds_all,lats,lons)\n",
    "ds_all['storm_mask']=ds_mask['storm_mask']\n",
    "#dist to storm\n",
    "print('calculating dist')\n",
    "dist,index,stime,position,ds_storm_interp = closest_dist(ds_all,ds_storm_info)\n",
    "dtem=xr.DataArray(dist, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "ds_all['dist_from_storm_km']=dtem\n",
    "dtem=xr.DataArray(index, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "ds_all['closest_storm_index']=dtem\n",
    "dtem=xr.DataArray(stime, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "ds_all['closest_storm_time']=dtem\n",
    "dtem=xr.DataArray(position, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "ds_all['side_of_storm']=dtem\n",
    "\n",
    "\n",
    "filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined__masking_data.nc'\n",
    "ds_all.to_netcdf(filename)\n",
    "print('out:',filename)\n",
    "filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "ds_storm_interp.to_netcdf(filename)\n",
    "print('out:',filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds_all.lon[:],'k.')\n",
    "iwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('caluculating mask')\n",
    "ds_mask = calculate_storm_mask(ds_all,lats,lons)\n",
    "ds_all['storm_mask']=ds_mask['storm_mask']\n",
    "#dist to storm\n",
    "print('calculating dist')\n",
    "dist,index,stime,position,ds_storm_interp = closest_dist(ds_all,ds_storm_info)\n",
    "dtem=xr.DataArray(dist, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "ds_all['dist_from_storm_km']=dtem\n",
    "dtem=xr.DataArray(index, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "ds_all['closest_storm_index']=dtem\n",
    "dtem=xr.DataArray(stime, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "ds_all['closest_storm_time']=dtem\n",
    "dtem=xr.DataArray(position, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "ds_all['side_of_storm']=dtem\n",
    "\n",
    "\n",
    "filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined__masking_data2.nc'\n",
    "ds_all.to_netcdf(filename)\n",
    "print('out:',filename)\n",
    "filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track2.nc'\n",
    "ds_storm_interp.to_netcdf(filename)\n",
    "print('out:',filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all.side_of_storm.plot()\n",
    "plt.plot(ds_storm_interp.lon[0,:],ds_storm_interp.lat[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test storm\n",
    "isave_mld_year = 0 #init MLD monthly data read flag\n",
    "for root, dirs, files in os.walk(dir_storm_info, topdown=False):\n",
    "    if root[len(dir_storm_info):len(dir_storm_info)+1]=='.':\n",
    "        continue\n",
    "    for name in files:\n",
    "        if not name.endswith('.nc'):\n",
    "            continue\n",
    "        filename=os.path.join(root, name)\n",
    "        print(filename[36:39],filename[31:35])\n",
    "        inum_storm=int(filename[36:39])\n",
    "        iyr_storm=int(filename[31:35])\n",
    "\n",
    "        \n",
    "        if iyr_storm!=2003:\n",
    "            continue\n",
    "        if inum_storm!=5:\n",
    "            continue\n",
    "#        if iyr_storm==2002 and inum_storm<9:\n",
    "#            continue\n",
    "        \n",
    "        \n",
    "#        if iyr_storm!=2007: # or iyr_storm<2003:\n",
    "#            continue\n",
    "        print(name,filename)\n",
    "        ds_storm_info = xr.open_dataset(filename)\n",
    "        lats = ds_storm_info.lat[0,:]\n",
    "        lons = ds_storm_info.lon[0,:]  #lons goes from 0 to 360\n",
    "        lons = (lons + 180) % 360 - 180 #put -180 to 180\n",
    "        dysince = ds_storm_info.time\n",
    "        ds_storm_info.close()\n",
    "        \n",
    "#make lat and lon of storm onto 25 km grid for below\n",
    "        lons = (((lons - .125)/.25+1).astype(int)-1)*.25+.125\n",
    "        lats = (((lats + 89.875)/.25+1).astype(int)-1)*.25-89.875\n",
    "        \n",
    "        iwrap=0\n",
    "#calculate size of box to get data in\n",
    "        minlon,maxlon = min(lons.values)-10, max(lons.values)+10\n",
    "        minlat,maxlat = min(lats.values)-10, max(lats.values)+10\n",
    "\n",
    "        ydim_storm = round((maxlat - minlat)/.25).astype(int)\n",
    "        new_lat_storm = np.linspace(minlat, maxlat, ydim_storm)\n",
    "        if (minlon<-90 and maxlon>=90) or (minlon<-180 and maxlon<0):  #this storm wraps  keep everythig 0 to 360 then wrap data at very end\n",
    "            iwrap = 1\n",
    "            lons2 = np.mod(lons, 360)\n",
    "            minlon, maxlon = min(lons2.values)-10, max(lons2.values)+10\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "        else:\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "\n",
    "        print(iwrap,minlon,maxlon)\n",
    "        print(iwrap,xdim_storm, new_lon_storm[:5],new_lon_storm[-5:])\n",
    "\n",
    "        \n",
    "        date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "        dims=lats.shape\n",
    "        tdim=dims[0]\n",
    "        tem_date=[0]*tdim #print(dysince.values)\n",
    "        for i in range(0,tdim):\n",
    "            tem_date[i]=date_1858+dt.timedelta(days=float(dysince[0,i].values))  #create new time array that can be queried for year etc\n",
    "        min_date = min(tem_date)+dt.timedelta(days=-5)\n",
    "        max_date = max(tem_date)+dt.timedelta(days=5)\n",
    "        minjdy = min_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        minyear =min_date.year #create new time array that can be queried for year etc\n",
    "        minmon =min_date.month #create new time array that can be queried for year etc\n",
    "        minday =min_date.day #create new time array that can be queried for year etc\n",
    "        maxjdy = max_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        maxyear =max_date.year  #create new time array that can be queried for year etc\n",
    "        print(minyear,minjdy,maxyear,maxjdy)\n",
    "        \n",
    "        dif = max(tem_date)-min(tem_date)\n",
    "        tdim=int(dif.days)+30             #calculate ssts for 30 days after storm\n",
    "\n",
    "        #print(tdim,xdim,ydim)            \n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "        ds_all = xr.open_dataset(filename,drop_variables=['uwnd','vwnd','dbss_obml','lhtfl','shtfl','tmp2m ','hum2m','analysed_sst_clim'])\n",
    "        ds_all.close()\n",
    "        \n",
    "#        ds_all = xr.merge([ds_storm_ccmp, ds_storm_mld, ds_storm_lhf, ds_storm_shf, ds_storm_ta, ds_storm_qa, ds_storm_sst])\n",
    "        if iwrap==1:\n",
    "            ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "            ds_storm_info['lon'] = np.mod(ds_storm_info['lon'], 360)\n",
    "\n",
    "        #calculate mask\n",
    "        print('caluculating mask')\n",
    "        ds_mask = calculate_storm_mask(ds_all,lats,lons)\n",
    "        ds_all['storm_mask']=ds_mask['storm_mask']\n",
    "        #dist to storm\n",
    "        print('calculating dist')\n",
    "        dist,index,stime,position,ds_storm_interp = closest_dist(ds_all,ds_storm_info)\n",
    "        dtem=xr.DataArray(dist, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['dist_from_storm_km']=dtem\n",
    "        dtem=xr.DataArray(index, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['closest_storm_index']=dtem\n",
    "        dtem=xr.DataArray(stime, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['closest_storm_time']=dtem\n",
    "        dtem=xr.DataArray(position, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['side_of_storm']=dtem\n",
    "\n",
    "        \n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined__masking_data.nc'\n",
    "        ds_all.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "        ds_storm_interp.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined__masking_data2.nc'\n",
    "        ds_all.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track2.nc'\n",
    "        ds_storm_interp.to_netcdf(filename)\n",
    "        print('out:',filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
