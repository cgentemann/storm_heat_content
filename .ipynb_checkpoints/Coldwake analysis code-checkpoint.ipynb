{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting and data analysis for global cold wakes\n",
    "#from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input directories\n",
    "dir_storm_info='f:/data/tc_wakes/database/info/'\n",
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "dir_flux = 'F:/data/model_data/oaflux/data_v3/daily/turbulence/'\n",
    "dir_cmc = 'F:/data/sst/cmc/CMC0.2deg/v2/'\n",
    "dir_ccmp='F:/data/sat_data/ccmp/v02.0/Y'\n",
    "##where to get the data through opendap, use these directories instead\n",
    "#dir_cmc = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/CMC/CMC0.1deg/v3/'\n",
    "#dir_flux = 'http://apdrc.soest.hawaii.edu:80/dods/public_data/WHOI_OAFlux/version3/daily/lh_oaflux/'\n",
    "#the latest ccmp is from www.remss.com but they do not have an opendap server so you can use this instead:\n",
    "#dir_ccmp='https://podaac-opendap.jpl.nasa.gov/opendap/allData/ccmp/L3.0/flk/'\n",
    "\n",
    "#################################################################################\n",
    "import geopy.distance\n",
    "from math import sin, pi\n",
    "from scipy import interpolate\n",
    "\n",
    "#functions for running storm data\n",
    "import sys\n",
    "#sys.path.append('C:/Users/gentemann/Google Drive/d_drive/python/storm_heat_content/subroutines/')\n",
    "#from storm_masking_routines import interpolate_storm_path\n",
    "#from storm_masking_routines import get_dist_grid\n",
    "#from storm_masking_routines import closest_dist\n",
    "#from storm_masking_routines import calculate_storm_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plot output data and mask\n",
    "\n",
    "import os\n",
    "dir_out_figures = 'F:/data/tc_wakes/database/figs/check_storm_dist/'\n",
    "iyr_storm,inum_storm = 2003,27\n",
    "for inum_storm in range(1,58): #100):\n",
    "    filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "    exists = os.path.isfile(filename)\n",
    "    if not exists:\n",
    "        continue\n",
    "    print(filename)\n",
    "    ds_storm=xr.open_dataset(filename)\n",
    "    ds_storm = ds_storm.sel(j2=0)\n",
    "    ds_storm.close()\n",
    "#    ds_storm['lon'] = (ds_storm.lon + 180) % 360 - 180\n",
    "    filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "    ds_all = xr.open_dataset(filename)\n",
    "    ds_all.close()\n",
    "    #ds_all = ds_all.sortby('lon')\n",
    "    print(ds_all.lon[0].values,ds_all.lon[-1].values)\n",
    "    if abs(ds_all.lon[-1]-ds_all.lon[0])>180:\n",
    "        ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "        #ds_all = ds_all.sortby(ds_all.lon)\n",
    "        ds_storm['lon'] = np.mod(ds_storm['lon'], 360)\n",
    "        #ds_storm = ds_storm.sortby(ds_storm.lon)\n",
    "    plt.figure(figsize=(15,3.5))\n",
    "    plt.subplot(131)\n",
    "    ds_all.side_of_storm.plot()\n",
    "    plt.plot(ds_storm.lon,ds_storm.lat,'w')\n",
    "    plt.plot(ds_storm.lon[0],ds_storm.lat[0],'w*')\n",
    "    plt.subplot(132)\n",
    "    ds_all.dist_from_storm_km.plot(vmin=0,vmax=3000)\n",
    "    plt.plot(ds_storm.lon,ds_storm.lat,'w')\n",
    "    max_lat = ds_storm.lat.max()\n",
    "    if max_lat<0:\n",
    "        cond = ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm<=0)) |  ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm>0))\n",
    "    else:\n",
    "        cond = ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm<0)) |  ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm>=0))\n",
    "    subset = ds_all.where(cond)\n",
    "    plt.subplot(133)\n",
    "    subset.dist_from_storm_km.plot(vmin=0,vmax=3000)\n",
    "    plt.plot(ds_storm.lon,ds_storm.lat,'w')\n",
    "    filename = dir_out_figures + str(iyr_storm) + str(inum_storm).zfill(3) + '_interpolated_track.png'\n",
    "    plt.savefig(filename,dpi=100,bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum7=np.zeros(50)\n",
    "print(sum7[0],sum7[49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:/data/tc_wakes/database/sst/2003/002_interpolated_track.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gentemann\\Miniconda3\\lib\\site-packages\\xarray\\core\\groupby.py:749: FutureWarning: Default reduction dimension will be changed to the grouped dimension after xarray 0.12. To silence this warning, pass dim=xarray.ALL_DIMS explicitly.\n",
      "  allow_lazy=True, **kwargs)\n",
      "C:\\Users\\gentemann\\Miniconda3\\lib\\site-packages\\xarray\\core\\groupby.py:755: FutureWarning: Default reduction dimension will be changed to the grouped dimension after xarray 0.12. To silence this warning, pass dim=xarray.ALL_DIMS explicitly.\n",
      "  **kwargs)\n",
      "C:\\Users\\gentemann\\Miniconda3\\lib\\site-packages\\xarray\\core\\groupby.py:749: FutureWarning: Default reduction dimension will be changed to the grouped dimension after xarray 0.12. To silence this warning, pass dim=xarray.ALL_DIMS explicitly.\n",
      "  allow_lazy=True, **kwargs)\n",
      "C:\\Users\\gentemann\\Miniconda3\\lib\\site-packages\\xarray\\core\\groupby.py:755: FutureWarning: Default reduction dimension will be changed to the grouped dimension after xarray 0.12. To silence this warning, pass dim=xarray.ALL_DIMS explicitly.\n",
      "  **kwargs)\n",
      "C:\\Users\\gentemann\\Miniconda3\\lib\\site-packages\\xarray\\core\\groupby.py:749: FutureWarning: Default reduction dimension will be changed to the grouped dimension after xarray 0.12. To silence this warning, pass dim=xarray.ALL_DIMS explicitly.\n",
      "  allow_lazy=True, **kwargs)\n",
      "C:\\Users\\gentemann\\Miniconda3\\lib\\site-packages\\xarray\\core\\groupby.py:755: FutureWarning: Default reduction dimension will be changed to the grouped dimension after xarray 0.12. To silence this warning, pass dim=xarray.ALL_DIMS explicitly.\n",
      "  **kwargs)\n",
      "C:\\Users\\gentemann\\Miniconda3\\lib\\site-packages\\xarray\\core\\groupby.py:749: FutureWarning: Default reduction dimension will be changed to the grouped dimension after xarray 0.12. To silence this warning, pass dim=xarray.ALL_DIMS explicitly.\n",
      "  allow_lazy=True, **kwargs)\n",
      "C:\\Users\\gentemann\\Miniconda3\\lib\\site-packages\\xarray\\core\\groupby.py:755: FutureWarning: Default reduction dimension will be changed to the grouped dimension after xarray 0.12. To silence this warning, pass dim=xarray.ALL_DIMS explicitly.\n",
      "  **kwargs)\n",
      "C:\\Users\\gentemann\\Miniconda3\\lib\\site-packages\\xarray\\core\\groupby.py:749: FutureWarning: Default reduction dimension will be changed to the grouped dimension after xarray 0.12. To silence this warning, pass dim=xarray.ALL_DIMS explicitly.\n",
      "  allow_lazy=True, **kwargs)\n",
      "C:\\Users\\gentemann\\Miniconda3\\lib\\site-packages\\xarray\\core\\groupby.py:755: FutureWarning: Default reduction dimension will be changed to the grouped dimension after xarray 0.12. To silence this warning, pass dim=xarray.ALL_DIMS explicitly.\n",
      "  **kwargs)\n",
      "C:\\Users\\gentemann\\Miniconda3\\lib\\site-packages\\xarray\\core\\groupby.py:749: FutureWarning: Default reduction dimension will be changed to the grouped dimension after xarray 0.12. To silence this warning, pass dim=xarray.ALL_DIMS explicitly.\n",
      "  allow_lazy=True, **kwargs)\n",
      "C:\\Users\\gentemann\\Miniconda3\\lib\\site-packages\\xarray\\core\\groupby.py:755: FutureWarning: Default reduction dimension will be changed to the grouped dimension after xarray 0.12. To silence this warning, pass dim=xarray.ALL_DIMS explicitly.\n",
      "  **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "conflicting sizes for dimension 'lat': length 720 on the data but length 117 on coordinate 'lat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-923cb65a52a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;31m#                                'num7': (('dyfrom'),num7)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m     \u001b[0mm1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'lat'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mds_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lon'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mds_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lon'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m     \u001b[0mm2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_cnt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'lat'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mds_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lon'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mds_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lon'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[0mm3\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'lat'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mds_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lon'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mds_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lon'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\xarray\\core\\dataarray.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, coords, dims, name, attrs, encoding, fastpath)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_compatible_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0mcoords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_infer_coords_and_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m             \u001b[0mvariable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\xarray\\core\\dataarray.py\u001b[0m in \u001b[0;36m_infer_coords_and_dims\u001b[1;34m(shape, coords, dims)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 raise ValueError('conflicting sizes for dimension %r: '\n\u001b[0;32m     83\u001b[0m                                  \u001b[1;34m'length %s on the data but length %s on '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                                  'coordinate %r' % (d, sizes[d], s, k))\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msizes\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: conflicting sizes for dimension 'lat': length 720 on the data but length 117 on coordinate 'lat'"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#plotting and data analysis for global cold wakes\n",
    "#from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input directories\n",
    "dir_storm_info='f:/data/tc_wakes/database/info/'\n",
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "dir_flux = 'F:/data/model_data/oaflux/data_v3/daily/turbulence/'\n",
    "dir_cmc = 'F:/data/sst/cmc/CMC0.2deg/v2/'\n",
    "dir_ccmp='F:/data/sat_data/ccmp/v02.0/Y'\n",
    "##where to get the data through opendap, use these directories instead\n",
    "#dir_cmc = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/CMC/CMC0.1deg/v3/'\n",
    "#dir_flux = 'http://apdrc.soest.hawaii.edu:80/dods/public_data/WHOI_OAFlux/version3/daily/lh_oaflux/'\n",
    "#the latest ccmp is from www.remss.com but they do not have an opendap server so you can use this instead:\n",
    "#dir_ccmp='https://podaac-opendap.jpl.nasa.gov/opendap/allData/ccmp/L3.0/flk/'\n",
    "\n",
    "#################################################################################\n",
    "import geopy.distance\n",
    "from math import sin, pi\n",
    "from scipy import interpolate\n",
    "\n",
    "#functions for running storm data\n",
    "import sys\n",
    "#sys.path.append('C:/Users/gentemann/Google Drive/d_drive/python/storm_heat_content/subroutines/')\n",
    "#from storm_masking_routines import interpolate_storm_path\n",
    "#from storm_masking_routines import get_dist_grid\n",
    "#from storm_masking_routines import closest_dist\n",
    "#from storm_masking_routines import calculate_storm_mask\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#start to look at data and make some pdfs\n",
    "\n",
    "\n",
    "date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "\n",
    "\n",
    "#init arrays\n",
    "init_data=0\n",
    "num1=np.zeros([101,101,201,101])\n",
    "sum1=np.zeros([101,101,201,101])\n",
    "num2=np.zeros([101,101,201,101])\n",
    "sum2=np.zeros([101,101,201,101])\n",
    "num3=np.zeros(101)\n",
    "num4=np.zeros(101)\n",
    "num5=np.zeros(101)\n",
    "sum5=np.zeros(101)\n",
    "num6=np.zeros(101)\n",
    "sum6=np.zeros(101)\n",
    "sum7=np.zeros(101)\n",
    "num7=np.zeros(101)\n",
    "dim1,dim2,dim3,dim4,dim5,dim6,dim7,dim8=np.zeros(101),np.zeros(101),np.zeros(101),np.zeros(101),np.zeros(101),np.zeros(201),np.zeros(101),np.zeros(101)\n",
    "\n",
    "map_lats=np.arange(-90,90,.25)\n",
    "map_lons=np.arange(-180,180,.25)\n",
    "imap_lats = map_lats.size\n",
    "imap_lons = map_lons.size\n",
    "map_sum,map_cnt,map_max = np.zeros([imap_lats,imap_lons]),np.zeros([imap_lats,imap_lons]),np.zeros([imap_lats,imap_lons])\n",
    "  \n",
    "iyr_storm = 2003\n",
    "for inum_storm in range(1,6): #(0,100): #100):\n",
    "    filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "    exists = os.path.isfile(filename)\n",
    "    if not exists:\n",
    "        continue\n",
    "    print(filename)\n",
    "    ds_storm_info=xr.open_dataset(filename)\n",
    "    ds_storm_info = ds_storm_info.sel(j2=0)\n",
    "    ds_storm_info.close()\n",
    "    filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data_all.nc'\n",
    "    ds_all = xr.open_dataset(filename)\n",
    "    ds_all['spd']=np.sqrt(ds_all.uwnd**2+ds_all.vwnd**2)\n",
    "    ds_all.close()\n",
    "    if abs(ds_all.lon[-1]-ds_all.lon[0])>180:\n",
    "        ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "        ds_storm_info['lon'] = np.mod(ds_storm_info['lon'], 360)\n",
    "    max_lat = ds_storm_info.lat.max()\n",
    "    #remove all data outsice 100km/800km or cold wake >0 or <-10\n",
    "    if max_lat<0:\n",
    "        cond = ((((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm<=0)) | \n",
    "        ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm>0))) \n",
    "        & (ds_all.coldwake_max<=0) & (ds_all.coldwake_max>=-10))\n",
    "    else:\n",
    "        cond = ((((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm<0)) | \n",
    "        ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm>=0))) \n",
    "        & (ds_all.coldwake_max<=0) & (ds_all.coldwake_max>=-10))\n",
    "    subset = ds_all.where(cond)\n",
    "\n",
    "    xdim,ydim,tdim = ds_all.lon.shape[0],ds_all.lat.shape[0],ds_all.time.shape[0]\n",
    "    \n",
    "    #change hr to recovery to day to recovery\n",
    "    subset['coldwake_dytorecovery'] = subset.coldwake_hrtorecovery/24.\n",
    "    \n",
    "    #calculate the historgram and mean for the cold wakes\n",
    "    cbin1 = np.arange(-10, 0, 0.1)  #cold wake bins\n",
    "    subset_sum1=subset.groupby_bins('coldwake_max',cbin1).sum()  #sums number\n",
    "    subset_cnt1=subset.groupby_bins('coldwake_max',cbin1).count()    #sums variable  \n",
    "    cbin2 = np.arange(0,24*6)  #hour to max\n",
    "    subset_sum2=subset.groupby_bins('coldwake_hrtomaxcold',cbin2).sum()  #sums number\n",
    "    subset_cnt2=subset.groupby_bins('coldwake_hrtomaxcold',cbin2).count()    #sums variable\n",
    "    cbin3 = np.arange(0,50)  #dy to recovery\n",
    "    subset_sum3=subset.groupby_bins('coldwake_dytorecovery',cbin3).sum()  #sums number\n",
    "    subset_cnt3=subset.groupby_bins('coldwake_dytorecovery',cbin3).count()    #sums variable\n",
    "    cbin4 = np.arange(0,500)  #mld\n",
    "    subset_sum4=subset.groupby_bins('dbss_obml',cbin4).sum()  #sums number\n",
    "    subset_cnt4=subset.groupby_bins('dbss_obml',cbin4).count()    #sums variable\n",
    "    cbin5 = np.arange(0,200)  #mld\n",
    "    subset_sum5=subset.groupby_bins('spd',cbin5).sum()  #sums number\n",
    "    subset_cnt5=subset.groupby_bins('spd',cbin5).count()    #sums variable\n",
    "\n",
    "    sdate = np.empty([ydim,xdim], dtype=dt.datetime)    \n",
    "    for i in range(0,xdim):\n",
    "        for j in range(0,ydim):\n",
    "            sdate[j,i] = np.datetime64(date_1858 + dt.timedelta(days=float(ds_all.closest_storm_time[j,i])))\n",
    "    #xsdate=xr.DataArray(sdate, coords={'lat': ds_data.lat.values, 'lon':ds_data.lon.values}, dims=('lat', 'lon'))        \n",
    "    sdate2 = np.empty([tdim,ydim,xdim])    \n",
    "    for i in range(0,xdim):\n",
    "        for j in range(0,ydim):\n",
    "            if np.isnan(subset.analysed_sst[0,j,i]):  #don't process masked values\n",
    "                continue\n",
    "            for k in range(0,tdim):\n",
    "                sdate2[k,j,i] = (ds_all.time[k] - sdate[j,i]) / np.timedelta64(1,'D')\n",
    "    xsdate2=xr.DataArray(sdate2, coords={'time':ds_all.time, 'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('time','lat', 'lon'))        \n",
    "    ds_all['dys_from_storm2']=xsdate2\n",
    "    cbin6 = np.arange(-10,50,1)  #cold wake bins\n",
    "    subset_sum6=ds_all.groupby_bins('dys_from_storm2',cbin6).sum()  #sums number\n",
    "    subset_cnt6=ds_all.groupby_bins('dys_from_storm2',cbin6).count()    #sums variable  \n",
    "\n",
    "        #start saving this data\n",
    "    if init_data == 0:\n",
    "        sv_sum1,sv_cnt1 = subset_sum1,subset_cnt1\n",
    "        sv_sum2,sv_cnt2 = subset_sum2,subset_cnt2\n",
    "        sv_sum3,sv_cnt3 = subset_sum3,subset_cnt3\n",
    "        sv_sum4,sv_cnt4 = subset_sum4,subset_cnt4\n",
    "        sv_sum5,sv_cnt5 = subset_sum5,subset_cnt5\n",
    "        sv_sum6,sv_cnt6 = subset_sum6,subset_cnt6\n",
    "        init_data=1\n",
    "    else:\n",
    "        sv_sum1+= subset_sum1\n",
    "        sv_cnt1+= subset_cnt1\n",
    "        sv_sum2+= subset_sum2\n",
    "        sv_cnt2+= subset_cnt2\n",
    "        sv_sum3+= subset_sum3\n",
    "        sv_cnt3+= subset_cnt3\n",
    "        sv_sum4+= subset_sum4\n",
    "        sv_cnt4+= subset_cnt4\n",
    "        sv_sum5+= subset_sum5\n",
    "        sv_cnt5+= subset_cnt5      \n",
    "        sv_sum6+= subset_sum6\n",
    "        sv_cnt6+= subset_cnt6           \n",
    "\n",
    "        #put on global map\n",
    "    tem = subset.coldwake_max.interp(lat=map_lats,lon=map_lons)\n",
    "    tem=tem.fillna(0)\n",
    "    temc=(tem/tem).fillna(0)\n",
    "    map_sum+=tem\n",
    "    map_cnt+=temc\n",
    "    map_max=np.where(tem.data < map_max, tem,map_max)  #where tem<max put tem value in otherwise leave max\n",
    "        \n",
    "    for i in range(0,xdim):\n",
    "        for j in range(0,ydim):\n",
    "            if np.isnan(subset.analysed_sst[0,j,i]):  #don't process masked values\n",
    "                continue\n",
    "            storm_date64 = np.datetime64(date_1858 + dt.timedelta(days=float(ds_all.closest_storm_time[j,i])))\n",
    "            time_diff = subset.time-storm_date64\n",
    "            storm_index = np.argmin(abs(time_diff)).data\n",
    "\n",
    "#            map_sum[j,i]+=subset.coldwake_max[j,i]\n",
    "#            map_cnt[j,i]+=1\n",
    "#            if  map_max[j,i]>subset.coldwake_max[j,i]:\n",
    "#                map_max[j,i]=subset.coldwake_max[j,i]\n",
    "            \n",
    "            if subset.coldwake_max[j,i]<-0.1:  #cold wake larger than -0.1 \n",
    "                icold=int(np.round(subset.coldwake_max[j,i].data*10.))+10\n",
    "                icoldhr=int(np.round(subset.coldwake_hrtomaxcold[j,i].data))\n",
    "                icolddy2=int(np.round(subset.coldwake_hrtorecovery[j,i].data/24.))\n",
    "                iswnd=int(np.round(subset.wmo_storm_wind[j,i].data*.5))\n",
    "                iwnd=int(np.round(subset.spd[storm_index,j,i].data))\n",
    "                isspd=int(abs(np.round(subset.wmo_storm_speed[j,i].data)))\n",
    "\n",
    "            #PROBLEM MLD interpolation pre 1/1 for each year need to fix\n",
    "            #for now just setting first data to 1/1 for each year\n",
    "                im=0\n",
    "                if np.isnan(subset.dbss_obml[im,j,i].data):\n",
    "                    while np.isnan(subset.dbss_obml[im,j,i].data):\n",
    "                        im+=1\n",
    "                        if im>=tdim:\n",
    "                            break\n",
    "                if im>=tdim:\n",
    "                    continue\n",
    "                if np.isnan(subset.dbss_obml[im,j,i].data):\n",
    "                    continue\n",
    "                imld=int(np.round(subset.dbss_obml[im,j,i].data))\n",
    "\n",
    "                if (icold<0) | (icold>100):  #cold wake max between 0,10 K\n",
    "                    continue\n",
    "                if (icold>=0) & (icold<=100) & (isspd>=0) & (isspd<=200) & (imld>=0) & (imld<=100):                \n",
    "                    if (iswnd>=0) & (iswnd<=100):             \n",
    "                        sum1[icold,iswnd,isspd,imld]=sum1[icold,iswnd,isspd,imld]+subset.coldwake_max[j,i].data  #uses storm wind from wmo track\n",
    "                        num1[icold,iswnd,isspd,imld]=num1[icold,iswnd,isspd,imld]+1  #uses storm wind from wmo track\n",
    "                    if (iwnd>=0) & (iwnd<=100):                  \n",
    "                        sum2[icold,iwnd,isspd,imld]=sum2[icold,iwnd,isspd,imld]+subset.coldwake_max[j,i].data    #uses collocated ccmp wind on storm day\n",
    "                        num2[icold,iwnd,isspd,imld]=num2[icold,iwnd,isspd,imld]+1    #uses collocated ccmp wind on storm day\n",
    " #               if (icoldhr>=0) & (icoldhr<100):\n",
    " #                   num3[icoldhr]=num3[icoldhr]+1   #pdf of when max cold wake occurs\n",
    " #               if (icolddy2>=0) & (icolddy2<100):\n",
    " #                   num4[icolddy2]=num4[icolddy2]+1   #pdf of when recovery occurs\n",
    "    #            num4[icold] = num4[icold]+1                     \n",
    " #               sum5[icold]=sum5[icold]+subset.coldwake_hrtorecovery[j,i].data  #mean of recorvery time as f(max coldwake)\n",
    " #               num5[icold]=num5[icold]+1  #mean of recorvery time as f(max coldwake)\n",
    " #               if (icoldhr>=0) & (icoldhr<100):\n",
    " #                   sum6[icoldhr]=sum6[icoldhr]+subset.coldwake_max[j,i].data  #mean of max cold wake as f(hr from storm)\n",
    " #                   num6[icoldhr]=num6[icoldhr]+1  #mean of max cold wake as f(hr from storm)\n",
    "\n",
    "                #caluclate the mean sst anomaly as f(days after storm)\n",
    " #               for k in range(storm_index,tdim):\n",
    " #                   data_time = subset.time[k]\n",
    " #                   dtime = data_time-storm_date64\n",
    " #                   ddy = dtime / np.timedelta64(1,'D')\n",
    " #                   iddy = int(np.round(ddy))\n",
    " #                   if (iddy<0) | (iddy>50):\n",
    " #                       continue\n",
    " #                   change_in_sst = subset.sst_prestorm[j,i].data-subset.analysed_sst[k,j,i].data\n",
    " #                   sum7[iddy]=sum7[iddy]+change_in_sst  \n",
    " #                   num7[iddy]=num7[iddy]+1  \n",
    "\n",
    "    for i in range(0,100):\n",
    "        dim1[i]=(i-10)/10\n",
    "        dim2[i]=i\n",
    "        dim3[i]=i*24\n",
    "        dim4[i]=i/.5\n",
    "        dim5[i]=i\n",
    "        dim7[i]=i\n",
    "        dim8[i]=i\n",
    "    for i in range(0,200):\n",
    "        dim6[i]=i\n",
    "#                                'num3': (('coldhr'),num3),\n",
    "#                                'num4': (('colddy'),num4),\n",
    "#                                'sum5': (('cold'),sum5),\n",
    "#                                'num5': (('cold'),num5),\n",
    "#                                'sum6': (('coldhr'),sum6),\n",
    "#                                'num6': (('coldhr'),num6),\n",
    "#                               'sum7': (('dyfrom'),sum7),\n",
    "#                                'num7': (('dyfrom'),num7)\n",
    " \n",
    "    m1=xr.DataArray(map_sum, coords={'lat': map_lats, 'lon':map_lons}, dims=('lat', 'lon'))        \n",
    "    m2=xr.DataArray(map_cnt, coords={'lat': map_lats, 'lon':map_lons}, dims=('lat', 'lon'))        \n",
    "    m3=xr.DataArray(map_max, coords={'lat': map_lats, 'lon':map_lons}, dims=('lat', 'lon'))        \n",
    "\n",
    "    ds=xr.Dataset(data_vars={'sum1': (('cold','swnd','sspd','mld'),sum1),\n",
    "                                'num1': (('cold','swnd','sspd','mld'),num1),\n",
    "                                'sum2': (('cold','cwnd','sspd','mld'),sum2),\n",
    "                                'num2': (('cold','cwnd','sspd','mld'),num2),\n",
    "                                'map_sum': (('lat','lon'),m1),\n",
    "                                'map_cnt': (('lat','lon'),m2),\n",
    "                                'map_max': (('lat','lon'),m3)\n",
    "                               },\n",
    "                                 coords={'cold':dim1,'coldhr':dim2,'colddy':dim3,'swnd':dim4,\n",
    "                                         'wnd':dim5,'sspd':dim6,'mld':dim7,'dyfrom':dim8,\n",
    "                                         'lat':map_lats,'lon':map_lons})\n",
    "    if 10*(int(inum_storm/10))==inum_storm:\n",
    "        filename='f:/data/tc_wakes/database/results/pdf'+str(iyr_storm)+'.nc'\n",
    "        ds.to_netcdf(filename)\n",
    "        print('output file',inum_storm, filename)\n",
    "filename='f:/data/tc_wakes/database/results/pdf'+str(iyr_storm)+'.nc'\n",
    "ds.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/sum1'+str(iyr_storm)+'.nc'\n",
    "sv_sum1.coords['coldwake_max_bins'] = cbin1[0:-1]  # could also use assign_coords\n",
    "sv_sum1.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/cnt1'+str(iyr_storm)+'.nc'\n",
    "sv_cnt1.coords['coldwake_max_bins'] = cbin1[0:-1]  # could also use assign_coords\n",
    "sv_cnt1.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/sum2'+str(iyr_storm)+'.nc'\n",
    "sv_sum2.coords['coldwake_hrtomaxcold_bins'] = cbin2[0:-1]  # could also use assign_coords\n",
    "sv_sum2.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/cnt2'+str(iyr_storm)+'.nc'\n",
    "sv_cnt2.coords['coldwake_hrtomaxcold_bins'] = cbin2[0:-1]  # could also use assign_coords\n",
    "sv_cnt2.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/sum3'+str(iyr_storm)+'.nc'\n",
    "sv_sum3.coords['coldwake_dytorecovery_bins'] = cbin3[0:-1]  # could also use assign_coords\n",
    "sv_sum3.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/cnt3'+str(iyr_storm)+'.nc'\n",
    "sv_cnt3.coords['coldwake_dytorecovery_bins'] = cbin3[0:-1]  # could also use assign_coords\n",
    "sv_cnt3.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/sum4'+str(iyr_storm)+'.nc'\n",
    "sv_sum4.coords['dbss_obml_bins'] = cbin4[0:-1]  # could also use assign_coords\n",
    "sv_sum4.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/cnt4'+str(iyr_storm)+'.nc'\n",
    "sv_cnt4.coords['dbss_obml_bins'] = cbin4[0:-1]  # could also use assign_coords\n",
    "sv_cnt4.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/sum5'+str(iyr_storm)+'.nc'\n",
    "sv_sum5.coords['spd_bins'] = cbin5[0:-1]  # could also use assign_coords\n",
    "sv_sum5.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/cnt5'+str(iyr_storm)+'.nc'\n",
    "sv_cnt5.coords['spd_bins'] = cbin5[0:-1]  # could also use assign_coords\n",
    "sv_cnt5.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/sum6'+str(iyr_storm)+'.nc'\n",
    "sv_sum6.coords['dys_from_storm2_bins'] = cbin6[0:-1]  # could also use assign_coords\n",
    "sv_sum6.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/cnt6'+str(iyr_storm)+'.nc'\n",
    "sv_cnt6.coords['dys_from_storm2_bins'] = cbin6[0:-1]  # could also use assign_coords\n",
    "sv_cnt6.to_netcdf(filename)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='f:/data/tc_wakes/database/results/pdf'+str(iyr_storm)+'.nc'\n",
    "ds.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/sum1'+str(iyr_storm)+'.nc'\n",
    "sv_sum1.coords['coldwake_max_bins'] = cbin1[0:-1]  # could also use assign_coords\n",
    "sv_sum1.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/cnt1'+str(iyr_storm)+'.nc'\n",
    "sv_cnt1.coords['coldwake_max_bins'] = cbin1[0:-1]  # could also use assign_coords\n",
    "sv_cnt1.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/sum2'+str(iyr_storm)+'.nc'\n",
    "sv_sum2.coords['coldwake_hrtomaxcold_bins'] = cbin2[0:-1]  # could also use assign_coords\n",
    "sv_sum2.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/cnt2'+str(iyr_storm)+'.nc'\n",
    "sv_cnt2.coords['coldwake_hrtomaxcold_bins'] = cbin2[0:-1]  # could also use assign_coords\n",
    "sv_cnt2.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/sum3'+str(iyr_storm)+'.nc'\n",
    "sv_sum3.coords['coldwake_dytorecovery_bins'] = cbin3[0:-1]  # could also use assign_coords\n",
    "sv_sum3.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/cnt3'+str(iyr_storm)+'.nc'\n",
    "sv_cnt3.coords['coldwake_dytorecovery_bins'] = cbin3[0:-1]  # could also use assign_coords\n",
    "sv_cnt3.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/sum4'+str(iyr_storm)+'.nc'\n",
    "sv_sum4.coords['dbss_obml_bins'] = cbin4[0:-1]  # could also use assign_coords\n",
    "sv_sum4.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/cnt4'+str(iyr_storm)+'.nc'\n",
    "sv_cnt4.coords['dbss_obml_bins'] = cbin4[0:-1]  # could also use assign_coords\n",
    "sv_cnt4.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/sum5'+str(iyr_storm)+'.nc'\n",
    "sv_sum5.coords['spd_bins'] = cbin5[0:-1]  # could also use assign_coords\n",
    "sv_sum5.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/cnt5'+str(iyr_storm)+'.nc'\n",
    "sv_cnt5.coords['spd_bins'] = cbin5[0:-1]  # could also use assign_coords\n",
    "sv_cnt5.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/sum6'+str(iyr_storm)+'.nc'\n",
    "sv_sum6.coords['dys_from_storm2_bins'] = cbin6[0:-1]  # could also use assign_coords\n",
    "sv_sum6.to_netcdf(filename)\n",
    "filename='f:/data/tc_wakes/database/results/cnt6'+str(iyr_storm)+'.nc'\n",
    "sv_cnt6.coords['dys_from_storm2_bins'] = cbin6[0:-1]  # could also use assign_coords\n",
    "sv_cnt6.to_netcdf(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_sum1,sv_cnt1 = subset_sum1,subset_cnt1\n",
    "sv_sum1.rename({'coldwake_max_bins': 'coldwake_max_bins'}).set_coords(['coldwake_max_bins'])\n",
    "sv_sum1.coords['coldwake_max_bins'] = cbin1[0:-1]  # could also use assign_coords\n",
    "sv_sum1a = sv_sum1.copy(deep=True)\n",
    "sv_sum1a.reset_coords(drop=True)\n",
    "sv_sum1a\n",
    "sv_sum1a.to_netcdf(filename)\n",
    "#sv_sum1a = sv_sum1.swap_dims({'coldwake_max_bins': 'cbin'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem = xr.DataArray\n",
    "#tem=subset.groupby_bins('coldwake_max',cbin1).sum()  #sums number\n",
    "cbin1.shape\n",
    "ds2=xr.Dataset(data_vars={'sv_sum1': (('cbin'),sv_sum1.coldwake_max.data),\n",
    "                            'sv_cnt1': (('cbin'),sv_cnt1.coldwake_max.data)\n",
    "                            },\n",
    "                             coords={'cbin':cbin1[0:-1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:                (coldwake_max_bins: 99)\n",
       "Coordinates:\n",
       "  * coldwake_max_bins      (coldwake_max_bins) object (-10.0, -9.9] ... (-0.2, -0.1]\n",
       "Data variables:\n",
       "    uwnd                   (coldwake_max_bins) float64 nan nan ... -2.483e+05\n",
       "    vwnd                   (coldwake_max_bins) float64 nan nan ... 6.182e+04\n",
       "    date                   (coldwake_max_bins) float64 nan nan ... 5.206e+09\n",
       "    timePlot               (coldwake_max_bins) float64 nan nan ... 5.208e+07\n",
       "    dbss_obml              (coldwake_max_bins) float64 nan nan ... 9.694e+05\n",
       "    lhtfl                  (coldwake_max_bins) float64 nan nan ... 6.329e+06\n",
       "    shtfl                  (coldwake_max_bins) float64 nan nan ... 3.207e+05\n",
       "    tmp2m                  (coldwake_max_bins) float64 nan nan ... 1.548e+06\n",
       "    hum2m                  (coldwake_max_bins) float64 nan nan ... 1.034e+06\n",
       "    analysed_sst           (coldwake_max_bins) float64 nan nan ... 1.716e+07\n",
       "    mask                   (coldwake_max_bins) float64 nan nan ... 5.7e+04\n",
       "    storm_mask             (coldwake_max_bins) float64 nan nan ... -5.455e+04\n",
       "    dist_from_storm_km     (coldwake_max_bins) float64 nan nan ... 3.735e+05\n",
       "    closest_storm_index    (coldwake_max_bins) float64 nan nan ... 2.946e+04\n",
       "    closest_storm_time     (coldwake_max_bins) float64 nan nan ... 5.26e+07\n",
       "    side_of_storm          (coldwake_max_bins) float64 nan nan ... 408.0 798.0\n",
       "    sst_prestorm           (coldwake_max_bins) float64 nan nan ... 3.006e+05\n",
       "    wmo_storm_wind         (coldwake_max_bins) float64 nan nan ... 4.088e+04\n",
       "    wmo_storm_pres         (coldwake_max_bins) float64 nan nan ... 9.9e+05\n",
       "    wmo_storm_speed        (coldwake_max_bins) float64 nan nan ... 1.552e+04\n",
       "    coldwake_max           (coldwake_max_bins) float64 nan nan ... -134.9 -146.7\n",
       "    coldwake_maxindex      (coldwake_max_bins) float64 nan nan ... 1.310e+04\n",
       "    coldwake_hrtomaxcold   (coldwake_max_bins) float64 nan nan ... 7.236e+04\n",
       "    coldwake_hrtorecovery  (coldwake_max_bins) float64 nan nan ... 7.236e+04\n",
       "    spd                    (coldwake_max_bins) float64 nan nan ... 3.476e+05\n",
       "    coldwake_dytorecovery  (coldwake_max_bins) float64 nan nan ... 3.015e+03"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_sum1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xf5596668>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cbin1[0:-1],sv_cnt1.coldwake_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "conflicting sizes for dimension 'time': length 156 on the data but length 56 on coordinate 'time'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-76c60e860eec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0msdate2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mds_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msdate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'D'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mxsdate2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mds_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lat'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mds_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lon'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mds_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'lat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lon'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mds_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dys_from_storm2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxsdate2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\xarray\\core\\dataarray.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, coords, dims, name, attrs, encoding, fastpath)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_compatible_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0mcoords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_infer_coords_and_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m             \u001b[0mvariable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\xarray\\core\\dataarray.py\u001b[0m in \u001b[0;36m_infer_coords_and_dims\u001b[1;34m(shape, coords, dims)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 raise ValueError('conflicting sizes for dimension %r: '\n\u001b[0;32m     83\u001b[0m                                  \u001b[1;34m'length %s on the data but length %s on '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                                  'coordinate %r' % (d, sizes[d], s, k))\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msizes\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: conflicting sizes for dimension 'time': length 156 on the data but length 56 on coordinate 'time'"
     ]
    }
   ],
   "source": [
    "m1=xr.DataArray(sv_sum1, coords={'cbin1': cbin1}, dims=('cbin1'))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'time' ()>\n",
      "array('2002-12-18T12:00:00.000000000', dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 2002-12-18T12:00:00\n",
      "2003-01-03T18:00:00.000043\n",
      "<xarray.DataArray 'time' ()>\n",
      "array(-1404000000043000, dtype='timedelta64[ns]')\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 2002-12-18T12:00:00\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-26fccf9d2692>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;31m#print((ds_all.time[k] -storm_date64) / np.timedelta64(1,'D'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0msdate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mds_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mstorm_date64\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'D'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mxsdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mds_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lat'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mds_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lon'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mds_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lon'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mds_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dys_from_storm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxsdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#            iddy = int(np.round(ddy))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ds_data' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:                (lat: 156, lon: 147, time: 56)\n",
       "Coordinates:\n",
       "  * lat                    (lat) float64 -37.12 -36.87 -36.62 ... 1.623 1.875\n",
       "  * lon                    (lon) float64 157.9 158.1 158.4 ... 194.1 194.4 194.6\n",
       "  * time                   (time) datetime64[ns] 2002-12-18T12:00:00 ... 2003-02-11T12:00:00\n",
       "Data variables:\n",
       "    uwnd                   (time, lat, lon) float64 -3.757 -3.51 ... -5.762\n",
       "    vwnd                   (time, lat, lon) float64 3.898 4.173 ... -2.506\n",
       "    date                   (time) float64 ...\n",
       "    timePlot               (time) float64 ...\n",
       "    dbss_obml              (time, lat, lon) float64 ...\n",
       "    lhtfl                  (time, lat, lon) float64 ...\n",
       "    shtfl                  (time, lat, lon) float64 ...\n",
       "    tmp2m                  (time, lat, lon) float64 ...\n",
       "    hum2m                  (time, lat, lon) float64 ...\n",
       "    analysed_sst           (time, lat, lon) float64 ...\n",
       "    mask                   (time, lat, lon) float64 ...\n",
       "    storm_mask             (time, lat, lon) int32 ...\n",
       "    dist_from_storm_km     (lat, lon) float64 1.496e+03 1.481e+03 ... 1.567e+03\n",
       "    closest_storm_index    (lat, lon) float64 ...\n",
       "    closest_storm_time     (lat, lon) float64 ...\n",
       "    side_of_storm          (lat, lon) float64 1.0 1.0 1.0 1.0 ... -1.0 -1.0 -1.0\n",
       "    sst_prestorm           (lat, lon) float64 ...\n",
       "    wmo_storm_wind         (lat, lon) float64 ...\n",
       "    wmo_storm_pres         (lat, lon) float64 ...\n",
       "    wmo_storm_speed        (lat, lon) float64 ...\n",
       "    coldwake_max           (lat, lon) float64 nan nan nan nan ... nan nan nan\n",
       "    coldwake_maxindex      (lat, lon) float64 ...\n",
       "    coldwake_hrtomaxcold   (lat, lon) float64 ...\n",
       "    coldwake_hrtorecovery  (lat, lon) float64 ...\n",
       "    spd                    (time, lat, lon) float64 5.414 5.453 ... 6.376 6.283\n",
       "    sst_anom               (lat, lon, time) float64 nan nan nan ... nan nan nan\n",
       "    dys_from_storm2        (time, lat, lon) float64 -16.25 -16.25 ... 49.75"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gentemann\\Miniconda3\\lib\\site-packages\\xarray\\core\\groupby.py:749: FutureWarning: Default reduction dimension will be changed to the grouped dimension after xarray 0.12. To silence this warning, pass dim=xarray.ALL_DIMS explicitly.\n",
      "  allow_lazy=True, **kwargs)\n",
      "C:\\Users\\gentemann\\Miniconda3\\lib\\site-packages\\xarray\\core\\groupby.py:755: FutureWarning: Default reduction dimension will be changed to the grouped dimension after xarray 0.12. To silence this warning, pass dim=xarray.ALL_DIMS explicitly.\n",
      "  **kwargs)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cbin1[0:-1],cwake_cnt1.coldwake_max)\n",
    "#subset.coldwake_dytorecovery[40:80,75]\n",
    "\n",
    "#subset.coldwake_hrtomaxcold[40:80,75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem=np.empty([ydim,xdim], dtype=dt.datetime)    \n",
    "for i in range(0,xdim):\n",
    "        for j in range(0,ydim):\n",
    "            tem[j,i] = np.datetime64(date_1858 + dt.timedelta(days=float(ds_all.closest_storm_time[j,i])))\n",
    "ds_all['closest_storm_time64']=xr.DataArray(tem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all['closest_storm_time64']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sum7/num7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbin = np.arange(-10, 0, 0.1)\n",
    "icold=subset.coldwake_max*10.+10\n",
    "#icoldhr=int(np.round(subset.coldwake_hrtomaxcold.data))\n",
    "#icolddy2=int(np.round(subset.coldwake_hrtorecovery.data/24.))\n",
    "#iswnd=int(np.round(subset.wmo_storm_wind.data*.5))\n",
    "#iwnd=int(np.round(subset.spd.data))\n",
    "#isspd=int(abs(np.round(subset.wmo_storm_speed.data)))\n",
    "m=subset.groupby_bins('coldwake_max',cbin).mean()   #calculates mean\n",
    "c=subset.groupby_bins('coldwake_max',cbin).count()  #sums number\n",
    "s=subset.groupby_bins('coldwake_max',cbin).sum()    #sums variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.coldwake_max[-2].values,s.coldwake_max[-2].values,c.coldwake_max[-2].values,s.coldwake_max[-2].values/c.coldwake_max[-2].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iyr_storm = 2003\n",
    "filename='f:/data/tc_wakes/database/results/pdf'+str(iyr_storm)+'.nc'\n",
    "#plot the pdsf different ways\n",
    "ds = xr.open_dataset(filename)\n",
    "ds.close()\n",
    "ds\n",
    "ds.num7.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "from math import sin, pi\n",
    "import numpy as np\n",
    "#this routine takes a point and finds distance to all points in a grid of lat and lon\n",
    "#it is slowwwwwww\n",
    "tdim_storm = ds_storm_info.time.size\n",
    "storm_speed = ds_storm_info.time.copy(deep=True)*np.nan    \n",
    "for i in range(0,tdim_storm-1):\n",
    "    coords_1 = (ds_storm_info.lat[i], ds_storm_info.lon[i])  \n",
    "    coords_2 = (ds_storm_info.lat[i+1], ds_storm_info.lon[i+1])  \n",
    "    arclen_temp = geopy.distance.geodesic(coords_1, coords_2).km  #distance in km  \n",
    "    storm_date1 = np.datetime64(date_1858 + dt.timedelta(days=float(ds_storm_info.time[i])))  #create new time array that can be queried for year etc\n",
    "    storm_date2 = np.datetime64(date_1858 + dt.timedelta(days=float(ds_storm_info.time[i+1])))  #create new time array that can be queried for year etc\n",
    "    arclen_time = storm_date2 - storm_date1\n",
    "    arclen_hr = arclen_time / np.timedelta64(1, 'h')\n",
    "    storm_speed[i]=arclen_temp/(arclen_hr)\n",
    "storm_speed[-1]=storm_speed[-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all = xr.open_dataset(filename)\n",
    "ds_all['spd']=np.sqrt(ds_all.uwnd**2+ds_all.vwnd**2)\n",
    "ds_all.close()\n",
    "if abs(ds_all.lon[-1]-ds_all.lon[0])>180:\n",
    "    ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "    ds_storm_info['lon'] = np.mod(ds_storm_info['lon'], 360)\n",
    "max_lat = ds_storm_info.lat.max()\n",
    "if max_lat<0:\n",
    "    cond = ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm<=0)) |  ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm>0))\n",
    "else:\n",
    "    cond = ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm<0)) |  ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm>=0))\n",
    "subset = ds_all.where(cond)\n",
    "#subset now only has the data within 100 and 800 km of storm\n",
    "xdim,ydim,tdim = ds_all.lon.shape[0],ds_all.lat.shape[0],ds_all.time.shape[0]\n",
    "date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "coldwake_max=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "coldwake_delay=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "for i in range(0,xdim):\n",
    "    for j in range(0,ydim):\n",
    "        storm_date = date_1858 + dt.timedelta(days=float(ds_all.closest_storm_time[j,i]))  #create new time array that can be queried for year etc\n",
    "        storm_date64 = np.datetime64(storm_date)\n",
    "        if np.isnan(subset.analysed_sst[0,j,i]):  #don't process masked values\n",
    "            continue\n",
    "        time_diff = subset.time-storm_date64\n",
    "        storm_index = np.argmin(abs(time_diff)).data\n",
    "        istart,iend = int(storm_index)-1,int(storm_index)+5\n",
    "        if istart<0:\n",
    "            istart=0\n",
    "        if iend>tdim:\n",
    "            iend=tdim\n",
    "        coldwake_max[j,i] = (subset.sst_prestorm[j,i]-subset.analysed_sst[istart:iend,j,i]).min()\n",
    "coldwake_max.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(subset.sst_prestorm-subset.analysed_sst[0,:,:]).plot()  cold wake\n",
    "xdim,ydim,tdim = ds_all.lon.shape[0],ds_all.lat.shape[0],ds_all.time.shape[0]\n",
    "date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "coldwake_max=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "coldwake_maxindex=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "coldwake_hrtomaxcold=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "coldwake_recovery=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "for i in range(0,xdim):\n",
    "    for j in range(0,ydim):\n",
    "        storm_date = date_1858 + dt.timedelta(days=float(ds_all.closest_storm_time[j,i]))  #create new time array that can be queried for year etc\n",
    "        storm_date64 = np.datetime64(storm_date)\n",
    "        if np.isnan(subset.analysed_sst[0,j,i]):  #don't process masked values\n",
    "            continue\n",
    "        time_diff = subset.time-storm_date64\n",
    "        storm_index = np.argmin(abs(time_diff)).data\n",
    "        istart,iend = int(storm_index)-1,int(storm_index)+5\n",
    "        if istart<0:\n",
    "            istart=0\n",
    "        if iend>tdim:\n",
    "            iend=tdim\n",
    "        coldwake_max[j,i] = (subset.sst_prestorm[j,i]-subset.analysed_sst[istart:iend,j,i]).min()\n",
    "        itmp = np.argmin(subset.sst_prestorm[j,i]-subset.analysed_sst[istart:iend,j,i]).data\n",
    "        coldwake_maxindex[j,i]=istart+itmp\n",
    "        delay = subset.time[istart+itmp].values-subset.time[istart+1].values\n",
    "        coldwake_hrtomaxcold[j,i]=delay / np.timedelta64(1, 'h')\n",
    "        for k in range(istart+itmp,tdim):\n",
    "            sst_change = subset.sst_prestorm[j,i]-subset.analysed_sst[k,j,i]\n",
    "            if sst_change>-0.2:\n",
    "                break\n",
    "        delay = subset.time[k].values-subset.time[istart+1].values\n",
    "        coldwake_recovery[j,i]=delay / np.timedelta64(1, 'h')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j = 100,100\n",
    "coldwake_max=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "coldwake_maxindex=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "coldwake_hrtomaxcold=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "coldwake_recovery=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "storm_date = date_1858 + dt.timedelta(days=float(ds_all.closest_storm_time[j,i]))  #create new time array that can be queried for year etc\n",
    "storm_date64 = np.datetime64(storm_date)\n",
    "time_diff = subset.time-storm_date64\n",
    "storm_index = np.argmin(abs(time_diff)).data\n",
    "istart,iend = int(storm_index)-1,int(storm_index)+5\n",
    "if istart<0:\n",
    "    istart=0\n",
    "if iend>tdim:\n",
    "    iend=tdim\n",
    "coldwake_max[j,i] = (subset.sst_prestorm[j,i]-subset.analysed_sst[istart:iend,j,i]).min()\n",
    "itmp = np.argmin(subset.sst_prestorm[j,i]-subset.analysed_sst[istart:iend,j,i]).data\n",
    "coldwake_maxindex[j,i]=istart+itmp\n",
    "delay = subset.time[istart+itmp].values-subset.time[istart+1].values\n",
    "coldwake_hrtomaxcold[j,i]=delay / np.timedelta64(1, 'h')\n",
    "print(coldwake_hrtomaxcold[j,i])\n",
    "for k in range(istart+itmp,tdim):\n",
    "    sst_change = subset.sst_prestorm[j,i]-subset.analysed_sst[k,j,i]\n",
    "    if sst_change>-0.2:\n",
    "        break\n",
    "delay = subset.time[k].values-subset.time[istart+1].values\n",
    "coldwake_recovery[j,i]=delay / np.timedelta64(1, 'h')\n",
    "print(coldwake_recovery[j,i].data)\n",
    "print(subset.sst_prestorm[j,i]-subset.analysed_sst[istart+itmp:,j,i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_all.time[0])\n",
    "plt.subplot(131)\n",
    "plt.pcolormesh(ds_all.lon,ds_all.lat,ds_all.coldwake_max,vmin=-1,vmax=1)\n",
    "plt.plot(ds_storm_info.lon+360,ds_storm_info.lat,'w')\n",
    "plt.plot(ds_storm_info.lon[0]+360,ds_storm_info.lat[0],'r*')\n",
    "plt.subplot(132)\n",
    "plt.pcolormesh(subset.lon,subset.lat,subset.coldwake_max,vmin=-1,vmax=1)\n",
    "plt.subplot(133)\n",
    "plt.pcolormesh(coldwake_recovery/24,vmin=0,vmax=40)\n",
    "plt.colorbar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_storm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,tdim):\n",
    "    storm_date = date_1858 + dt.timedelta(days=float(ds_storm_info.time[i]))  #create new time array that can be queried for year etc\n",
    "    print(storm_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.sst_prestorm[40,100].data\n",
    "i,j = 100,40\n",
    "storm_date = date_1858 + dt.timedelta(days=float(ds_all.closest_storm_time[j,i]))  #create new time array that can be queried for year etc\n",
    "storm_date64 = np.datetime64(storm_date)\n",
    "#print(subset.time)\n",
    "time_diff = subset.time-storm_date64\n",
    "storm_index = np.argmin(abs(time_diff)).data\n",
    "istart,iend = int(storm_index)-1,int(storm_index)+5\n",
    "if istart<0:\n",
    "    istart=0\n",
    "if iend>tdim:\n",
    "    iend=tdim\n",
    "coldwake_max[j,i] = (subset.sst_prestorm[j,i]-subset.analysed_sst[istart:iend,j,i]).min()\n",
    "itmp = np.argmin(subset.sst_prestorm[j,i]-subset.analysed_sst[istart:iend,j,i])\n",
    "print(itmp)\n",
    "print(subset.sst_prestorm[j,i].values,subset.analysed_sst[istart:iend,j,i].values)\n",
    "print(istart,iend)\n",
    "print(coldwake_max[j,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(subset.sst_prestorm[j,i]-subset.analysed_sst[istart:iend,j,i]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_out2 = 'F:/data/tc_wakes/database/sst/'\n",
    "iyr_storm,inum_storm = 2003,2\n",
    "filename = dir_out2 + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "dsint=xr.open_dataset(filename)\n",
    "dsint.close()\n",
    "filename = dir_out2 + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data_all.nc'\n",
    "ds_all = xr.open_dataset(filename)\n",
    "ds_all.close()\n",
    "print(dsint.dims)\n",
    "print(ds_all.closest_storm_index.min(),ds_all.closest_storm_index.max())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all.spd.max('time').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyr=2003\n",
    "filename='F:/data/tc_wakes/ibtracks/year/Year.'+str(lyr)+'.ibtracs_wmo.v03r10.nc'\n",
    "ds_storm_info = xr.open_dataset(filename)\n",
    "ds_storm_info.close()\n",
    "ds_storm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test storm\n",
    "isave_mld_year = 0 #init MLD monthly data read flag\n",
    "for root, dirs, files in os.walk(dir_storm_info, topdown=False):\n",
    "    if root[len(dir_storm_info):len(dir_storm_info)+1]=='.':\n",
    "        continue\n",
    "    for name in files:\n",
    "        if not name.endswith('.nc'):\n",
    "            continue\n",
    "        filename=os.path.join(root, name)\n",
    "        print(filename[36:39],filename[31:35])\n",
    "        inum_storm=int(filename[36:39])\n",
    "        iyr_storm=int(filename[31:35])\n",
    "\n",
    "        \n",
    "        if iyr_storm!=2003:\n",
    "            continue\n",
    "        if inum_storm!=5:\n",
    "            continue\n",
    "#        if iyr_storm==2002 and inum_storm<9:\n",
    "#            continue\n",
    "        \n",
    "        \n",
    "#        if iyr_storm!=2007: # or iyr_storm<2003:\n",
    "#            continue\n",
    "        print(name,filename)\n",
    "        ds_storm_info = xr.open_dataset(filename)\n",
    "        lats = ds_storm_info.lat[0,:]\n",
    "        lons = ds_storm_info.lon[0,:]  #lons goes from 0 to 360\n",
    "        lons = (lons + 180) % 360 - 180 #put -180 to 180\n",
    "        dysince = ds_storm_info.time\n",
    "        ds_storm_info.close()\n",
    "        \n",
    "#make lat and lon of storm onto 25 km grid for below\n",
    "        lons = (((lons - .125)/.25+1).astype(int)-1)*.25+.125\n",
    "        lats = (((lats + 89.875)/.25+1).astype(int)-1)*.25-89.875\n",
    "        \n",
    "        iwrap=0\n",
    "#calculate size of box to get data in\n",
    "        minlon,maxlon = min(lons.values)-10, max(lons.values)+10\n",
    "        minlat,maxlat = min(lats.values)-10, max(lats.values)+10\n",
    "\n",
    "        ydim_storm = round((maxlat - minlat)/.25).astype(int)\n",
    "        new_lat_storm = np.linspace(minlat, maxlat, ydim_storm)\n",
    "        if (minlon<-90 and maxlon>=90) or (minlon<-180 and maxlon<0):  #this storm wraps  keep everythig 0 to 360 then wrap data at very end\n",
    "            iwrap = 1\n",
    "            lons2 = np.mod(lons, 360)\n",
    "            minlon, maxlon = min(lons2.values)-10, max(lons2.values)+10\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "        else:\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "\n",
    "        print(iwrap,minlon,maxlon)\n",
    "        print(iwrap,xdim_storm, new_lon_storm[:5],new_lon_storm[-5:])\n",
    "\n",
    "        \n",
    "        date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "        dims=lats.shape\n",
    "        tdim=dims[0]\n",
    "        tem_date=[0]*tdim #print(dysince.values)\n",
    "        for i in range(0,tdim):\n",
    "            tem_date[i]=date_1858+dt.timedelta(days=float(dysince[0,i].values))  #create new time array that can be queried for year etc\n",
    "        min_date = min(tem_date)+dt.timedelta(days=-5)\n",
    "        max_date = max(tem_date)+dt.timedelta(days=5)\n",
    "        minjdy = min_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        minyear =min_date.year #create new time array that can be queried for year etc\n",
    "        minmon =min_date.month #create new time array that can be queried for year etc\n",
    "        minday =min_date.day #create new time array that can be queried for year etc\n",
    "        maxjdy = max_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        maxyear =max_date.year  #create new time array that can be queried for year etc\n",
    "        print(minyear,minjdy,maxyear,maxjdy)\n",
    "        \n",
    "        dif = max(tem_date)-min(tem_date)\n",
    "        tdim=int(dif.days)+30             #calculate ssts for 30 days after storm\n",
    "\n",
    "        #print(tdim,xdim,ydim)            \n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "        ds_all = xr.open_dataset(filename,drop_variables=['uwnd','vwnd','dbss_obml','lhtfl','shtfl','tmp2m ','hum2m','analysed_sst_clim'])\n",
    "        ds_all.close()\n",
    "        \n",
    "#        ds_all = xr.merge([ds_storm_ccmp, ds_storm_mld, ds_storm_lhf, ds_storm_shf, ds_storm_ta, ds_storm_qa, ds_storm_sst])\n",
    "        if iwrap==1:\n",
    "            ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "            ds_storm_info['lon'] = np.mod(ds_storm_info['lon'], 360)\n",
    "\n",
    "        #calculate mask\n",
    "        print('caluculating mask')\n",
    "        ds_mask = calculate_storm_mask(ds_all,lats,lons)\n",
    "        ds_all['storm_mask']=ds_mask['storm_mask']\n",
    "        #dist to storm\n",
    "        print('calculating dist')\n",
    "        dist,index,stime,position,ds_storm_interp = closest_dist(ds_all,ds_storm_info)\n",
    "        dtem=xr.DataArray(dist, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['dist_from_storm_km']=dtem\n",
    "        dtem=xr.DataArray(index, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['closest_storm_index']=dtem\n",
    "        dtem=xr.DataArray(stime, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['closest_storm_time']=dtem\n",
    "        dtem=xr.DataArray(position, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['side_of_storm']=dtem\n",
    "\n",
    "        \n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined__masking_data.nc'\n",
    "        ds_all.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "        ds_storm_interp.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined__masking_data2.nc'\n",
    "        ds_all.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track2.nc'\n",
    "        ds_storm_interp.to_netcdf(filename)\n",
    "        print('out:',filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
