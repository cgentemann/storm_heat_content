{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
    "import os\n",
    "import time\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input files\n",
    "dir_in='f:/data/tc_wakes/database/info/'\n",
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "dir_mur = 'F:/data/sst/jpl_mur/v4.1/'\n",
    "dir_flux = 'F:/data/model_data/oaflux/data_v3/daily/turbulence/'\n",
    "dir_cmc = 'F:/data/sst/cmc/CMC0.2deg/v2/'\n",
    "#output files\n",
    "#filename_out_nc='F:/data/cruise_data/saildrone/baja-2018/daily_files/sd-1002/data_so_far.nc'\n",
    "#filename_out_kml='F:/data/cruise_data/saildrone/baja-2018/daily_files/sd-1002/data_so_far.kml'\n",
    "#################################################################################\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "#import pandas\n",
    "#import matplotlib as mpl\n",
    "#import openpyxl\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "import math\n",
    "#from math import cos, radians\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ccmp='F:/data/sat_data/ccmp/v02.0/Y'\n",
    "date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "dx=0.25\n",
    "dy=0.25\n",
    "dx_offset = -179.875\n",
    "dy_offset = -78.3750\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(dir_in, topdown=False):\n",
    "    if root[len(dir_in):len(dir_in)+1]=='.':\n",
    "        continue\n",
    "#    if root[len(dir_in):len(dir_in)+4]=='2002':\n",
    "#        continue\n",
    "#    for ii in range(12,13): \n",
    "    for name in files:\n",
    "#        name = files[ii]\n",
    "#    for name in files:\n",
    "        fname_in=os.path.join(root, name)\n",
    "        fname_out=dir_out + fname_in[31:39] + '_all_25km.nc'\n",
    "        inum_storm=int(fname_in[36:39])\n",
    "        iyr_storm=int(fname_in[31:35])\n",
    "        if iyr_storm<=2003: # or iyr_storm<2003:\n",
    "            continue\n",
    "#        if iyr_storm==2011 and inum_storm<15:\n",
    "#            continue\n",
    "        print(name,fname_in)\n",
    "        dsx = xr.open_dataset(fname_in)\n",
    "        lats = dsx.lat[0,:]\n",
    "        lons = dsx.lon[0,:]  #lons goes from 0 to 360\n",
    "        lons = (lons + 180) % 360 - 180\n",
    "        dysince = dsx.time\n",
    "        dsx.close()\n",
    "        \n",
    "#make lat and lon of storm onto 25 km grid for below\n",
    "        lons = (((lons - .125)/.25+1).astype(int)-1)*.25+.125\n",
    "        lats = (((lats + 89.875)/.25+1).astype(int)-1)*.25-89.875\n",
    "        \n",
    "        iwrap=0\n",
    "        minlon=min(lons.values)-10\n",
    "#        if minlon<-180.0:\n",
    "#            minlon+=360.\n",
    "        maxlon=max(lons.values)+10\n",
    "#        if maxlon>180.0:\n",
    "#            maxlon-=360.\n",
    "        minlat=min(lats.values)-10\n",
    "        maxlat=max(lats.values)+10\n",
    "        print('here:',minlon,maxlon)\n",
    "#        if minlon<10 and maxlon>350:  #wrapping around meridion need to cal new min/max lon\n",
    "#            minlon=max(lons[lons<180].values)+10\n",
    "#            maxlon=min(lons[lons>180].values)-10\n",
    "#            iwrap=1 #set flag for wraparound\n",
    "\n",
    "        ydim_storm = round((maxlat - minlat)/.25).astype(int)\n",
    "        new_lat_storm = np.linspace(minlat, maxlat, ydim_storm)\n",
    "        if (minlon<-90 and maxlon>=90) or (minlon<-180 and maxlon<0):  #this storm wraps  keep everythig 0 to 360 then wrap data at very end\n",
    "            iwrap = 1\n",
    "            lons2 = np.mod(lons, 360)\n",
    "            #minlon = max(lons[lons<0].values)+10\n",
    "            #maxlon=min(lons[lons>0].values)-10\n",
    "            #minlon += 360\n",
    "            #maxlon2 = maxlon\n",
    "            #minlon, maxlon = maxlon2, minlon\n",
    "            minlon, maxlon = min(lons2.values)-10, max(lons2.values)+10\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "        else:\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "\n",
    "        print(iwrap,minlon,maxlon)\n",
    "        print(iwrap,xdim_storm, new_lon_storm[:5],new_lon_storm[-5:])\n",
    "\n",
    "        \n",
    "        dims=lats.shape\n",
    "        tdim=dims[0]\n",
    "        tem_date=[0]*tdim #print(dysince.values)\n",
    "        for i in range(0,tdim):\n",
    "            tem_date[i]=date_1858+dt.timedelta(days=float(dysince[0,i].values))  #create new time array that can be queried for year etc\n",
    "        minjdy = min(tem_date).timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        minyear =min(tem_date).year #create new time array that can be queried for year etc\n",
    "        minmon =min(tem_date).month #create new time array that can be queried for year etc\n",
    "        minday =min(tem_date).day #create new time array that can be queried for year etc\n",
    "        maxjdy = max(tem_date).timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        maxyear =max(tem_date).year  #create new time array that can be queried for year etc\n",
    "        print(minyear,minjdy,maxyear,maxjdy)\n",
    "        \n",
    "        dif = max(tem_date)-min(tem_date)\n",
    "        tdim=int(dif.days)+30             #calculate ssts for 30 days after storm\n",
    "\n",
    "        #print(tdim,xdim,ydim)      \n",
    "                      \n",
    "        #print('sst_out_sv',sst_out_sv.shape)\n",
    "        for i in range(0,tdim):\n",
    "            storm_date = dt.datetime(minyear,minmon,minday)+dt.timedelta(days=i)+dt.timedelta(hours=12)\n",
    "            #print(storm_date)\n",
    "            \n",
    "            syr=str(storm_date.year)\n",
    "            smon=str(storm_date.month)\n",
    "            sdym=str(storm_date.day)\n",
    "            sjdy=str(storm_date.timetuple().tm_yday)\n",
    "\n",
    "#sst data   \n",
    "            fname_tem=syr + smon.zfill(2) + sdym.zfill(2) + '120000-CMC-L4_GHRSST-SSTfnd-CMC0.2deg-GLOB-v02.0-fv02.0.nc'\n",
    "            filename = dir_cmc + syr + '/' + sjdy.zfill(3) + '/' + fname_tem\n",
    "            ds_day=xr.open_dataset(filename,drop_variables=['analysis_error','sea_ice_fraction'])\n",
    "            if iwrap==1:  #data is -180 to 180 for sst, so need to bring to 0 to 360 when wrapped\n",
    "                ds_day.coords['lon'] = np.mod(ds_day['lon'], 360)\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds_day.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            #ds_storm['time']=storm_date\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            if i==0:\n",
    "                ds_storm_sst = ds_storm\n",
    "            else:\n",
    "                ds_storm_sst = xr.concat([ds_storm_sst,ds_storm],dim='time')\n",
    "\n",
    "#sst climatology\n",
    "            if storm_date.timetuple().tm_yday==366:\n",
    "                sjdy = '365'\n",
    "            filename='F:/data/sst/cmc/CMC0.2deg/v2/climatology/clim1993_2016' + sjdy.zfill(3) + '-CMC-L4_GHRSST-SSTfnd-CMC0.2deg-GLOB-v02.0-fv02.0.nc'\n",
    "            ds_day=xr.open_dataset(filename,drop_variables=['analysis_error','sea_ice_fraction','sq_sst'])\n",
    "            ds_day = ds_day.rename({'analysed_sst':'analysed_sst_clim','mask':'mask_clim'}) #, inplace = True)            \n",
    "            if iwrap==1:  #data is -180 to 180 for sst, so need to bring to 0 to 360 when wrapped\n",
    "                ds_day.coords['lon'] = np.mod(ds_day['lon'], 360)\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds_day.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            if i==0:\n",
    "                ds_storm_sst_clim = ds_storm\n",
    "            else:\n",
    "                ds_storm_sst_clim = xr.concat([ds_storm_sst_clim,ds_storm],dim='time')           \n",
    "            \n",
    "#ccmp wind data\n",
    "            dir_ccmp='F:/data/sat_data/ccmp/v02.0/Y'\n",
    "#            lyr, idyjl = 2015,1\n",
    "#            storm_date = dt.datetime(2015,1,1)\n",
    "            syr, smon, sdym, sjdy=str(storm_date.year),str(storm_date.month),str(storm_date.day),str(storm_date.timetuple().tm_yday)\n",
    "            fname_tem='/CCMP_Wind_Analysis_' + syr + smon.zfill(2) + sdym.zfill(2) + '_V02.0_L3.0_RSS.nc'\n",
    "            ccmp_filename = dir_ccmp + syr + '/M' + smon.zfill(2) + fname_tem      \n",
    "            ds=xr.open_dataset(ccmp_filename,drop_variables=['nobs'])\n",
    "            ds_day = ds.mean(dim='time')     #take average across all 6 hourly data fields\n",
    "            ds_day = ds_day.rename({'longitude':'lon','latitude':'lat'}) #, inplace = True)            \n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if i==0:\n",
    "                ds_storm_ccmp = ds_storm\n",
    "            else:\n",
    "                ds_storm_ccmp = xr.concat([ds_storm_ccmp,ds_storm],dim='time')\n",
    "              \n",
    "                \n",
    "#latent heat flux data                \n",
    "            filename = dir_flux + 'lh_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_lhf = ds_storm\n",
    "            else:\n",
    "                ds_storm_lhf = xr.concat([ds_storm_lhf,ds_storm],dim='time')\n",
    "\n",
    "#sensible heat flux data                \n",
    "            filename = dir_flux + 'sh_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_shf = ds_storm\n",
    "            else:\n",
    "                ds_storm_shf = xr.concat([ds_storm_shf,ds_storm],dim='time')\n",
    "\n",
    "#surface humid flux data                \n",
    "            filename = dir_flux + 'qa_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_qa = ds_storm\n",
    "            else:\n",
    "                ds_storm_qa = xr.concat([ds_storm_qa,ds_storm],dim='time')\n",
    "\n",
    "#air temp flux data                \n",
    "            filename = dir_flux + 'ta_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_ta = ds_storm\n",
    "            else:\n",
    "                ds_storm_ta = xr.concat([ds_storm_ta,ds_storm],dim='time')\n",
    "                \n",
    "        ds_all = xr.merge([ds_storm_ccmp, ds_storm_lhf, ds_storm_shf, ds_storm_ta, ds_storm_qa, ds_storm_sst, ds_storm_sst_clim])\n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "        ds_all.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "     # filename = dir_out + str(iyr_storm) + '/' + 'str(inum_storm)' + '_other_data.nc'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#OLD CODE using netcdf and calculating location directly rather than using xarray\n",
    "###### dir_mur = 'F:/data/sst/jpl_mur/v4.1/'\n",
    "for root, dirs, files in os.walk(dir_in, topdown=False):\n",
    "#    for ii in range(12,13): \n",
    "    for name in files:\n",
    "    #    name = files[ii]\n",
    "#    for name in files:\n",
    "        fname_in=os.path.join(root, name)\n",
    "        fname_out=dir_out + fname_in[31:39] + '_all_25km.nc'\n",
    "        inum_storm=int(fname_in[36:39])\n",
    "        iyr_storm=int(fname_in[31:35])\n",
    "        if iyr_storm>2003 or iyr_storm<2003:\n",
    "            continue\n",
    "#        if iyr_storm==2011 and inum_storm<15:\n",
    "#            continue\n",
    "        print(name,fname_in)\n",
    "        dsx = xr.open_dataset(fname_in)\n",
    "        lats = dsx.lat[0,:]\n",
    "        lons = dsx.lon[0,:]  #lons goes from 0 to 360\n",
    "        dysince = dsx.time\n",
    "        #minlon=min(lons[0,:].values)-10\n",
    "        #maxlon=max(lons[0,:].values)+10\n",
    "        #minlat=min(lats[0,:].values)-10\n",
    "        #maxlat=max(lats[0,:].values)+10\n",
    "        \n",
    "        iwrap=0\n",
    "        minlon=min(lons.values)-10\n",
    "        maxlon=max(lons.values)+10\n",
    "        minlat=min(lats.values)-10\n",
    "        maxlat=max(lats.values)+10\n",
    "        if minlon<10 and maxlon>350:  #wrapping around meridion need to cal new min/max lon\n",
    "            minlon=max(lons[lons<180].values)+10\n",
    "            maxlon=min(lons[lons>180].values)-10\n",
    "            iwrap=1 #set flag for wraparound\n",
    "        \n",
    "        #here is a fix for when a storm goes from 350 across 360 to 1 2 longitude\n",
    "#        iwrap=0\n",
    "#        print('first and last!',lons[0,1].values,lons[0,-1].values)\n",
    "#        if abs(min(lons[0,:].values)-max(lons[0,:].values))>180:\n",
    "#            lons1=lons[0,:].values-10>180\n",
    "#            lons2=lons[0,:].values+10<180\n",
    "#            maxlon=min(lons[0,lons1].values-10)\n",
    "#            minlon=max(lons[0,lons2].values+10)\n",
    "#            print('wrapped',minlon,maxlon)\n",
    "#            iwrap=1\n",
    "            #wrap_lons = ((lons+180) % 360) - 180        \n",
    "            #maxlon=max(wrap_lons[0,:].values)+10 #this will find the positive maximum\n",
    "            #minlon=min(wrap_lons[0,:].values)-10\n",
    "            #if minlon<0:\n",
    "            #    maxlon=min(wrap_lons[0,:].values)-10+360\n",
    "            #    minlon=max(wrap_lons[0,:].values)+10\n",
    "\n",
    "        print('min/max lon lat',minlon,maxlon,minlat,maxlat)\n",
    "\n",
    "        ix1=int(round((minlon-dx_offset)/dx))\n",
    "        ix2=int(round((maxlon-dx_offset)/dx))\n",
    "        iy1=int(round((minlat-dy_offset)/dy))\n",
    "        iy2=int(round((maxlat-dy_offset)/dy))\n",
    "        if iy2 > 628:\n",
    "            iy2=628\n",
    "        if iy1 < 1:\n",
    "            iy1=1    \n",
    "        if ix1 < 0:\n",
    "            ix1 = ix1 + 1440\n",
    "        if ix2 < 0:\n",
    "            ix2 = ix2 + 1440\n",
    "        print(minlon,maxlon,minlat,maxlat)\n",
    "        xdim=ix2-ix1\n",
    "        if iwrap==1:  #wraps around so make sure xdim reflects that\n",
    "            xdim=ix1-ix2+1440\n",
    "        ydim=iy2-iy1\n",
    "        \n",
    "        dims=lats.shape\n",
    "        tdim=dims[0]\n",
    "        tem_date=[0]*tdim #print(dysince.values)\n",
    "        for i in range(0,tdim):\n",
    "            tem_date[i]=date_1858+dt.timedelta(days=float(dysince[0,i].values))  #create new time array that can be queried for year etc\n",
    "        minjdy = min(tem_date).timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        minyear =min(tem_date).year #create new time array that can be queried for year etc\n",
    "        maxjdy = max(tem_date).timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        maxyear =max(tem_date).year  #create new time array that can be queried for year etc\n",
    "        print(minyear,minjdy,maxyear,maxjdy)\n",
    "        \n",
    "        dif = max(tem_date)-min(tem_date)\n",
    "        tdim=int(dif.days)+30\n",
    "\n",
    "\n",
    "        print(tdim,ix1,ix2,iy1,iy2,xdim,ydim)      \n",
    "               \n",
    "        sst_out_sv= np.zeros([tdim,ydim,xdim], dtype=\"float\")\n",
    "        sst_clim_out_sv= np.zeros([tdim,ydim,xdim], dtype=\"float\")\n",
    "        wndu_out_sv= np.zeros([tdim,ydim,xdim], dtype=\"float\")\n",
    "        wndv_out_sv= np.zeros([tdim,ydim,xdim], dtype=\"float\")\n",
    "        wndu_clim_out_sv= np.zeros([tdim,ydim,xdim], dtype=\"float\")\n",
    "        wndv_clim_out_sv= np.zeros([tdim,ydim,xdim], dtype=\"float\")\n",
    "        \n",
    "        print('sst_out_sv',sst_out_sv.shape)\n",
    "        for i in range(0,tdim):\n",
    "            storm_date = tem_date[0]+dt.timedelta(days=i)\n",
    "            #print(storm_date)\n",
    "            \n",
    "            syr=str(storm_date.year)\n",
    "            smon=str(storm_date.month)\n",
    "            sdym=str(storm_date.day)\n",
    "            sjdy=str(storm_date.timetuple().tm_yday)\n",
    "            \n",
    "            fname_tem=syr + smon.zfill(2) + sdym.zfill(2) + '090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc'\n",
    "            mur_filename = dir_mur + syr + '/' + sjdy.zfill(3) + '/' + fname_tem\n",
    "            \n",
    "            fname_tem='/CCMP_Wind_Analysis_' + syr + smon.zfill(2) + sdym.zfill(2) + '_V02.0_L3.0_RSS.nc'\n",
    "            ccmp_filename = dir_ccmp + syr + '/M' + smon.zfill(2) + fname_tem      \n",
    "\n",
    "            #flux data\n",
    "            lh_flux_filename = dir_flux + 'lh_oaflux_' + syr + '.nc';\n",
    "            sh_flux_filename = dir_flux + 'sh_oaflux_' + syr + '.nc';\n",
    "            ta_flux_filename = dir_flux + 'ta_oaflux_' + syr + '.nc';\n",
    "            qa_flux_filename = dir_flux + 'qa_oaflux_' + syr + '.nc';\n",
    "  #          fname='F:\\data\\model_data\\oaflux\\data_v3\\daily\\radiation_1985-2009\\sw_isccp_2004.nc';\n",
    " #           [nswrs]=ncread(fname,'nswrs',[1 1 idy],[360 180 1]);\n",
    "            \n",
    "            if storm_date.timetuple().tm_yday==366:\n",
    "                sjdy = '365'\n",
    "            clim_filename='F:/data/sst/jpl_mur/v4.1/clim/clim2_' + sjdy.zfill(3) +'_2003_2013_MUR-GLOB-v02.0-fv04.1.nc'\n",
    "            ccmp_clim_filename='F:/data/sat_data/ccmp/v02.0/clim/ccmp_daily_clim_' + sjdy.zfill(3) +'.nc'\n",
    "#            print(ccmp_filename)\n",
    "#            print(ccmp_clim_filename)\n",
    "\n",
    "#ccmp wind            \n",
    "            nc_fid = Dataset(ccmp_filename, 'r')\n",
    "            nc_fid2 = Dataset(ccmp_clim_filename, 'r') \n",
    "            tem = nc_fid.variables['uwnd'][:,iy1:iy2,:]  #read in data all longitude, limited latitude\n",
    "            tem = np.mean(tem,axis=0)                     #take average across all 6 hourly data fields\n",
    "            wndu = np.append(tem[:,ydim:],tem[:,:ydim], axis=1) #switch from 0-360 to -180 to 180  ydim is half of xdim\n",
    "            tem = nc_fid.variables['vwnd'][:,iy1:iy2,:]\n",
    "            tem = np.mean(tem,axis=0)\n",
    "            wndv = np.append(tem[:,ydim:],tem[:,:ydim], axis=1)               \n",
    "            mlat_ccmp = nc_fid.variables['latitude'][iy1:iy2]\n",
    "            tem = nc_fid.variables['longitude'][:]\n",
    "            mlon_ccmp = np.append(tem[ydim:],tem[:ydim], axis=0)  \n",
    "            mlon_save = mlon_ccmp[:]\n",
    "            mlon_ccmp = ((mlon_ccmp - 180) % 360) - 180  #make -180 to 180 rather than 0 360\n",
    "            tem = nc_fid2.variables['av_u'][iy1:iy2,:]\n",
    "            wndu_clim = np.append(tem[:,ydim:],tem[:,:ydim], axis=1)               \n",
    "            tem = nc_fid2.variables['av_v'][iy1:iy2,:]\n",
    "            wndv_clim = np.append(tem[:,ydim:],tem[:,:ydim], axis=1)               \n",
    "            nc_fid.close()\n",
    "            nc_fid2.close()           \n",
    "\n",
    "\n",
    "            #flux data\n",
    "            ds = xr.open_dataset(lh_flux_filename)\n",
    "            ds_subset = ds.sel(time = idyjl)\n",
    "            ds_res = ds_subset.interp(latitude = new_lat,longitude = mlon_save)\n",
    "           # nc_fid4 = Dataset(sh_flux_filename, 'r')\n",
    "           # nc_fid5 = Dataset(ta_flux_filename, 'r')\n",
    "           # nc_fid6 = Dataset(qa_flux_filename, 'r')\n",
    "\n",
    "            #[lhf1]=ncread(fname,'lhtfl',[1 1 idy],[360 180 1]);\n",
    "            #[Tair1]=ncread(fname,'tmp2m',[1 1 idy],[360 180 1]);\n",
    "            #[Qair1]=ncread(fname,'hum2m',[1 1 idy],[360 180 1]);  \n",
    "            #[shf1]=ncread(fname,'shtfl',[1 1 idy],[360 180 1]);\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "#            if i==0:\n",
    "#                print('i=0',iy1,iy2,ix1,ix2,iy2-iy1,ix2-ix1)\n",
    "            if ix1<=1440 and ix2<=1440 and iwrap==0:\n",
    "#                if i==0:\n",
    "#                    print('inside1',iy1,iy2,ix1,ix2)\n",
    "                wndu_out = wndu[:,ix1:ix2]\n",
    "                wndu_clim_out = wndu_clim[:,ix1:ix2]\n",
    "                wndv_out = wndv[:,ix1:ix2]\n",
    "                wndv_clim_out = wndv_clim[:,ix1:ix2]\n",
    "            if ix1>1440 and ix2>1440 and iwrap==0:\n",
    " #               if i==0:\n",
    "#                    print('inside2',iy1,iy2,ix1,ix2)\n",
    "                wndu_out = wndu[:,ix1-1440:ix2-1440]\n",
    "                wndu_clim_out = wndu_clim[:,ix1-1440:ix2-1440]\n",
    "                wndv_out = wndv[:,ix1-1440:ix2-1440]\n",
    "                wndv_clim_out = wndv_clim[:,ix1-1440:ix2-1440]\n",
    "            if ix1<=1440 and ix2>1440 and iwrap==0:\n",
    "#                if i==0:\n",
    "#                    print('inside3',iy1,iy2,ix1,ix2)\n",
    "\n",
    "                tem1 = wndu[:,ix1:]\n",
    "                tem2 = wndu[:,:ix2-1440]\n",
    "                wndu_out = np.append(tem1,tem2, axis=1)\n",
    "                tem1 = wndv[:,ix1:]\n",
    "                tem2 = wndv[:,:ix2-1440]\n",
    "                wndv_out = np.append(tem1,tem2, axis=1)\n",
    "                tem1 = wndu_clim[:,ix1:]\n",
    "                tem2 = wndu_clim[:,:ix2-1440]\n",
    "                wndu_clim_out = np.append(tem1,tem2, axis=1)\n",
    "                tem1 = wndv_clim[:,ix1:]\n",
    "                tem2 = wndv_clim[:,:ix2-1440]\n",
    "                wndv_clim_out = np.append(tem1,tem2, axis=1)\n",
    "\n",
    "            if ix1<=1440 and ix2>1440 and iwrap==1:\n",
    "#                if i==0:\n",
    "#                    print('inside1',iy1,iy2,ix1,ix2)\n",
    "                wndu_out = wndu[:,ix2-1440:ix1]\n",
    "                wndu_clim_out = wndu_clim[:,ix2-1440:ix1]\n",
    "                wndv_out = wndv[:,ix2-1440:ix1]\n",
    "                wndv_clim_out = wndv_clim[:,ix2-1440:ix1]\n",
    "\n",
    "            wndu_out_sv[i,:,:]=wndu_out\n",
    "            wndv_out_sv[i,:,:]=wndv_out\n",
    "            wndu_clim_out_sv[i,:,:]=wndu_clim_out\n",
    "            wndv_clim_out_sv[i,:,:]=wndv_clim_out\n",
    "            \n",
    "#sst data   \n",
    "\n",
    "            nc_fid = Dataset(mur_filename, 'r')\n",
    "            mlat = nc_fid.variables['lat'][1149:16849]\n",
    "            ilat_mur1 = np.argmin(abs(mlat-mlat_ccmp.min()))-12\n",
    "            #print('mlat first point:', mlat[ilat_mur1_tem])\n",
    "            ilat_mur2 = np.argmin(abs(mlat-mlat_ccmp.max()))+13\n",
    "            mlat = mlat[ilat_mur1:ilat_mur2]\n",
    "            sst = nc_fid.variables['analysed_sst'][0,ilat_mur1:ilat_mur2,:]\n",
    "            mlon = nc_fid.variables['lon'][:]\n",
    "            nc_fid.close()\n",
    "\n",
    "            nc_fid2 = Dataset(clim_filename, 'r')                      \n",
    "            sst_clim = nc_fid2.variables['sst'][ilat_mur1:ilat_mur2,:]\n",
    "            nc_fid2.close()\n",
    "                       \n",
    "            coarseness = 25\n",
    "            temp = mlon.reshape((mlon.shape[0] // coarseness, coarseness))\n",
    "            coarse_mlon = np.mean(temp, axis=(1), dtype=np.float64)\n",
    "            temp = mlat.reshape((mlat.shape[0] // coarseness, coarseness))\n",
    "            coarse_mlat = np.mean(temp, axis=(1), dtype=np.float64)\n",
    "            temp = sst.reshape((sst.shape[0] // coarseness, coarseness, sst.shape[1] // coarseness, coarseness))\n",
    "            coarse_sst = np.mean(temp, axis=(1,3), dtype=np.float64)\n",
    "            temp = sst_clim.reshape((sst_clim.shape[0] // coarseness, coarseness, sst_clim.shape[1] // coarseness, coarseness))\n",
    "            coarse_sst_clim = np.mean(temp, axis=(1,3), dtype=np.float64)\n",
    "\n",
    "            #need to recalculate iy1 and iy2 because of offset made earlier to read less of file\n",
    "#            iy1=np.argmin(abs(coarse_mlat-minlat))\n",
    "#            iy2=np.argmin(abs(coarse_mlat-maxlat))\n",
    "#            ydim=iy2-iy1   \n",
    "#            print(coarse_mlat[0],coarse_mlat[-1])\n",
    "#            print(iy1,iy2,ydim,minlat,maxlat)\n",
    "\n",
    "#            if i==0:\n",
    "#                print('i=0',ix1,ix2,iy2-iy1,ix2-ix1)\n",
    "            if ix1<=1440 and ix2<=1440 and iwrap==0:\n",
    "#                if i==0:\n",
    "#                    print('inside1',ix1,ix2)\n",
    "                sst_out = coarse_sst[:,ix1:ix2]\n",
    "                mlat_out = coarse_mlat[:]\n",
    "                mlon_out = coarse_mlon[ix1:ix2]\n",
    "                sst_clim_out = coarse_sst_clim[:,ix1:ix2]\n",
    "            if ix1>1440 and ix2>1440 and iwrap==0:\n",
    "#                if i==0:\n",
    "#                    print('inside2',ix1,ix2)\n",
    "                sst_out = coarse_sst[:,ix1-1440:ix2-1440]\n",
    "                mlat_out = coarse_mlat[:]\n",
    "                mlon_out = coarse_mlon[ix1-1440:ix2-1440]\n",
    "                sst_clim_out = coarse_sst_clim[:,ix1-1440:ix2-1440]\n",
    "            if ix1<=1440 and ix2>1440 and iwrap==0:\n",
    "#                if i==0:\n",
    "#                    print('inside3',ix1,ix2)\n",
    "                tem1 = coarse_sst[:,ix1:]\n",
    "                tem2 = coarse_sst[:,:ix2-1440]\n",
    "                sst_out = np.append(tem1,tem2, axis=1)\n",
    "                mlat_out = coarse_mlat[:]\n",
    "                mlon1 = coarse_mlon[ix1:]\n",
    "                mlon2 = coarse_mlon[:ix2-1440]\n",
    "                print(mlon1.shape,mlon2.shape)\n",
    "                mlon_out = np.append(mlon1,mlon2, axis=0)               \n",
    "                tem1 = coarse_sst_clim[:,ix1:]\n",
    "                tem2 = coarse_sst_clim[:,:ix2-1440]\n",
    "                sst_clim_out = np.append(tem1,tem2, axis=1)\n",
    "\n",
    "            if ix1<=1440 and ix2>1440 and iwrap==1:\n",
    " #               if i==0:\n",
    " #                   print('inside1',ix1,ix2)\n",
    "                sst_out = coarse_sst[:,ix2-1440:ix1]\n",
    "                mlat_out = coarse_mlat[:]\n",
    "                mlon_out = coarse_mlon[ix2-1440:ix1]\n",
    "                sst_clim_out = coarse_sst_clim[:,ix2-1440:ix1]\n",
    "\n",
    "            if i==0:\n",
    "                print('sst',sst_out.shape,'coarse',coarse_sst.shape,wndu_out.shape)\n",
    "                print('sst',sst_out.shape,'sst_sv',sst_out_sv.shape)\n",
    "                print('mlon',mlon_out.shape,'mlat',mlat_out.shape)\n",
    "            #sst_sv[i,:,:]=sst-sst_clim\n",
    "            sst_out_sv[i,:,:]=sst_out\n",
    "            sst_clim_out_sv[i,:,:]=sst_clim_out\n",
    "\n",
    "\n",
    "  \n",
    " \n",
    "            \n",
    "            \n",
    "        ilen=len(fname_in)\n",
    "        \n",
    "        dif_dys=[0]*tdim\n",
    "        for i in range(0,tdim):\n",
    "            dif_dys[i] = i\n",
    "\n",
    "        print('file out:',fname_out)\n",
    "        #f.close()\n",
    "        f = Dataset(fname_out,'w', format='NETCDF4') \n",
    "        tempgrp = f.createGroup('data')\n",
    "        tempgrp.setncattr_string('start time',str(tem_date[0]))\n",
    "        tempgrp.createDimension('t', tdim)\n",
    "        tempgrp.createDimension('y', ydim)\n",
    "        tempgrp.createDimension('x', xdim)\n",
    "\n",
    "    #tem_date[i]\n",
    "        sst_netcdf = tempgrp.createVariable('sst', 'f4', ('t', 'y', 'x'))\n",
    "        sst_clim_netcdf = tempgrp.createVariable('sst_clim', 'f4', ('t', 'y', 'x'))\n",
    "        wndu_netcdf = tempgrp.createVariable('wndu', 'f4', ('t', 'y', 'x'))\n",
    "        wndv_netcdf = tempgrp.createVariable('wndv', 'f4', ('t', 'y', 'x'))\n",
    "        wndu_clim_netcdf = tempgrp.createVariable('wndu_clim', 'f4', ('t', 'y', 'x'))\n",
    "        wndv_clim_netcdf = tempgrp.createVariable('wndv_clim', 'f4', ('t', 'y', 'x'))\n",
    "        longitude = tempgrp.createVariable('lon', 'f4', 'x')\n",
    "        latitude = tempgrp.createVariable('lat', 'f4', 'y')  \n",
    "        time = tempgrp.createVariable('time', 'i4', 't')\n",
    "        \n",
    "        sst_netcdf[:] = sst_out_sv\n",
    "        sst_clim_netcdf[:] = sst_clim_out_sv\n",
    "        wndu_netcdf[:] = wndu_out_sv\n",
    "        wndv_netcdf[:] = wndv_out_sv\n",
    "        wndu_clim_netcdf[:] = wndu_clim_out_sv\n",
    "        wndv_clim_netcdf[:] = wndv_clim_out_sv\n",
    "        latitude[:] = mlat_out\n",
    "        longitude[:] = mlon_out\n",
    "        time[:]=dif_dys\n",
    "        f.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
