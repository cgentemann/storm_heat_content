{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input directories\n",
    "dir_storm_info='f:/data/tc_wakes/database/info/'\n",
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "dir_flux = 'F:/data/model_data/oaflux/data_v3/daily/turbulence/'\n",
    "dir_cmc = 'F:/data/sst/cmc/CMC0.2deg/v2/'\n",
    "dir_ccmp='F:/data/sat_data/ccmp/v02.0/Y'\n",
    "##where to get the data through opendap, use these directories instead\n",
    "#dir_cmc = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/CMC/CMC0.1deg/v3/'\n",
    "#dir_flux = 'http://apdrc.soest.hawaii.edu:80/dods/public_data/WHOI_OAFlux/version3/daily/lh_oaflux/'\n",
    "#the latest ccmp is from www.remss.com but they do not have an opendap server so you can use this instead:\n",
    "#dir_ccmp='https://podaac-opendap.jpl.nasa.gov/opendap/allData/ccmp/L3.0/flk/'\n",
    "\n",
    "#################################################################################\n",
    "import geopy.distance\n",
    "from math import sin, pi\n",
    "from scipy import interpolate\n",
    "\n",
    "#functions for running storm data\n",
    "import sys\n",
    "sys.path.append('C:/Users/gentemann/Google Drive/d_drive/python/storm_heat_content/subroutines/')\n",
    "from storm_masking_routines import interpolate_storm_path\n",
    "from storm_masking_routines import get_dist_grid\n",
    "from storm_masking_routines import closest_dist\n",
    "from storm_masking_routines import calculate_storm_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_year=int(str(sys.argv[1]))\n",
    "print ('processing year:', input_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_year = 2003\n",
    "#input_storm = 2\n",
    "date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "isave_mld_year = 0 #init MLD monthly data read flag\n",
    "for root, dirs, files in os.walk(dir_storm_info, topdown=False):\n",
    "    if root[len(dir_storm_info):len(dir_storm_info)+1]=='.':\n",
    "        continue\n",
    "    for name in files:\n",
    "        if not name.endswith('.nc'):\n",
    "            continue\n",
    "        filename=os.path.join(root, name)\n",
    "        print(filename[36:39],filename[31:35])\n",
    "        inum_storm=int(filename[36:39])\n",
    "        iyr_storm=int(filename[31:35])\n",
    "\n",
    "        if iyr_storm!=input_year:\n",
    "            continue\n",
    "#        if input_storm!=inum_storm:\n",
    "#            continue\n",
    "\n",
    "#        if iyr_storm!=2007: # or iyr_storm<2003:\n",
    "#            continue\n",
    "        print(name,filename)\n",
    "        ds_storm_info = xr.open_dataset(filename)\n",
    "        lats = ds_storm_info.lat[0,:]\n",
    "        lons = ds_storm_info.lon[0,:]  #lons goes from 0 to 360\n",
    "        lons = (lons + 180) % 360 - 180 #put -180 to 180\n",
    "        dysince = ds_storm_info.time\n",
    "        ds_storm_info.close()\n",
    "#        print(ds_storm_info)\n",
    "#        break\n",
    "\n",
    "#make lat and lon of storm onto 25 km grid for below\n",
    "        lons = (((lons - .125)/.25+1).astype(int)-1)*.25+.125\n",
    "        lats = (((lats + 89.875)/.25+1).astype(int)-1)*.25-89.875\n",
    "        \n",
    "        iwrap=0\n",
    "#calculate size of box to get data in\n",
    "        minlon,maxlon = min(lons.values)-10, max(lons.values)+10\n",
    "        minlat,maxlat = min(lats.values)-10, max(lats.values)+10\n",
    "\n",
    "        ydim_storm = round((maxlat - minlat)/.25).astype(int)\n",
    "        new_lat_storm = np.linspace(minlat, maxlat, ydim_storm)\n",
    "        if (minlon<-90 and maxlon>=90) or (minlon<-180 and maxlon<0):  #this storm wraps  keep everythig 0 to 360 then wrap data at very end\n",
    "            iwrap = 1\n",
    "            lons2 = np.mod(lons, 360)\n",
    "            minlon, maxlon = min(lons2.values)-10, max(lons2.values)+10\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "        else:\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "\n",
    "        print(iwrap,minlon,maxlon)\n",
    "        print(iwrap,xdim_storm, new_lon_storm[:5],new_lon_storm[-5:])\n",
    "\n",
    "        \n",
    "        dims=lats.shape\n",
    "        tdim=dims[0]\n",
    "        tem_date=[0]*tdim #print(dysince.values)\n",
    "        for i in range(0,tdim):\n",
    "            tem_date[i]=date_1858+dt.timedelta(days=float(dysince[0,i].values))  #create new time array that can be queried for year etc\n",
    "        min_date = min(tem_date)+dt.timedelta(days=-5)\n",
    "#        max_date = max(tem_date)+dt.timedelta(days=5)\n",
    "        minjdy = min_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        minyear =min_date.year #create new time array that can be queried for year etc\n",
    "        minmon =min_date.month #create new time array that can be queried for year etc\n",
    "        minday =min_date.day #create new time array that can be queried for year etc\n",
    "#        maxjdy = max_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "#        maxyear =max_date.year  #create new time array that can be queried for year etc\n",
    "        print(minyear,minjdy)#,maxyear,maxjdy)\n",
    "        \n",
    "        dif = max(tem_date)-min(tem_date)\n",
    "        tdim=int(dif.days)+45             #calculate ssts for 30 days after storm\n",
    "        \n",
    "        #print(tdim,xdim,ydim)            \n",
    "        \n",
    "        #print('sst_out_sv',sst_out_sv.shape)\n",
    "        for i in range(0,tdim):\n",
    "            storm_date = dt.datetime(minyear,minmon,minday)+dt.timedelta(days=i)+dt.timedelta(hours=12)\n",
    "            #print(storm_date)\n",
    "            \n",
    "            syr=str(storm_date.year)\n",
    "            smon=str(storm_date.month)\n",
    "            sdym=str(storm_date.day)\n",
    "            sjdy=str(storm_date.timetuple().tm_yday)\n",
    "\n",
    "#sst data   \n",
    "            fname_tem=syr + smon.zfill(2) + sdym.zfill(2) + '120000-CMC-L4_GHRSST-SSTfnd-CMC0.2deg-GLOB-v02.0-fv02.0.nc'\n",
    "            filename = dir_cmc + syr + '/' + sjdy.zfill(3) + '/' + fname_tem\n",
    "            ds_day=xr.open_dataset(filename,drop_variables=['analysis_error','sea_ice_fraction'])\n",
    "            if iwrap==1:  #data is -180 to 180 for sst, so need to bring to 0 to 360 when wrapped\n",
    "                ds_day.coords['lon'] = np.mod(ds_day['lon'], 360)\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds_day.close()\n",
    "            ds_day = ds_day.where(ds_day['mask'] == 1.) \n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            #ds_storm['time']=storm_date\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            if i==0:\n",
    "                ds_storm_sst = ds_storm\n",
    "            else:\n",
    "                ds_storm_sst = xr.concat([ds_storm_sst,ds_storm],dim='time')\n",
    "\n",
    "#sst climatology  --- this isn't used, should remove from dataset in next round\n",
    "#            if storm_date.timetuple().tm_yday==366:\n",
    "#                sjdy = '365'\n",
    "#            filename='F:/data/sst/cmc/CMC0.2deg/v2/climatology/clim1993_2016' + sjdy.zfill(3) + '-CMC-L4_GHRSST-SSTfnd-CMC0.2deg-GLOB-v02.0-fv02.0.nc'\n",
    "#            ds_day=xr.open_dataset(filename,drop_variables=['analysis_error','sea_ice_fraction','sq_sst'])\n",
    "#            ds_day = ds_day.rename({'analysed_sst':'analysed_sst_clim','mask':'mask_clim'}) #, inplace = True)            \n",
    "#            if iwrap==1:  #data is -180 to 180 for sst, so need to bring to 0 to 360 when wrapped\n",
    "#                ds_day.coords['lon'] = np.mod(ds_day['lon'], 360)\n",
    "#                ds_day = ds_day.sortby(ds_day.lon)\n",
    "#            ds_day.close()\n",
    "#            ds_day = ds_day.where(ds_day['mask_clim'] == 1.) \n",
    "#            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "#            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "#            if iwrap==1:\n",
    "#                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "#            if i==0:\n",
    "#                ds_storm_sst_clim = ds_storm\n",
    "#            else:\n",
    "#                ds_storm_sst_clim = xr.concat([ds_storm_sst_clim,ds_storm],dim='time')           \n",
    "            \n",
    "#ccmp wind data, no masked data, a complete field\n",
    "#            lyr, idyjl = 2015,1\n",
    "#            storm_date = dt.datetime(2015,1,1)\n",
    "            syr, smon, sdym, sjdy=str(storm_date.year),str(storm_date.month),str(storm_date.day),str(storm_date.timetuple().tm_yday)\n",
    "            fname_tem='/CCMP_Wind_Analysis_' + syr + smon.zfill(2) + sdym.zfill(2) + '_V02.0_L3.0_RSS.nc'\n",
    "            ccmp_filename = dir_ccmp + syr + '/M' + smon.zfill(2) + fname_tem      \n",
    "            ds=xr.open_dataset(ccmp_filename,drop_variables=['nobs'])\n",
    "            ds_day = ds.mean(dim='time')     #take average across all 6 hourly data fields\n",
    "            ds_day = ds_day.rename({'longitude':'lon','latitude':'lat'}) #, inplace = True)            \n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if i==0:\n",
    "                ds_storm_ccmp = ds_storm\n",
    "            else:\n",
    "                ds_storm_ccmp = xr.concat([ds_storm_ccmp,ds_storm],dim='time')\n",
    "              \n",
    "#ocean mixed layer depth from monthly data GODAS NOAA, lon 0 to 360, monthly data so interp to day\n",
    "#this is monthly data (all other data daily) so need to read in year before and year after\n",
    "#so any storms <1/15 or greater than 12/15 in the year still get data\n",
    "#dir_godas='https://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/godas/'\n",
    "            dir_godas = 'f:/data/model_data/godas/'\n",
    "            if isave_mld_year != storm_date.year:\n",
    "                filename = dir_godas + 'dbss_obml.' + str(storm_date.year-1) + '.nc'\n",
    "                ds_day_mld=xr.open_dataset(filename)\n",
    "                ds_day_mld['time']=ds_day_mld.time+np.timedelta64(14,'D')  #data provider gave 1st day of ave in time \n",
    "                ds_day_mld.close()\n",
    "                filename = dir_godas + 'dbss_obml.' + str(storm_date.year) + '.nc'\n",
    "                ds_day_mld2=xr.open_dataset(filename)\n",
    "                ds_day_mld2['time']=ds_day_mld2.time+np.timedelta64(14,'D')  #data provider gave 1st day of ave in time \n",
    "                ds_day_mld2.close()\n",
    "                ds_day_mld = xr.concat([ds_day_mld,ds_day_mld2],dim='time')\n",
    "                filename = dir_godas + 'dbss_obml.' + str(storm_date.year+1) + '.nc'\n",
    "                ds_day_mld2=xr.open_dataset(filename)\n",
    "                ds_day_mld2['time']=ds_day_mld2.time+np.timedelta64(14,'D')  #data provider gave 1st day of ave in time \n",
    "                ds_day_mld2.close()\n",
    "                ds_day_mld = xr.concat([ds_day_mld,ds_day_mld2],dim='time')\n",
    "                if iwrap==0:\n",
    "                    ds_day_mld.coords['lon'] = (ds_day_mld.coords['lon'] + 180) % 360 - 180\n",
    "                    ds_day_mld = ds_day_mld.sortby(ds_day_mld.lon)\n",
    "                isave_mld_year = storm_date.year\n",
    "            ds_storm = ds_day_mld.interp(time = storm_date, lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if i==0:\n",
    "                ds_storm_mld = ds_storm\n",
    "            else:\n",
    "                ds_storm_mld = xr.concat([ds_storm_mld,ds_storm],dim='time')            \n",
    "            \n",
    "#latent heat flux data, masked already set to NaN                \n",
    "            filename = dir_flux + 'lh_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_lhf = ds_storm\n",
    "            else:\n",
    "                ds_storm_lhf = xr.concat([ds_storm_lhf,ds_storm],dim='time')\n",
    "\n",
    "#sensible heat flux data , masked already set to NaN                \n",
    "            filename = dir_flux + 'sh_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_shf = ds_storm\n",
    "            else:\n",
    "                ds_storm_shf = xr.concat([ds_storm_shf,ds_storm],dim='time')\n",
    "\n",
    "#surface humid flux data   , masked already set to NaN              \n",
    "            filename = dir_flux + 'qa_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_qa = ds_storm\n",
    "            else:\n",
    "                ds_storm_qa = xr.concat([ds_storm_qa,ds_storm],dim='time')\n",
    "\n",
    "#air temp flux data   , masked already set to NaN              \n",
    "            filename = dir_flux + 'ta_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_ta = ds_storm\n",
    "            else:\n",
    "                ds_storm_ta = xr.concat([ds_storm_ta,ds_storm],dim='time')\n",
    "                \n",
    "#        ds_all = xr.merge([ds_storm_ccmp, ds_storm_mld, ds_storm_lhf, ds_storm_shf, ds_storm_ta, ds_storm_qa, ds_storm_sst, ds_storm_sst_clim])\n",
    "        ds_all = xr.merge([ds_storm_ccmp, ds_storm_mld, ds_storm_lhf, ds_storm_shf, ds_storm_ta, ds_storm_qa, ds_storm_sst])\n",
    "\n",
    "        #calculate mask\n",
    "#        print('caluculating mask')\n",
    "#        ds_mask = calculate_storm_mask(ds_all,lats,lons)\n",
    "#        ds_all['storm_mask']=ds_mask['storm_mask']\n",
    "#        #dist to storm\n",
    "#        print('calculating dist')\n",
    "#        dist,index,stime,position = closest_dist(ds_all,ds_storm_info)\n",
    "#        dtem=xr.DataArray(dist, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "#        ds_all['dist_from_storm_km']=dtem\n",
    "#        dtem=xr.DataArray(index, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "#        ds_all['closest_storm_index']=dtem\n",
    "#        dtem=xr.DataArray(stime, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "#        ds_all['closest_storm_time']=dtem\n",
    "#        dtem=xr.DataArray(position, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "#        ds_all['side_of_storm']=dtem\n",
    "\n",
    "        if iwrap==1:\n",
    "            ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "            ds_storm_info['lon'] = np.mod(ds_storm_info['lon'], 360)\n",
    "\n",
    "        #calculate mask\n",
    "        print('caluculating mask')\n",
    "        ds_mask = calculate_storm_mask(ds_all,lats,lons)\n",
    "        ds_all['storm_mask']=ds_mask['storm_mask']\n",
    "        #dist to storm\n",
    "        print('calculating dist')\n",
    "        dist,index,stime,position,ds_storm_interp = closest_dist(ds_all,ds_storm_info)\n",
    "        dtem=xr.DataArray(dist, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['dist_from_storm_km']=dtem\n",
    "        dtem=xr.DataArray(index, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['closest_storm_index']=dtem\n",
    "        dtem=xr.DataArray(stime, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['closest_storm_time']=dtem\n",
    "        dtem=xr.DataArray(position, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['side_of_storm']=dtem\n",
    "\n",
    "#add storm translation speed to storm information\n",
    "        tdim_storm = ds_storm_interp.time.size\n",
    "        storm_speed = ds_storm_interp.time.copy(deep=True)*np.nan    \n",
    "        for i in range(0,tdim_storm-1):\n",
    "            coords_1 = (ds_storm_interp.lat[0,i], ds_storm_interp.lon[0,i])  \n",
    "            coords_2 = (ds_storm_interp.lat[0,i+1], ds_storm_interp.lon[0,i+1])  \n",
    "            arclen_temp = geopy.distance.geodesic(coords_1, coords_2).km  #distance in km  \n",
    "            storm_date1 = np.datetime64(date_1858 + dt.timedelta(days=float(ds_storm_interp.time[0,i])))  \n",
    "            storm_date2 = np.datetime64(date_1858 + dt.timedelta(days=float(ds_storm_interp.time[0,i+1])))  \n",
    "            arclen_time = storm_date2 - storm_date1\n",
    "            arclen_hr = arclen_time / np.timedelta64(1, 'h')\n",
    "            storm_speed[0,i]=arclen_temp/(arclen_hr)\n",
    "        storm_speed[0,-1]=storm_speed[0,-2]\n",
    "        ds_storm_interp['storm_speed']=storm_speed\n",
    "        \n",
    "        xdim,ydim,tdim = ds_all.lon.shape[0],ds_all.lat.shape[0],ds_all.time.shape[0]\n",
    "        wtem=np.empty([ydim,xdim])\n",
    "        ptem=np.empty([ydim,xdim])\n",
    "        stem=np.empty([ydim,xdim])\n",
    "        for i in range(0,xdim):\n",
    "            for j in range(0,ydim):\n",
    "                storm_index = ds_all.closest_storm_index[j,i].data\n",
    "                wtem[j,i]=ds_storm_interp.wind[0,int(storm_index)].data\n",
    "                ptem[j,i]=ds_storm_interp.pres[0,int(storm_index)].data\n",
    "                stem[j,i]=ds_storm_interp.storm_speed[0,int(storm_index)].data\n",
    "        xrtem=xr.DataArray(wtem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n",
    "        ds_all['wmo_storm_wind']=xrtem\n",
    "        xrtem=xr.DataArray(ptem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n",
    "        ds_all['wmo_storm_pres']=xrtem\n",
    "        xrtem=xr.DataArray(stem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n",
    "        ds_all['wmo_storm_speed']=xrtem\n",
    "        \n",
    "        #find max sst 5 days before storm location\n",
    "        #first create an array with the storm crossover time (from nearest point) as an array\n",
    "        sdate = np.empty([ydim,xdim], dtype=dt.datetime)    \n",
    "        for i in range(0,xdim):\n",
    "            for j in range(0,ydim):\n",
    "                tem=date_1858+dt.timedelta(days=float(ds_all.closest_storm_time[j,i])) \n",
    "                sdate[j,i]=np.datetime64(tem)\n",
    "        xsdate=xr.DataArray(sdate, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))    \n",
    "        ds_all['closest_storm_time_np64']=xsdate\n",
    "        #now use array of storm time to calculate prestorm sst\n",
    "        sst0 = ds_all.dist_from_storm_km.copy(deep=True)\n",
    "        for i in range(0,xdim):\n",
    "            for j in range(0,ydim):\n",
    "                #sst0[j,i] = ds_data.analysed_sst[:,j,i].interp(time=xsdate[j,i])\n",
    "                sst0[j,i] = ds_all.analysed_sst[:,j,i].sel(time=slice(xsdate[j,i]-np.timedelta64(5,'D'),xsdate[j,i])).max()\n",
    "        ds_all['sst_prestorm']=sst0\n",
    "\n",
    "#now calculate coldwake information\n",
    "        if abs(ds_all.lon[-1]-ds_all.lon[0])>180:\n",
    "            ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "            ds_storm_interp['lon'] = np.mod(ds_storm_interp['lon'], 360)\n",
    "        max_lat = ds_storm_interp.lat.max()\n",
    "\n",
    "    #remove all data outsice 100km/800km or cold wake >0 or <-10\n",
    "        if max_lat<0:\n",
    "            cond = ((((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm<=0)) | \n",
    "            ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm>0))) )       \n",
    "        else:\n",
    "            cond = ((((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm<0)) | \n",
    "            ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm>=0))))            \n",
    "        subset = ds_all.where(cond)\n",
    "          \n",
    "        xdim,ydim,tdim = ds_all.lon.shape[0],ds_all.lat.shape[0],ds_all.time.shape[0]\n",
    "        date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "        coldwake_max=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "        coldwake_maxindex=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "        coldwake_hrtomaxcold=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "        coldwake_recovery=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "#go through entire array lat/lon dims\n",
    "        for i in range(0,xdim):\n",
    "            for j in range(0,ydim):\n",
    "                 #calculate the storm time for the closest collocated storm point then find the combined data index for closest time\n",
    "                #this gives you the combined data storm index cross over\n",
    "                storm_date64 = ds_all.closest_storm_time_np64[j,i]\n",
    "                if np.isnan(subset.analysed_sst[0,j,i]):  #don't process masked values\n",
    "                    continue\n",
    "                time_diff = subset.time-storm_date64\n",
    "                storm_index = np.argmin(abs(time_diff)).data\n",
    "                #now look for cold wake for 1 day before strom to 5 days after strom\n",
    "                #caluclate hours to cold wake, maximum cold wake, hours until it returns to prestorm sst\n",
    "                #there is NO filter on wheither coldwake large enough here, just does all points\n",
    "                istart,iend = int(storm_index),int(storm_index)+5\n",
    "                if iend>tdim:\n",
    "                    iend=tdim\n",
    "                if np.isnan(subset.sst_prestorm[j,i]):\n",
    "                    continue\n",
    "                coldwake_max[j,i] = (subset.analysed_sst[istart:iend,j,i]-subset.sst_prestorm[j,i]).min()\n",
    "                itmp = np.argmin(subset.analysed_sst[istart:iend,j,i]-subset.sst_prestorm[j,i]).data\n",
    "                coldwake_maxindex[j,i]=istart+itmp\n",
    "                delay = subset.time[istart+itmp].values-subset.time[istart].values\n",
    "                coldwake_hrtomaxcold[j,i]=delay / np.timedelta64(1, 'h')\n",
    "                for k in range(istart+itmp,tdim):\n",
    "                    sst_change = subset.analysed_sst[k,j,i]-subset.sst_prestorm[j,i]\n",
    "                    if sst_change>-0.2:\n",
    "                        break\n",
    "                delay = subset.time[k].values-subset.time[istart].values\n",
    "                coldwake_recovery[j,i]=delay / np.timedelta64(1, 'D')\n",
    "\n",
    "        ds_all['coldwake_max']=coldwake_max\n",
    "        ds_all['coldwake_maxindex']=coldwake_maxindex\n",
    "        ds_all['coldwake_hrtomaxcold']=coldwake_hrtomaxcold\n",
    "        ds_all['coldwake_dytorecovery']=coldwake_recovery\n",
    "        \n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "        ds_all.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "        ds_storm_interp.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_all.dbss_obml[50,:,:]-ds_all.dbss_obml[10,:,:]).plot()\n",
    "#plt.plot(ds_storm_info.lon[0,:],ds_storm_info.lat[0,:],'r')\n",
    "#plt.plot(ds_storm_info.lon[0,0],ds_storm_info.lat[0,0],'r*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_all.dbss_obml[:,60,60].data)\n",
    "#ds_all.dbss_obml[53,:,:].plot()\n",
    "plt.plot(ds_all.dbss_obml[:,60,60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all.time.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(istart)\n",
    "print(itmp)\n",
    "print(k)\n",
    "print(subset.time[k].data,subset.time[istart].data)\n",
    "print(coldwake_recovery[j,i].data)\n",
    "plt.plot(subset.analysed_sst[istart:,j,i]-subset.sst_prestorm[j,i],'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldwake_recovery.plot(vmin=0,vmax=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(coldwake_max,vmin=-1,vmax=1)\n",
    "#coldwake_max.plot()\n",
    "#plt.plot(ds_storm_interp.lon[0,:]+360,ds_storm_interp.lat[0,:],'r')\n",
    "#plt.plot(ds_storm_interp.lon[0,0]+360,ds_storm_interp.lat[0,0],'r*')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.analysed_sst[:,60,210].plot()\n",
    "plt.plot(subset.time[istart],subset.analysed_sst[istart,60,240],'r*')\n",
    "plt.plot(subset.time[istart],subset.analysed_sst[istart,60,240],'r*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.time[istart]\n",
    "print(subset.lon[240].data,subset.lat[60].data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_storm_interp.wind[0,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_storm_interp.time[0,25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_storm_interp.time[0,0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            dir_godas = 'f:/data/model_data/godas/'\n",
    "            if isave_mld_year != storm_date.year:\n",
    "                filename = dir_godas + 'dbss_obml.' + syr + '.nc'\n",
    "                ds_day_mld=xr.open_dataset(filename)\n",
    "                ds_day_mld['time']=ds_day_mld.time+np.timedelta64(14,'D')  #data provider gave 1st day of ave in time \n",
    "                if iwrap==0:\n",
    "                    ds_day_mld.coords['lon'] = (ds_day_mld.coords['lon'] + 180) % 360 - 180\n",
    "                    ds_day_mld = ds_day_mld.sortby(ds_day_mld.lon)\n",
    "                ds_day_mld.close()\n",
    "                isave_mld_year = storm_date.year\n",
    "            ds_storm = ds_day_mld.interp(time = storm_date, lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if i==0:\n",
    "                ds_storm_mld = ds_storm\n",
    "            else:\n",
    "                ds_storm_mld = xr.concat([ds_storm_mld,ds_storm],dim='time')            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_day_mld.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
