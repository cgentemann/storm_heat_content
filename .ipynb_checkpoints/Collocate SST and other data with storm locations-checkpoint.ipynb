{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import cartopy.crs as ccrs\n",
    "dir_storm_wmo='F:/data/tc_wakes/ibtracks/year/'\n",
    "\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input directories\n",
    "dir_storm_info='f:/data/tc_wakes/database/info/'\n",
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "dir_flux = 'F:/data/model_data/oaflux/data_v3/daily/turbulence/'\n",
    "dir_cmc = 'F:/data/sst/cmc/CMC0.2deg/v2/'\n",
    "dir_ccmp='F:/data/sat_data/ccmp/v02.0/Y'\n",
    "##where to get the data through opendap, use these directories instead\n",
    "#dir_cmc = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/CMC/CMC0.1deg/v3/'\n",
    "#dir_flux = 'http://apdrc.soest.hawaii.edu:80/dods/public_data/WHOI_OAFlux/version3/daily/lh_oaflux/'\n",
    "#the latest ccmp is from www.remss.com but they do not have an opendap server so you can use this instead:\n",
    "#dir_ccmp='https://podaac-opendap.jpl.nasa.gov/opendap/allData/ccmp/L3.0/flk/'\n",
    "\n",
    "#################################################################################\n",
    "import geopy.distance\n",
    "from math import sin, pi\n",
    "from scipy import interpolate\n",
    "\n",
    "#functions for running storm data\n",
    "import sys\n",
    "sys.path.append('C:/Users/gentemann/Google Drive/d_drive/python/storm_heat_content/subroutines/')\n",
    "from storm_masking_routines import interpolate_storm_path\n",
    "from storm_masking_routines import get_dist_grid\n",
    "from storm_masking_routines import closest_dist\n",
    "from storm_masking_routines import calculate_storm_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_year=int(str(sys.argv[1]))\n",
    "print ('processing year:', input_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001 2002\n",
      "002 2002\n",
      "005 2002\n",
      "007 2002\n",
      "009 2002\n",
      "010 2002\n",
      "011 2002\n",
      "012 2002\n",
      "013 2002\n",
      "014 2002\n",
      "015 2002\n",
      "017 2002\n",
      "018 2002\n",
      "019 2002\n",
      "020 2002\n",
      "021 2002\n",
      "022 2002\n",
      "023 2002\n",
      "025 2002\n",
      "027 2002\n",
      "028 2002\n",
      "030 2002\n",
      "031 2002\n",
      "033 2002\n",
      "034 2002\n",
      "035 2002\n",
      "036 2002\n",
      "037 2002\n",
      "039 2002\n",
      "040 2002\n",
      "041 2002\n",
      "042 2002\n",
      "043 2002\n",
      "046 2002\n",
      "047 2002\n",
      "048 2002\n",
      "049 2002\n",
      "050 2002\n",
      "051 2002\n",
      "052 2002\n",
      "053 2002\n",
      "054 2002\n",
      "055 2002\n",
      "056 2002\n",
      "058 2002\n",
      "060 2002\n",
      "061 2002\n",
      "063 2002\n",
      "064 2002\n",
      "066 2002\n",
      "067 2002\n",
      "068 2002\n",
      "069 2002\n",
      "070 2002\n",
      "072 2002\n",
      "075 2002\n",
      "077 2002\n",
      "081 2002\n",
      "082 2002\n",
      "083 2002\n",
      "084 2002\n",
      "002 2003\n",
      "003 2003\n",
      "004 2003\n",
      "005 2003\n",
      "006 2003\n",
      "008 2003\n",
      "009 2003\n",
      "010 2003\n",
      "011 2003\n",
      "012 2003\n",
      "013 2003\n",
      "015 2003\n",
      "016 2003\n",
      "017 2003\n",
      "018 2003\n",
      "019 2003\n",
      "021 2003\n",
      "022 2003\n",
      "023 2003\n",
      "024 2003\n",
      "025 2003\n",
      "026 2003\n",
      "027 2003\n",
      "028 2003\n",
      "031 2003\n",
      "032 2003\n",
      "033 2003\n",
      "034 2003\n",
      "035 2003\n",
      "036 2003\n",
      "037 2003\n",
      "038 2003\n",
      "040 2003\n",
      "042 2003\n",
      "044 2003\n",
      "046 2003\n",
      "047 2003\n",
      "048 2003\n",
      "049 2003\n",
      "050 2003\n",
      "051 2003\n",
      "055 2003\n",
      "057 2003\n",
      "059 2003\n",
      "060 2003\n",
      "064 2003\n",
      "065 2003\n",
      "066 2003\n",
      "067 2003\n",
      "070 2003\n",
      "071 2003\n",
      "072 2003\n",
      "073 2003\n",
      "075 2003\n",
      "076 2003\n",
      "077 2003\n",
      "078 2003\n",
      "079 2003\n",
      "080 2003\n",
      "081 2003\n",
      "082 2003\n",
      "083 2003\n",
      "086 2003\n",
      "087 2003\n",
      "088 2003\n",
      "089 2003\n",
      "091 2003\n",
      "092 2003\n",
      "093 2003\n",
      "094 2003\n",
      "095 2003\n",
      "097 2003\n",
      "001 2004\n",
      "002 2004\n",
      "003 2004\n",
      "004 2004\n",
      "005 2004\n",
      "006 2004\n",
      "007 2004\n",
      "008 2004\n",
      "009 2004\n",
      "011 2004\n",
      "012 2004\n",
      "013 2004\n",
      "014 2004\n",
      "015 2004\n",
      "016 2004\n",
      "017 2004\n",
      "018 2004\n",
      "019 2004\n",
      "021 2004\n",
      "022 2004\n",
      "023 2004\n",
      "024 2004\n",
      "025 2004\n",
      "026 2004\n",
      "028 2004\n",
      "029 2004\n",
      "030 2004\n",
      "031 2004\n",
      "032 2004\n",
      "033 2004\n",
      "034 2004\n",
      "037 2004\n",
      "038 2004\n",
      "039 2004\n",
      "040 2004\n",
      "041 2004\n",
      "043 2004\n",
      "044 2004\n",
      "045 2004\n",
      "046 2004\n",
      "047 2004\n",
      "048 2004\n",
      "049 2004\n",
      "051 2004\n",
      "052 2004\n",
      "053 2004\n",
      "054 2004\n",
      "055 2004\n",
      "058 2004\n",
      "059 2004\n",
      "060 2004\n",
      "062 2004\n",
      "063 2004\n",
      "064 2004\n",
      "065 2004\n",
      "066 2004\n",
      "068 2004\n",
      "069 2004\n",
      "071 2004\n",
      "072 2004\n",
      "073 2004\n",
      "074 2004\n",
      "075 2004\n",
      "076 2004\n",
      "081 2004\n",
      "084 2004\n",
      "087 2004\n",
      "089 2004\n",
      "091 2004\n",
      "092 2004\n",
      "093 2004\n",
      "002 2005\n",
      "003 2005\n",
      "004 2005\n",
      "005 2005\n",
      "006 2005\n",
      "007 2005\n",
      "009 2005\n",
      "010 2005\n",
      "011 2005\n",
      "012 2005\n",
      "014 2005\n",
      "015 2005\n",
      "016 2005\n",
      "017 2005\n",
      "018 2005\n",
      "019 2005\n",
      "020 2005\n",
      "021 2005\n",
      "022 2005\n",
      "023 2005\n",
      "024 2005\n",
      "025 2005\n",
      "026 2005\n",
      "027 2005\n",
      "028 2005\n",
      "029 2005\n",
      "031 2005\n",
      "034 2005\n",
      "035 2005\n",
      "038 2005\n",
      "042 2005\n",
      "043 2005\n",
      "044 2005\n",
      "045 2005\n",
      "047 2005\n",
      "048 2005\n",
      "049 2005\n",
      "053 2005\n",
      "055 2005\n",
      "056 2005\n",
      "057 2005\n",
      "058 2005\n",
      "060 2005\n",
      "061 2005\n",
      "062 2005\n",
      "063 2005\n",
      "065 2005\n",
      "066 2005\n",
      "067 2005\n",
      "068 2005\n",
      "069 2005\n",
      "070 2005\n",
      "071 2005\n",
      "072 2005\n",
      "073 2005\n",
      "074 2005\n",
      "078 2005\n",
      "079 2005\n",
      "081 2005\n",
      "082 2005\n",
      "083 2005\n",
      "084 2005\n",
      "085 2005\n",
      "086 2005\n",
      "087 2005\n",
      "088 2005\n",
      "093 2005\n",
      "095 2005\n",
      "096 2005\n",
      "097 2005\n",
      "099 2005\n",
      "101 2005\n",
      "102 2005\n",
      "103 2005\n",
      "104 2005\n",
      "106 2005\n",
      "107 2005\n",
      "108 2005\n",
      "109 2005\n",
      "110 2005\n",
      "111 2005\n",
      "001 2006\n",
      "002 2006\n",
      "003 2006\n",
      "006 2006\n",
      "007 2006\n",
      "008 2006\n",
      "009 2006\n",
      "010 2006\n",
      "012 2006\n",
      "014 2006\n",
      "015 2006\n",
      "016 2006\n",
      "017 2006\n",
      "018 2006\n",
      "019 2006\n",
      "021 2006\n",
      "022 2006\n",
      "023 2006\n",
      "026 2006\n",
      "027 2006\n",
      "028 2006\n",
      "029 2006\n",
      "030 2006\n",
      "031 2006\n",
      "032 2006\n",
      "033 2006\n",
      "034 2006\n",
      "037 2006\n",
      "038 2006\n",
      "040 2006\n",
      "041 2006\n",
      "042 2006\n",
      "043 2006\n",
      "044 2006\n",
      "045 2006\n",
      "047 2006\n",
      "049 2006\n",
      "050 2006\n",
      "052 2006\n",
      "053 2006\n",
      "054 2006\n",
      "055 2006\n",
      "056 2006\n",
      "057 2006\n",
      "058 2006\n",
      "060 2006\n",
      "061 2006\n",
      "062 2006\n",
      "064 2006\n",
      "065 2006\n",
      "066 2006\n",
      "067 2006\n",
      "068 2006\n",
      "070 2006\n",
      "072 2006\n",
      "073 2006\n",
      "075 2006\n",
      "076 2006\n",
      "077 2006\n",
      "078 2006\n",
      "079 2006\n",
      "080 2006\n",
      "083 2006\n",
      "086 2006\n",
      "087 2006\n",
      "088 2006\n",
      "089 2006\n",
      "001 2007\n",
      "002 2007\n",
      "003 2007\n",
      "004 2007\n",
      "005 2007\n",
      "007 2007\n",
      "008 2007\n",
      "009 2007\n",
      "010 2007\n",
      "011 2007\n",
      "012 2007\n",
      "013 2007\n",
      "014 2007\n",
      "015 2007\n",
      "016 2007\n",
      "017 2007\n",
      "018 2007\n",
      "020 2007\n",
      "021 2007\n",
      "022 2007\n",
      "024 2007\n",
      "026 2007\n",
      "027 2007\n",
      "028 2007\n",
      "030 2007\n",
      "031 2007\n",
      "035 2007\n",
      "037 2007\n",
      "039 2007\n",
      "040 2007\n",
      "041 2007\n",
      "043 2007\n",
      "045 2007\n",
      "046 2007\n",
      "047 2007\n",
      "048 2007\n",
      "049 2007\n",
      "050 2007\n",
      "052 2007\n",
      "053 2007\n",
      "055 2007\n",
      "056 2007\n",
      "058 2007\n",
      "059 2007\n",
      "060 2007\n",
      "066 2007\n",
      "067 2007\n",
      "068 2007\n",
      "069 2007\n",
      "070 2007\n",
      "071 2007\n",
      "072 2007\n",
      "073 2007\n",
      "074 2007\n",
      "075 2007\n",
      "076 2007\n",
      "078 2007\n",
      "080 2007\n",
      "081 2007\n",
      "082 2007\n",
      "083 2007\n",
      "002 2008\n",
      "003 2008\n",
      "004 2008\n",
      "005 2008\n",
      "006 2008\n",
      "007 2008\n",
      "008 2008\n",
      "009 2008\n",
      "010 2008\n",
      "012 2008\n",
      "013 2008\n",
      "014 2008\n",
      "015 2008\n",
      "016 2008\n",
      "017 2008\n",
      "018 2008\n",
      "019 2008\n",
      "020 2008\n",
      "021 2008\n",
      "022 2008\n",
      "023 2008\n",
      "024 2008\n",
      "027 2008\n",
      "028 2008\n",
      "029 2008\n",
      "031 2008\n",
      "036 2008\n",
      "037 2008\n",
      "038 2008\n",
      "040 2008\n",
      "042 2008\n",
      "043 2008\n",
      "044 2008\n",
      "046 2008\n",
      "047 2008\n",
      "048 2008\n",
      "049 2008\n",
      "051 2008\n",
      "052 2008\n",
      "055 2008\n",
      "056 2008\n",
      "057 2008\n",
      "058 2008\n",
      "060 2008\n",
      "061 2008\n",
      "062 2008\n",
      "063 2008\n",
      "065 2008\n",
      "066 2008\n",
      "068 2008\n",
      "069 2008\n",
      "070 2008\n",
      "071 2008\n",
      "072 2008\n",
      "074 2008\n",
      "075 2008\n",
      "077 2008\n",
      "079 2008\n",
      "086 2008\n",
      "087 2008\n",
      "088 2008\n",
      "093 2008\n",
      "001 2009\n",
      "002 2009\n",
      "003 2009\n",
      "004 2009\n",
      "005 2009\n",
      "006 2009\n",
      "008 2009\n",
      "009 2009\n",
      "011 2009\n",
      "013 2009\n",
      "014 2009\n",
      "016 2009\n",
      "017 2009\n",
      "018 2009\n",
      "020 2009\n",
      "023 2009\n",
      "025 2009\n",
      "026 2009\n",
      "027 2009\n",
      "028 2009\n",
      "029 2009\n",
      "030 2009\n",
      "032 2009\n",
      "038 2009\n",
      "040 2009\n",
      "041 2009\n",
      "045 2009\n",
      "046 2009\n",
      "047 2009\n",
      "048 2009\n",
      "049 2009\n",
      "050 2009\n",
      "052 2009\n",
      "053 2009\n",
      "054 2009\n",
      "055 2009\n",
      "057 2009\n",
      "060 2009\n",
      "061 2009\n",
      "062 2009\n",
      "065 2009\n",
      "066 2009\n",
      "068 2009\n",
      "071 2009\n",
      "072 2009\n",
      "073 2009\n",
      "074 2009\n",
      "076 2009\n",
      "078 2009\n",
      "079 2009\n",
      "080 2009\n",
      "081 2009\n",
      "083 2009\n",
      "084 2009\n",
      "085 2009\n",
      "086 2009\n",
      "087 2009\n",
      "089 2009\n",
      "090 2009\n",
      "001 2010\n",
      "002 2010\n",
      "003 2010\n",
      "004 2010\n",
      "005 2010\n",
      "007 2010\n",
      "008 2010\n",
      "010 2010\n",
      "012 2010\n",
      "015 2010\n",
      "016 2010\n",
      "017 2010\n",
      "018 2010\n",
      "019 2010\n",
      "020 2010\n",
      "020annual_storm_info_extended.nc f:/data/tc_wakes/database/info/2010\\020annual_storm_info_extended.nc\n",
      "1 170.625 244.875\n",
      "1 297 [170.625      170.87584459 171.12668919 171.37753378 171.62837838] [243.87162162 244.12246622 244.37331081 244.62415541 244.875     ]\n",
      "2010 64\n"
     ]
    }
   ],
   "source": [
    "input_year = 2010\n",
    "input_storm = 20\n",
    "date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "isave_mld_year = 0 #init MLD monthly data read flag\n",
    "for root, dirs, files in os.walk(dir_storm_info, topdown=False):\n",
    "    if root[len(dir_storm_info):len(dir_storm_info)+1]=='.':\n",
    "        continue\n",
    "    for name in files:\n",
    "        if not name.endswith('.nc'):\n",
    "            continue\n",
    "        filename=os.path.join(root, name)\n",
    "        print(filename[36:39],filename[31:35])\n",
    "        inum_storm=int(filename[36:39])\n",
    "        iyr_storm=int(filename[31:35])\n",
    "\n",
    "        if iyr_storm!=input_year:\n",
    "            continue\n",
    "        if input_storm!=inum_storm:\n",
    "            continue\n",
    "\n",
    "#        if iyr_storm!=2007: # or iyr_storm<2003:\n",
    "#            continue\n",
    "        print(name,filename)\n",
    "        ds_storm_info = xr.open_dataset(filename)\n",
    "        lats = ds_storm_info.lat[0,:]\n",
    "        lons = ds_storm_info.lon[0,:]  #lons goes from 0 to 360\n",
    "        lons = (lons + 180) % 360 - 180 #put -180 to 180\n",
    "        dysince = ds_storm_info.time\n",
    "        ds_storm_info.close()\n",
    "#        print(ds_storm_info)\n",
    "#        break\n",
    "#        ds_storm_interp = interpolate_storm_path(ds_storm_info)\n",
    "#        print(ds_storm_interp)\n",
    "#        break\n",
    "\n",
    "#make lat and lon of storm onto 25 km grid for below\n",
    "        lons = (((lons - .125)/.25+1).astype(int)-1)*.25+.125\n",
    "        lats = (((lats + 89.875)/.25+1).astype(int)-1)*.25-89.875\n",
    "        \n",
    "        iwrap=0\n",
    "#calculate size of box to get data in\n",
    "        minlon,maxlon = min(lons.values)-10, max(lons.values)+10\n",
    "        minlat,maxlat = min(lats.values)-10, max(lats.values)+10\n",
    "\n",
    "        ydim_storm = round((maxlat - minlat)/.25).astype(int)\n",
    "        new_lat_storm = np.linspace(minlat, maxlat, ydim_storm)\n",
    "        if (minlon<-90 and maxlon>=90) or (minlon<-180 and maxlon<0):  #this storm wraps  keep everythig 0 to 360 then wrap data at very end\n",
    "            iwrap = 1\n",
    "            lons2 = np.mod(lons, 360)\n",
    "            minlon, maxlon = min(lons2.values)-10, max(lons2.values)+10\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "        else:\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "\n",
    "        print(iwrap,minlon,maxlon)\n",
    "        print(iwrap,xdim_storm, new_lon_storm[:5],new_lon_storm[-5:])\n",
    "\n",
    "        \n",
    "        dims=lats.shape\n",
    "        tdim=dims[0]\n",
    "        tem_date=[0]*tdim #print(dysince.values)\n",
    "        for i in range(0,tdim):\n",
    "            tem_date[i]=date_1858+dt.timedelta(days=float(dysince[0,i].values))  #create new time array that can be queried for year etc\n",
    "        min_date = min(tem_date)+dt.timedelta(days=-5)\n",
    "#        max_date = max(tem_date)+dt.timedelta(days=5)\n",
    "        minjdy = min_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        minyear =min_date.year #create new time array that can be queried for year etc\n",
    "        minmon =min_date.month #create new time array that can be queried for year etc\n",
    "        minday =min_date.day #create new time array that can be queried for year etc\n",
    "#        maxjdy = max_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "#        maxyear =max_date.year  #create new time array that can be queried for year etc\n",
    "        print(minyear,minjdy)#,maxyear,maxjdy)\n",
    "        \n",
    "        dif = max(tem_date)-min(tem_date)\n",
    "        tdim=int(dif.days)+45             #calculate ssts for 30 days after storm\n",
    "        \n",
    "        #print(tdim,xdim,ydim)            \n",
    "        \n",
    "        #print('sst_out_sv',sst_out_sv.shape)\n",
    "        for i in range(0,tdim):\n",
    "            storm_date = dt.datetime(minyear,minmon,minday)+dt.timedelta(days=i)+dt.timedelta(hours=12)\n",
    "            #print(storm_date)\n",
    "            \n",
    "            syr=str(storm_date.year)\n",
    "            smon=str(storm_date.month)\n",
    "            sdym=str(storm_date.day)\n",
    "            sjdy=str(storm_date.timetuple().tm_yday)\n",
    "\n",
    "#sst data   \n",
    "            fname_tem=syr + smon.zfill(2) + sdym.zfill(2) + '120000-CMC-L4_GHRSST-SSTfnd-CMC0.2deg-GLOB-v02.0-fv02.0.nc'\n",
    "            filename = dir_cmc + syr + '/' + sjdy.zfill(3) + '/' + fname_tem\n",
    "            ds_day=xr.open_dataset(filename,drop_variables=['analysis_error','sea_ice_fraction'])\n",
    "            if iwrap==1:  #data is -180 to 180 for sst, so need to bring to 0 to 360 when wrapped\n",
    "                ds_day.coords['lon'] = np.mod(ds_day['lon'], 360)\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds_day.close()\n",
    "            ds_day = ds_day.where(ds_day['mask'] == 1.) \n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            #ds_storm['time']=storm_date\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            if i==0:\n",
    "                ds_storm_sst = ds_storm\n",
    "            else:\n",
    "                ds_storm_sst = xr.concat([ds_storm_sst,ds_storm],dim='time')\n",
    "\n",
    "#sst climatology  \n",
    "            if storm_date.timetuple().tm_yday==366:\n",
    "                sjdy = '365'\n",
    "            filename='F:/data/sst/cmc/CMC0.2deg/v2/climatology/clim1993_2016' + sjdy.zfill(3) + '-CMC-L4_GHRSST-SSTfnd-CMC0.2deg-GLOB-v02.0-fv02.0.nc'\n",
    "            ds_day=xr.open_dataset(filename,drop_variables=['analysis_error','sea_ice_fraction','sq_sst'])\n",
    "            ds_day = ds_day.rename({'analysed_sst':'analysed_sst_clim','mask':'mask_clim'}) #, inplace = True)            \n",
    "            if iwrap==1:  #data is -180 to 180 for sst, so need to bring to 0 to 360 when wrapped\n",
    "                ds_day.coords['lon'] = np.mod(ds_day['lon'], 360)\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds_day.close()\n",
    "            ds_day = ds_day.where(ds_day['mask_clim'] == 1.) \n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            if i==0:\n",
    "                ds_storm_sst_clim = ds_storm\n",
    "            else:\n",
    "                ds_storm_sst_clim = xr.concat([ds_storm_sst_clim,ds_storm],dim='time')           \n",
    "            \n",
    "#ccmp wind data, no masked data, a complete field\n",
    "#            lyr, idyjl = 2015,1\n",
    "#            storm_date = dt.datetime(2015,1,1)\n",
    "            syr, smon, sdym, sjdy=str(storm_date.year),str(storm_date.month),str(storm_date.day),str(storm_date.timetuple().tm_yday)\n",
    "            fname_tem='/CCMP_Wind_Analysis_' + syr + smon.zfill(2) + sdym.zfill(2) + '_V02.0_L3.0_RSS.nc'\n",
    "            ccmp_filename = dir_ccmp + syr + '/M' + smon.zfill(2) + fname_tem      \n",
    "            ds=xr.open_dataset(ccmp_filename,drop_variables=['nobs'])\n",
    "            ds_day = ds.mean(dim='time')     #take average across all 6 hourly data fields\n",
    "            ds_day = ds_day.rename({'longitude':'lon','latitude':'lat'}) #, inplace = True)            \n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if i==0:\n",
    "                ds_storm_ccmp = ds_storm\n",
    "            else:\n",
    "                ds_storm_ccmp = xr.concat([ds_storm_ccmp,ds_storm],dim='time')\n",
    "              \n",
    "#ocean mixed layer depth from monthly data GODAS NOAA, lon 0 to 360, monthly data so interp to day\n",
    "#this is monthly data (all other data daily) so need to read in year before and year after\n",
    "#so any storms <1/15 or greater than 12/15 in the year still get data\n",
    "#dir_godas='https://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/godas/'\n",
    "            dir_godas = 'f:/data/model_data/godas/'\n",
    "            if isave_mld_year != storm_date.year:\n",
    "                filename = dir_godas + 'dbss_obml.' + str(storm_date.year-1) + '.nc'\n",
    "                ds_day_mld=xr.open_dataset(filename)\n",
    "                ds_day_mld['time']=ds_day_mld.time+np.timedelta64(14,'D')  #data provider gave 1st day of ave in time \n",
    "                ds_day_mld.close()\n",
    "                filename = dir_godas + 'dbss_obml.' + str(storm_date.year) + '.nc'\n",
    "                ds_day_mld2=xr.open_dataset(filename)\n",
    "                ds_day_mld2['time']=ds_day_mld2.time+np.timedelta64(14,'D')  #data provider gave 1st day of ave in time \n",
    "                ds_day_mld2.close()\n",
    "                ds_day_mld = xr.concat([ds_day_mld,ds_day_mld2],dim='time')\n",
    "                filename = dir_godas + 'dbss_obml.' + str(storm_date.year+1) + '.nc'\n",
    "                ds_day_mld2=xr.open_dataset(filename)\n",
    "                ds_day_mld2['time']=ds_day_mld2.time+np.timedelta64(14,'D')  #data provider gave 1st day of ave in time \n",
    "                ds_day_mld2.close()\n",
    "                ds_day_mld = xr.concat([ds_day_mld,ds_day_mld2],dim='time')\n",
    "                if iwrap==0:\n",
    "                    ds_day_mld.coords['lon'] = (ds_day_mld.coords['lon'] + 180) % 360 - 180\n",
    "                    ds_day_mld = ds_day_mld.sortby(ds_day_mld.lon)\n",
    "                isave_mld_year = storm_date.year\n",
    "            ds_storm = ds_day_mld.interp(time = storm_date, lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if i==0:\n",
    "                ds_storm_mld = ds_storm\n",
    "            else:\n",
    "                ds_storm_mld = xr.concat([ds_storm_mld,ds_storm],dim='time')            \n",
    "            \n",
    "#latent heat flux data, masked already set to NaN                \n",
    "            filename = dir_flux + 'lh_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_lhf = ds_storm\n",
    "            else:\n",
    "                ds_storm_lhf = xr.concat([ds_storm_lhf,ds_storm],dim='time')\n",
    "\n",
    "#sensible heat flux data , masked already set to NaN                \n",
    "            filename = dir_flux + 'sh_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_shf = ds_storm\n",
    "            else:\n",
    "                ds_storm_shf = xr.concat([ds_storm_shf,ds_storm],dim='time')\n",
    "\n",
    "#surface humid flux data   , masked already set to NaN              \n",
    "            filename = dir_flux + 'qa_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_qa = ds_storm\n",
    "            else:\n",
    "                ds_storm_qa = xr.concat([ds_storm_qa,ds_storm],dim='time')\n",
    "\n",
    "#air temp flux data   , masked already set to NaN              \n",
    "            filename = dir_flux + 'ta_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_ta = ds_storm\n",
    "            else:\n",
    "                ds_storm_ta = xr.concat([ds_storm_ta,ds_storm],dim='time')\n",
    "                \n",
    "#        ds_all = xr.merge([ds_storm_ccmp, ds_storm_mld, ds_storm_lhf, ds_storm_shf, ds_storm_ta, ds_storm_qa, ds_storm_sst, ds_storm_sst_clim])\n",
    "        ds_all = xr.merge([ds_storm_ccmp, ds_storm_mld, ds_storm_lhf, ds_storm_shf, ds_storm_ta, ds_storm_qa, ds_storm_sst,ds_storm_sst_clim])\n",
    "\n",
    "        #calculate mask\n",
    "#        print('caluculating mask')\n",
    "#        ds_mask = calculate_storm_mask(ds_all,lats,lons)\n",
    "#        ds_all['storm_mask']=ds_mask['storm_mask']\n",
    "#        #dist to storm\n",
    "#        print('calculating dist')\n",
    "#        dist,index,stime,position = closest_dist(ds_all,ds_storm_info)\n",
    "#        dtem=xr.DataArray(dist, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "#        ds_all['dist_from_storm_km']=dtem\n",
    "#        dtem=xr.DataArray(index, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "#        ds_all['closest_storm_index']=dtem\n",
    "#        dtem=xr.DataArray(stime, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "#        ds_all['closest_storm_time']=dtem\n",
    "#        dtem=xr.DataArray(position, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "#        ds_all['side_of_storm']=dtem\n",
    "\n",
    "        if iwrap==1:\n",
    "            ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "            ds_storm_info['lon'] = np.mod(ds_storm_info['lon'], 360)\n",
    "\n",
    "        #calculate mask\n",
    "        print('caluculating mask')\n",
    "        ds_mask = calculate_storm_mask(ds_all,lats,lons)\n",
    "        ds_all['storm_mask']=ds_mask['storm_mask']\n",
    "        #dist to storm\n",
    "        print('calculating dist')\n",
    "        dist,index,stime,position,ds_storm_interp = closest_dist(ds_all,ds_storm_info)\n",
    "        dtem=xr.DataArray(dist, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['dist_from_storm_km']=dtem\n",
    "        dtem=xr.DataArray(index, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['closest_storm_index']=dtem\n",
    "        dtem=xr.DataArray(stime, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['closest_storm_time']=dtem\n",
    "        dtem=xr.DataArray(position, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['side_of_storm']=dtem\n",
    "       \n",
    "        xdim,ydim,tdim = ds_all.lon.shape[0],ds_all.lat.shape[0],ds_all.time.shape[0]\n",
    "        wtem=np.empty([ydim,xdim])\n",
    "        ptem=np.empty([ydim,xdim])\n",
    "        stem=np.empty([ydim,xdim])\n",
    "        for i in range(0,xdim):\n",
    "            for j in range(0,ydim):\n",
    "                storm_index = ds_all.closest_storm_index[j,i].data\n",
    "                wtem[j,i]=ds_storm_interp.wind[0,int(storm_index)].data\n",
    "                ptem[j,i]=ds_storm_interp.pres[0,int(storm_index)].data\n",
    "                stem[j,i]=ds_storm_interp.storm_speed_kmhr[0,int(storm_index)].data\n",
    "        xrtem=xr.DataArray(wtem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n",
    "        ds_all['wmo_storm_wind']=xrtem\n",
    "        xrtem=xr.DataArray(ptem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n",
    "        ds_all['wmo_storm_pres']=xrtem\n",
    "        xrtem=xr.DataArray(stem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n",
    "        ds_all['wmo_storm_speed_kmhr']=xrtem\n",
    "        \n",
    "        #find max sst 5 days before storm location\n",
    "        #first create an array with the storm crossover time (from nearest point) as an array\n",
    "        sdate = np.empty([ydim,xdim], dtype=dt.datetime)    \n",
    "        for i in range(0,xdim):\n",
    "            for j in range(0,ydim):\n",
    "                tem=date_1858+dt.timedelta(days=float(ds_all.closest_storm_time[j,i])) \n",
    "                sdate[j,i]=np.datetime64(tem)\n",
    "        xsdate=xr.DataArray(sdate, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))    \n",
    "        ds_all['closest_storm_time_np64']=xsdate\n",
    "        #now use array of storm time to calculate prestorm sst\n",
    "        sst0 = ds_all.dist_from_storm_km.copy(deep=True)\n",
    "        sst0_clim_anom = ds_all.dist_from_storm_km.copy(deep=True)\n",
    "        for i in range(0,xdim):\n",
    "            for j in range(0,ydim):\n",
    "                #sst0[j,i] = ds_data.analysed_sst[:,j,i].interp(time=xsdate[j,i])\n",
    "                sst0[j,i] = ds_all.analysed_sst[:,j,i].sel(time=slice(xsdate[j,i]-np.timedelta64(5,'D'),xsdate[j,i])).max()\n",
    "                sst0_clim_anom[j,i] = (ds_all.analysed_sst[:,j,i]-ds_all.analysed_sst_clim[:,j,i]).sel(time=slice(xsdate[j,i]-np.timedelta64(5,'D'),xsdate[j,i])).max()\n",
    "        ds_all['sst_prestorm']=sst0\n",
    "        ds_all['sst_prestorm_clim']=sst0_clim_anom\n",
    "#create sst anomalies\n",
    "        ds_all['sst_anom']=ds_all.analysed_sst-ds_all.analysed_sst_clim\n",
    "#now calculate coldwake information\n",
    "        if abs(ds_all.lon[-1]-ds_all.lon[0])>180:\n",
    "            ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "            ds_storm_interp['lon'] = np.mod(ds_storm_interp['lon'], 360)\n",
    "        max_lat = ds_storm_interp.lat.max()\n",
    "\n",
    "    #remove all data outsice 100km/800km or cold wake >0 or <-10\n",
    "        if max_lat<0:\n",
    "            cond = ((((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm<=0)) | \n",
    "            ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm>0))) )       \n",
    "        else:\n",
    "            cond = ((((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm<0)) | \n",
    "            ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm>=0))))            \n",
    "        subset = ds_all.where(cond)\n",
    "          \n",
    "        xdim,ydim,tdim = ds_all.lon.shape[0],ds_all.lat.shape[0],ds_all.time.shape[0]\n",
    "        date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "        coldwake_max=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "        coldwake_maxindex=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "        coldwake_hrtomaxcold=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "        coldwake_recovery=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "#go through entire array lat/lon dims\n",
    "        for i in range(0,xdim):\n",
    "            for j in range(0,ydim):\n",
    "                 #calculate the storm time for the closest collocated storm point then find the combined data index for closest time\n",
    "                #this gives you the combined data storm index cross over\n",
    "                storm_date64 = ds_all.closest_storm_time_np64[j,i]\n",
    "                if np.isnan(subset.analysed_sst[0,j,i]):  #don't process masked values\n",
    "                    continue\n",
    "                time_diff = subset.time-storm_date64\n",
    "                storm_index = np.argmin(abs(time_diff)).data\n",
    "                #now look for cold wake for 1 day before strom to 5 days after strom\n",
    "                #caluclate hours to cold wake, maximum cold wake, hours until it returns to prestorm sst\n",
    "                #there is NO filter on wheither coldwake large enough here, just does all points\n",
    "                istart,iend = int(storm_index),int(storm_index)+8\n",
    "                if iend>tdim:\n",
    "                    iend=tdim\n",
    "                if np.isnan(subset.sst_prestorm_clim[j,i]):\n",
    "                    continue\n",
    "                #coldwake_max[j,i] = (subset.analysed_sst[istart:iend,j,i]-subset.sst_prestorm[j,i]).min()\n",
    "                coldwake_max[j,i] = (subset.sst_anom[istart:iend,j,i]-subset.sst_prestorm_clim[j,i]).min()\n",
    "                if all(np.isnan(subset.sst_anom[istart:iend,j,i])):\n",
    "                    continue\n",
    "                itmp = np.argmin(subset.sst_anom[istart:iend,j,i]-subset.sst_prestorm_clim[j,i]).data\n",
    "                coldwake_maxindex[j,i]=istart+itmp\n",
    "                delay = subset.time[istart+itmp].values-subset.time[istart].values\n",
    "                coldwake_hrtomaxcold[j,i]=delay / np.timedelta64(1, 'h')\n",
    "                for k in range(istart+itmp,tdim):\n",
    "                    sst_change = subset.sst_anom[k,j,i]-subset.sst_prestorm_clim[j,i]\n",
    "                   # sst_change_clim = subset.analysed_sst_clim[k,j,i]-subset.sst_prestorm_clim[j,i]\n",
    "            #NEED TO ADD CLIMATOLOGY SST CHANGE HERE\n",
    "                    if sst_change>-0.1:  #changed 2/27 based on dare and mcbride paper criteria\n",
    "                        break\n",
    "                delay = subset.time[k].values-subset.time[istart].values\n",
    "                coldwake_recovery[j,i]=delay / np.timedelta64(1, 'D')\n",
    "\n",
    "        ds_all['coldwake_max']=coldwake_max\n",
    "        ds_all['coldwake_maxindex']=coldwake_maxindex\n",
    "        ds_all['coldwake_hrtomaxcold']=coldwake_hrtomaxcold\n",
    "        ds_all['coldwake_dytorecovery']=coldwake_recovery\n",
    "        \n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "        ds_all.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "        ds_storm_interp.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all.dbss_obml[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all.analysed_sst[0,:,:].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iyr_storm=2010\n",
    "inum_storm=20\n",
    "filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "ds_old = xr.open_dataset(filename)\n",
    "ds_old.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_old.dbss_obml[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "for iyr_storm in range(2010,2017):\n",
    "#inum_storm = 55\n",
    "#for iyr_storm in range(2003,2011):\n",
    "    filename=dir_storm_wmo+'Year.'+str(iyr_storm)+'.ibtracs_wmo.v03r10.nc'\n",
    "    ds_ibtrak = xr.open_dataset(filename)\n",
    "    ds_ibtrak.close()\n",
    "    for inum_storm in range(0,100): #0,100):#100): #(0,100): #100):\n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "        exists = os.path.isfile(filename)\n",
    "        if exists:       \n",
    "            print(filename)\n",
    "            ds_storm_info=xr.open_dataset(filename)\n",
    "            ds_storm_info = ds_storm_info.sel(j2=0)\n",
    "            ds_storm_info.close()\n",
    "            filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "            ds_all = xr.open_dataset(filename)\n",
    "            ds_all['spd']=np.sqrt(ds_all.uwnd**2+ds_all.vwnd**2)\n",
    "            ds_all.close()\n",
    "            icenter=0\n",
    "\n",
    "    #        if ds_all.lon.max()>180:\n",
    "    #            ds_all.coords['lon'] = (ds_all.coords['lon'] + 180) % 360 - 180\n",
    "\n",
    "            if ((abs(ds_storm_info.lon[-1]-ds_storm_info.lon[0])>180) | ((ds_all.lon[-1].data>180) & (ds_all.lon[0].data<180))):\n",
    "    #            ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "    #            ds_storm_info['lon'] = np.mod(ds_storm_info['lon'], 360)\n",
    "                icenter=-180\n",
    "            max_lat = ds_storm_info.lat.max()\n",
    "            #remove all data outsice 100km/800km or cold wake >0 or <-10\n",
    "            if max_lat<0:\n",
    "                cond = ((((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm<=0)) | \n",
    "                ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm>0))) \n",
    "                & (ds_all.coldwake_max<=-.1) & (ds_all.coldwake_max>=-10))\n",
    "            else:\n",
    "                cond = ((((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm<0)) | \n",
    "                ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm>=0))) \n",
    "                & (ds_all.coldwake_max<=-.1) & (ds_all.coldwake_max>=-10))\n",
    "            subset = ds_all.where(cond)\n",
    "\n",
    "            #create coldwake anomaly with nan for all values before wmo storm time\n",
    "            subset['sst_anomaly']=subset.analysed_sst-subset.sst_prestorm\n",
    "\n",
    "            frac = ds_all.lon.size/ds_all.lat.size*1.2\n",
    "\n",
    "            #create array with day.frac since closest storm passage\n",
    "            tdif_dy = (subset.time-subset.closest_storm_time_np64)/np.timedelta64(1, 'D')\n",
    "            subset['tdif_dy']=tdif_dy\n",
    "\n",
    "            plt.figure(1,figsize=(4, 3),dpi=100)\n",
    "            gs1 = gridspec.GridSpec(1, 3)\n",
    "            gs1.update(wspace=0.025, hspace=0.05) # set the spacing between axes. \n",
    "\n",
    "            ax = plt.subplot(gs1[0],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "            ax.set_global(), ax.coastlines()\n",
    "            cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, (ds_all.analysed_sst-ds_all.sst_prestorm).min('time'),vmin=-5,vmax=5,cmap='seismic')  \n",
    "            ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'g.',markersize=.5)\n",
    "            ax.plot(ds_storm_info.lon+icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "     #       plt.arrow(ds_storm_info.lon[3], ds_storm_info.lat[3], ds_storm_info.lon[4]-ds_storm_info.lon[3], \n",
    "     #                 ds_storm_info.lat[4]-ds_storm_info.lat[3],color='g',width=.01,head_width=1)\n",
    "     #       ax.annotate(\"\", xy=(0.5, 0.5), xytext=(0, 0),arrowprops=dict(arrowstyle=\"->\"))\n",
    "            ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "            cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "            cb.set_label('SST (K)',size=8),cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "            ax = plt.subplot(gs1[1],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "            ax.set_global(), ax.coastlines()\n",
    "            cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, ds_all.spd.max('time'))  \n",
    "            ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.plot(ds_storm_info.lon+icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.set_title(str(ds_ibtrak.name[inum_storm-1].data)[2:-1])      \n",
    "            ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "            cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "            cb.set_label('wind speed (m/s)',size=8),cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "            ax = plt.subplot(gs1[2],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "            ax.set_global(), ax.coastlines()\n",
    "            cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, ds_all.dbss_obml[0,:,:])  \n",
    "            ax.plot(ds_storm_info.lon+icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "            cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "            cb.set_label('MLD (m)',size=8),cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "            dir_figs = 'f:/data/tc_wakes/database/figs/data_images/'\n",
    "            plt.savefig(dir_figs+str(iyr_storm)+'_'+str(inum_storm)+'data.png', bbox_inches='tight',pad_inches = 0, dpi = 200)\n",
    "            plt.clf()\n",
    "\n",
    "            plt.figure(2,figsize=(4, 3),dpi=100)\n",
    "            gs1 = gridspec.GridSpec(1, 3)\n",
    "            gs1.update(wspace=0.025, hspace=0.05) # set the spacing between axes. \n",
    "\n",
    "            ax = plt.subplot(gs1[0],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "            ax.set_global(), ax.coastlines()\n",
    "            cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, (ds_all.dist_from_storm_km),cmap='seismic')  \n",
    "            ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'g.',markersize=.5)\n",
    "            ax.plot(ds_storm_info.lon+icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "            cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "            cb.set_label('dist. from storm (km)',size=8),cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "            ax = plt.subplot(gs1[1],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "            ax.set_global(), ax.coastlines()\n",
    "            cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, ds_all.side_of_storm)  \n",
    "            ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.plot(ds_storm_info.lon+icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "            ax.set_title(str(ds_ibtrak.name[inum_storm-1].data)[2:-1])      \n",
    "            cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "    #        cb = plt.colorbar(cs, ax=ax, shrink=frac,orientation=\"horizontal\",pad=0.02)\n",
    "            cb.set_label('side of storm',size=8),cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "            ax = plt.subplot(gs1[2],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "            ax.set_global(), ax.coastlines()\n",
    "            cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, ds_all.coldwake_max,vmin=-3,vmax=0)  \n",
    "            ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.plot(ds_storm_info.lon+icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "    #        ax.set_xlim([ds_all.lon[0]+icenter*2-icenter/180*20,ds_all.lon[-1]+icenter*2]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "            cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "            cb.set_label('max coldwake (K)',size=8),cb.ax.tick_params(labelsize=6)\n",
    "            #plt.colorbar(cs,fraction=0.046, pad=0.04,orientation='horizontal')\n",
    "\n",
    "            dir_figs = 'f:/data/tc_wakes/database/figs/data_images/'\n",
    "            plt.savefig(dir_figs+str(iyr_storm)+'_'+str(inum_storm)+'info.png', bbox_inches='tight',pad_inches = 0, dpi = 200)\n",
    "            plt.clf()\n",
    "\n",
    "            print(str(ds_ibtrak.name[inum_storm-1].data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        plt.figure(1,figsize=(4, 3),dpi=100)\n",
    "        gs1 = gridspec.GridSpec(1, 3)\n",
    "        gs1.update(wspace=0.025, hspace=0.05) # set the spacing between axes. \n",
    "\n",
    "        ax = plt.subplot(gs1[0],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "        ax.set_global(), ax.coastlines()\n",
    "        cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, (ds_all.analysed_sst-ds_all.sst_prestorm).min('time'),vmin=-5,vmax=5,cmap='seismic')  \n",
    "        ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'g.',markersize=1)\n",
    " #       plt.arrow(ds_storm_info.lon[3], ds_storm_info.lat[3], ds_storm_info.lon[4]-ds_storm_info.lon[3], \n",
    " #                 ds_storm_info.lat[4]-ds_storm_info.lat[3],color='g',width=.01,head_width=1)\n",
    " #       ax.annotate(\"\", xy=(0.5, 0.5), xytext=(0, 0),arrowprops=dict(arrowstyle=\"->\"))\n",
    "        ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "        cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "        cb.set_label('SST (K)',size=8),cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "        ax = plt.subplot(gs1[1],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "        ax.set_global(), ax.coastlines()\n",
    "        cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, ds_all.spd.max('time'))  \n",
    "        ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'r.',markersize=1)\n",
    "        ax.set_title(str(ds_ibtrak.name[inum_storm-1].data)[2:-1])      \n",
    "        ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "        cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "        cb.set_label('wind speed (m/s)',size=8),cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "        ax = plt.subplot(gs1[2],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "        ax.set_global(), ax.coastlines()\n",
    "        cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, ds_all.dbss_obml[0,:,:])  \n",
    "        ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'r.',markersize=1)\n",
    "        ax.plot(ds_storm_info.lon+icenter,ds_storm_info.lat,'r.',markersize=1)\n",
    "        ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "        cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "        cb.set_label('MLD (m)',size=8),cb.ax.tick_params(labelsize=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all.coldwake_max.plot()\n",
    "plt.plot(ds_storm_info.lon,ds_storm_info.lat,'r.',markersize=.5)\n",
    "print(icenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_all.lon[-1].data,ds_all.lon[0].data,(ds_all.lon[-1]-ds_all.lon[0]).data)\n",
    "if ((ds_all.lon[-1].data>180) & (ds_all.lon[0].data<180)):\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = plt.axes(projection=ccrs.Orthographic(-80, 35))\n",
    "ds_all.analysed_sst[0,:,:].plot.contourf(ax=ax, transform=ccrs.PlateCarree());\n",
    "ax.set_global(); ax.coastlines();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(12, 3),dpi=100)\n",
    "\n",
    "ax = plt.subplot(131,projection=ccrs.PlateCarree())\n",
    "ax.set_global(), ax.coastlines()\n",
    "cs=ax.pcolormesh(ds_all.lon,ds_all.lat, (ds_all.dist_from_storm_km),cmap='seismic')  \n",
    "ax.plot(ds_storm_info.lon,ds_storm_info.lat,'g')\n",
    "ax.set_xlim([ds_all.lon[0],ds_all.lon[-1]]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "cb = fig.colorbar(cs, ax=ax, shrink=0.9)\n",
    "cb.set_label('distance from storm')\n",
    "\n",
    "ax = plt.subplot(132,projection=ccrs.PlateCarree())\n",
    "ax.set_global(), ax.coastlines()\n",
    "cs=ax.pcolormesh(ds_all.lon,ds_all.lat, ds_all.side_of_storm)  \n",
    "ax.plot(ds_storm_info.lon,ds_storm_info.lat,'r')\n",
    "ax.set_xlim([ds_all.lon[0],ds_all.lon[-1]]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "cb = fig.colorbar(cs, ax=ax, shrink=0.9)\n",
    "cb.set_label('side of storm')\n",
    "\n",
    "ax = plt.subplot(133,projection=ccrs.PlateCarree())\n",
    "ax.set_global(), ax.coastlines()\n",
    "cs=ax.pcolormesh(ds_all.lon,ds_all.lat, ds_all.coldwake_max,vmin=-3,vmax=0)  \n",
    "ax.plot(ds_storm_info.lon,ds_storm_info.lat,'r')\n",
    "ax.set_xlim([ds_all.lon[0],ds_all.lon[-1]]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "cb = fig.colorbar(cs, ax=ax, shrink=0.9)\n",
    "cb.set_label('Max coldwake (K)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.analysed_sst[:,60,210].plot()\n",
    "plt.plot(subset.time[istart],subset.analysed_sst[istart,60,240],'r*')\n",
    "plt.plot(subset.time[istart],subset.analysed_sst[istart,60,240],'r*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.time[istart]\n",
    "print(subset.lon[240].data,subset.lat[60].data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
