{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this program is redundant with updated code for collocating storms.\n",
    "#i only ran it to update the interpolated track files for data that was already run\n",
    "#but then i included the new interpolatin code into the subroutine so all future runs\n",
    "#collocating storms will automatically include the additional variables (wind, pres, bas9n)\n",
    "\n",
    "#second part of reducndant code adds storm wind and pressure to combined data \n",
    "\n",
    "#from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input directories\n",
    "dir_storm_info='f:/data/tc_wakes/database/info/'\n",
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "dir_flux = 'F:/data/model_data/oaflux/data_v3/daily/turbulence/'\n",
    "dir_cmc = 'F:/data/sst/cmc/CMC0.2deg/v2/'\n",
    "dir_ccmp='F:/data/sat_data/ccmp/v02.0/Y'\n",
    "##where to get the data through opendap, use these directories instead\n",
    "#dir_cmc = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/CMC/CMC0.1deg/v3/'\n",
    "#dir_flux = 'http://apdrc.soest.hawaii.edu:80/dods/public_data/WHOI_OAFlux/version3/daily/lh_oaflux/'\n",
    "#the latest ccmp is from www.remss.com but they do not have an opendap server so you can use this instead:\n",
    "#dir_ccmp='https://podaac-opendap.jpl.nasa.gov/opendap/allData/ccmp/L3.0/flk/'\n",
    "\n",
    "#################################################################################\n",
    "import geopy.distance\n",
    "from math import sin, pi\n",
    "from scipy import interpolate\n",
    "\n",
    "#functions for running storm data\n",
    "import sys\n",
    "sys.path.append('C:/Users/gentemann/Google Drive/d_drive/python/storm_heat_content/subroutines/')\n",
    "from storm_masking_routines import interpolate_storm_path\n",
    "#from storm_masking_routines import get_dist_grid\n",
    "#from storm_masking_routines import closest_dist\n",
    "#from storm_masking_routines import calculate_storm_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_year = 2003\n",
    "date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "isave_mld_year = 0 #init MLD monthly data read flag\n",
    "for root, dirs, files in os.walk(dir_storm_info, topdown=False):\n",
    "    if root[len(dir_storm_info):len(dir_storm_info)+1]=='.':\n",
    "        continue\n",
    "    for name in files:\n",
    "        if not name.endswith('.nc'):\n",
    "            continue\n",
    "        filename=os.path.join(root, name)\n",
    "        print(filename[36:39],filename[31:35])\n",
    "        inum_storm=int(filename[36:39])\n",
    "        iyr_storm=int(filename[31:35])\n",
    "\n",
    "        if iyr_storm<2011:\n",
    "            continue\n",
    "\n",
    "#        if iyr_storm!=2007: # or iyr_storm<2003:\n",
    "#            continue\n",
    "        print(name,filename)\n",
    "        ds_storm_info = xr.open_dataset(filename)\n",
    "        ds_storm_info.close()\n",
    "\n",
    "        ds_storm_interp = interpolate_storm_path(ds_storm_info)\n",
    "        \n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "        ds_storm_interp.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'F:/data/tc_wakes/database/info/2004/003annual_storm_info_extended.nc'\n",
    "ds_storm_info = xr.open_dataset(filename)\n",
    "ds_storm_info.close()\n",
    "ds_storm_interp = interpolate_storm_path(ds_storm_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.plot(ds_storm_interp.time[0,:],ds_storm_interp.wind[0,:],'ko-')\n",
    "plt.plot(ds_storm_info.time[0,:],ds_storm_info.wind[0,:],'r.-')\n",
    "plt.subplot(122)\n",
    "plt.plot(ds_storm_interp.lon[0,:],ds_storm_interp.lat[0,:],'ko-')\n",
    "plt.plot(ds_storm_info.lon[0,:],ds_storm_info.lat[0,:],'r.-')\n",
    "print(ds_storm_info.dims)\n",
    "print(ds_storm_interp.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,112):\n",
    "    print(ds_storm_info.lon[0,i].values,ds_storm_interp.lon[0,i].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:/data/tc_wakes/database/sst/2004/006_interpolated_track.nc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "#still processing 2002, 2011 - onwards\n",
    "for iyr_storm=2003,2011:\n",
    "    for inum_storm=0,100:\n",
    "#iyr_storm,inum_storm = 2004,6\n",
    "    dir_out='f:/data/tc_wakes/database/sst/'\n",
    "    filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "    exists = os.path.isfile(filename)\n",
    "    if exists:\n",
    "        print(filename)\n",
    "        ds_storm_info=xr.open_dataset(filename)\n",
    "        ds_storm_info = ds_storm_info.sel(j2=0)\n",
    "        ds_storm_info.close()\n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "        ds_all = xr.open_dataset(filename)\n",
    "        ds_all.close()\n",
    "        xdim,ydim,tdim = ds_all.lon.shape[0],ds_all.lat.shape[0],ds_all.time.shape[0]\n",
    "        wtem=np.empty([ydim,xdim])\n",
    "        ptem=np.empty([ydim,xdim])\n",
    "        for i in range(0,xdim):\n",
    "            for j in range(0,ydim):\n",
    "                storm_index = ds_all.closest_storm_index[j,i].data\n",
    "                wtem[j,i]=ds_storm_info.wind[int(storm_index)].data\n",
    "                ptem[j,i]=ds_storm_info.pres[int(storm_index)].data\n",
    "        xrtem=xr.DataArray(wtem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n",
    "        ds_all['wmo_storm_wind']=xrtem\n",
    "        xrtem=xr.DataArray(ptem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n",
    "        ds_all['wmo_storm_pres']=xrtem\n",
    "    filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "    ds_all.to_netcdf(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:/data/tc_wakes/database/sst/2009/028_interpolated_track.nc\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All-NaN slice encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d9b76e8e5277>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     98\u001b[0m                         \u001b[0miend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtdim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                     \u001b[0mcoldwake_max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msst_prestorm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalysed_sst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mistart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0miend\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                     \u001b[0mitmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msst_prestorm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalysed_sst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mistart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0miend\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m                     \u001b[0mcoldwake_maxindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mistart\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mitmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                     \u001b[0mdelay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mistart\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mitmp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mistart\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmin\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m     \"\"\"\n\u001b[1;32m-> 1101\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\xarray\\core\\common.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[1;34m(self, dim, axis, skipna, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m                              **kwargs):\n\u001b[0;32m     24\u001b[0m                 return self.reduce(func, dim, axis,\n\u001b[1;32m---> 25\u001b[1;33m                                    skipna=skipna, allow_lazy=True, **kwargs)\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             def wrapped_func(self, dim=None, axis=None,\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\xarray\\core\\dataarray.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, func, dim, axis, keep_attrs, **kwargs)\u001b[0m\n\u001b[0;32m   1595\u001b[0m         \"\"\"\n\u001b[0;32m   1596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1597\u001b[1;33m         \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_attrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1598\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_replace_maybe_drop_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\xarray\\core\\variable.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, func, dim, axis, keep_attrs, allow_lazy, **kwargs)\u001b[0m\n\u001b[0;32m   1352\u001b[0m             \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_axis_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1353\u001b[0m         data = func(self.data if allow_lazy else self.values,\n\u001b[1;32m-> 1354\u001b[1;33m                     axis=axis, **kwargs)\n\u001b[0m\u001b[0;32m   1355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\xarray\\core\\duck_array_ops.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(values, axis, skipna, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdask_array_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\xarray\\core\\nanops.py\u001b[0m in \u001b[0;36mnanargmin\u001b[1;34m(a, axis)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"All-NaN slice encountered\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: All-NaN slice encountered"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import geopy.distance\n",
    "from math import sin, pi\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "\n",
    "#still processing 2002, 2011 - onwards\n",
    "for iyr_storm in range(2009,2010):\n",
    "    for inum_storm in range(28,29): #100):\n",
    "        dir_out='f:/data/tc_wakes/database/sst/'\n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "        exists = os.path.isfile(filename)\n",
    "        if exists:\n",
    "            print(filename)\n",
    "            ds_storm_info=xr.open_dataset(filename)\n",
    "            ds_storm_info = ds_storm_info.sel(j2=0)\n",
    "            ds_storm_info.close()\n",
    "\n",
    "#add storm translation speed to storm information\n",
    "            tdim_storm = ds_storm_info.time.size\n",
    "            storm_speed = ds_storm_info.time.copy(deep=True)*np.nan    \n",
    "            for i in range(0,tdim_storm-1):\n",
    "                coords_1 = (ds_storm_info.lat[i], ds_storm_info.lon[i])  \n",
    "                coords_2 = (ds_storm_info.lat[i+1], ds_storm_info.lon[i+1])  \n",
    "                arclen_temp = geopy.distance.geodesic(coords_1, coords_2).km  #distance in km  \n",
    "                storm_date1 = np.datetime64(date_1858 + dt.timedelta(days=float(ds_storm_info.time[i])))  \n",
    "                storm_date2 = np.datetime64(date_1858 + dt.timedelta(days=float(ds_storm_info.time[i+1])))  \n",
    "                arclen_time = storm_date2 - storm_date1\n",
    "                arclen_hr = arclen_time / np.timedelta64(1, 'h')\n",
    "                storm_speed[i]=arclen_temp/(arclen_hr)\n",
    "            storm_speed[-1]=storm_speed[-2]\n",
    "            ds_storm_info['storm_speed']=storm_speed\n",
    "            \n",
    "#add storm info to combined grid\n",
    "            filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "            ds_all = xr.open_dataset(filename)\n",
    "            ds_all.close()\n",
    "            xdim,ydim,tdim = ds_all.lon.shape[0],ds_all.lat.shape[0],ds_all.time.shape[0]\n",
    "            wtem=np.empty([ydim,xdim])\n",
    "            ptem=np.empty([ydim,xdim])\n",
    "            stem=np.empty([ydim,xdim])\n",
    "            for i in range(0,xdim):\n",
    "                for j in range(0,ydim):\n",
    "                    storm_index = ds_all.closest_storm_index[j,i].data\n",
    "                    wtem[j,i]=ds_storm_info.wind[int(storm_index)].data\n",
    "                    ptem[j,i]=ds_storm_info.pres[int(storm_index)].data\n",
    "                    stem[j,i]=ds_storm_info.storm_speed[int(storm_index)].data\n",
    "            xrtem=xr.DataArray(wtem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n",
    "            ds_all['wmo_storm_wind']=xrtem\n",
    "            xrtem=xr.DataArray(ptem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n",
    "            ds_all['wmo_storm_pres']=xrtem\n",
    "            xrtem=xr.DataArray(stem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n",
    "            ds_all['wmo_storm_speed']=xrtem\n",
    "\n",
    "#make subset by masking data\n",
    "#subset now only has the data within 100 and 800 km of storm\n",
    "            if abs(ds_all.lon[-1]-ds_all.lon[0])>180:\n",
    "                ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "                ds_storm_info['lon'] = np.mod(ds_storm_info['lon'], 360)\n",
    "            max_lat = ds_storm_info.lat.max()\n",
    "            if max_lat<0:\n",
    "                cond = ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm<=0)) |  ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm>0))\n",
    "            else:\n",
    "                cond = ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm<0)) |  ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm>=0))\n",
    "            subset = ds_all.where(cond)\n",
    "\n",
    "#now calculate coldwake information\n",
    "          \n",
    "            xdim,ydim,tdim = ds_all.lon.shape[0],ds_all.lat.shape[0],ds_all.time.shape[0]\n",
    "            date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "            coldwake_max=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "            coldwake_maxindex=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "            coldwake_hrtomaxcold=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "            coldwake_recovery=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "#go through entire array lat/lon dims\n",
    "            for i in range(0,xdim):\n",
    "                for j in range(0,ydim):\n",
    "                     #calculate the storm time for the closest collocated storm point then find the combined data index for closest time\n",
    "                    #this gives you the combined data storm index cross over\n",
    "                    #storm index is the combined data index for the strom cross over\n",
    "                    storm_date = date_1858 + dt.timedelta(days=float(ds_all.closest_storm_time[j,i]))  \n",
    "                    storm_date64 = np.datetime64(storm_date)\n",
    "                    if np.isnan(subset.analysed_sst[0,j,i]):  #don't process masked values\n",
    "                        continue\n",
    "                    time_diff = subset.time-storm_date64\n",
    "                    storm_index = np.argmin(abs(time_diff)).data\n",
    "                    #now look for cold wake for 1 day before strom to 5 days after strom\n",
    "                    #caluclate hours to cold wake, maximum cold wake, hours until it returns to prestorm sst\n",
    "                    #there is NO filter on wheither coldwake large enough here, just does all points\n",
    "                    istart,iend = int(storm_index)-1,int(storm_index)+5\n",
    "                    if istart<0:\n",
    "                        istart=0\n",
    "                    if iend>tdim:\n",
    "                        iend=tdim\n",
    "                    if np.isnan(subset.sst_prestorm[j,i]):\n",
    "                        continue\n",
    "                    coldwake_max[j,i] = (subset.sst_prestorm[j,i]-subset.analysed_sst[istart:iend,j,i]).min()\n",
    "                    itmp = np.argmin(subset.sst_prestorm[j,i]-subset.analysed_sst[istart:iend,j,i]).data\n",
    "                    coldwake_maxindex[j,i]=istart+itmp\n",
    "                    delay = subset.time[istart+itmp].values-subset.time[istart+1].values\n",
    "                    coldwake_hrtomaxcold[j,i]=delay / np.timedelta64(1, 'h')\n",
    "                    for k in range(istart+itmp,tdim):\n",
    "                        sst_change = subset.sst_prestorm[j,i]-subset.analysed_sst[k,j,i]\n",
    "                        if sst_change>-0.2:\n",
    "                            break\n",
    "                    delay = subset.time[k].values-subset.time[istart+1].values\n",
    "                    coldwake_recovery[j,i]=delay / np.timedelta64(1, 'h')\n",
    "\n",
    "            ds_all['coldwake_max']=coldwake_max\n",
    "            ds_all['coldwake_maxindex']=coldwake_maxindex\n",
    "            ds_all['coldwake_hrtomaxcold']=coldwake_hrtomaxcold\n",
    "            ds_all['coldwake_hrtorecovery']=coldwake_recovery\n",
    "            print('out:',filename)\n",
    "            filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data_all.nc'\n",
    "            ds_all.to_netcdf(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'sst_prestorm' ()>\n",
       "array(nan)\n",
       "Coordinates:\n",
       "    lat      float64 60.86\n",
       "    lon      float64 159.3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.sst_prestorm[j,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'analysed_sst' (time: 6)>\n",
       "array([       nan, 272.604691, 272.233219, 272.555623, 272.726984, 272.650825])\n",
       "Coordinates:\n",
       "    lat      float64 60.86\n",
       "    lon      float64 159.3\n",
       "  * time     (time) datetime64[ns] 2009-05-12T12:00:00 ... 2009-05-17T12:00:00\n",
       "Attributes:\n",
       "    long_name:      analysed sea surface temperature\n",
       "    standard_name:  sea_surface_foundation_temperature\n",
       "    units:          kelvin\n",
       "    valid_min:      -200\n",
       "    valid_max:      4000\n",
       "    source:         EUR-L2P-ATS_NR_2P,REMSS_GRIDDED_25-AMSRE,NAVO-L2P-AVHRR17...\n",
       "    comment:        SST defined at all grid points but no physical meaning is..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.analysed_sst[istart:iend,j,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
