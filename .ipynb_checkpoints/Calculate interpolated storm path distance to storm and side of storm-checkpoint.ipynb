{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input directories\n",
    "dir_storm_info='f:/data/tc_wakes/database/info/'\n",
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "dir_flux = 'F:/data/model_data/oaflux/data_v3/daily/turbulence/'\n",
    "dir_cmc = 'F:/data/sst/cmc/CMC0.2deg/v2/'\n",
    "dir_ccmp='F:/data/sat_data/ccmp/v02.0/Y'\n",
    "##where to get the data through opendap, use these directories instead\n",
    "#dir_cmc = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/CMC/CMC0.1deg/v3/'\n",
    "#dir_flux = 'http://apdrc.soest.hawaii.edu:80/dods/public_data/WHOI_OAFlux/version3/daily/lh_oaflux/'\n",
    "#the latest ccmp is from www.remss.com but they do not have an opendap server so you can use this instead:\n",
    "#dir_ccmp='https://podaac-opendap.jpl.nasa.gov/opendap/allData/ccmp/L3.0/flk/'\n",
    "\n",
    "#################################################################################\n",
    "import geopy.distance\n",
    "from math import sin, pi\n",
    "from scipy import interpolate\n",
    "import sys\n",
    "\n",
    "#functions for running storm data\n",
    "def interpolate_storm_path(dsx):\n",
    "    #after calculating the distance from the storm it became clear that the storm data is every 6 hours, no matter \n",
    "    #how much it may have moved.  So if the storm moved 300 km in 6 hr, when calculating the distance to the storm\n",
    "    #there were points on the storm track that showed large distances because of the separation to the 6hrly storm points\n",
    "    #this subroutine interpolates the storm path onto a higher spatial resolution\n",
    "    #the new storm dataset is carefully put into an identical format with i2 and j2 as dims to match the old format\n",
    "    ynew = []\n",
    "    tnew = []\n",
    "    xnew = []\n",
    "    dsx['lon'] = (dsx.lon-180) % 360 - 180 #put -180 to 180\n",
    "    for istep in range(1,dsx.lon.shape[1]):\n",
    "        dif_lat = dsx.lat[0,istep]-dsx.lat[0,istep-1]\n",
    "        dif_lon = dsx.lon[0,istep]-dsx.lon[0,istep-1]\n",
    "        x,y,t = dsx.lon[0,istep-1:istep+1].values,dsx.lat[0,istep-1:istep+1].values,dsx.time[0,istep-1:istep+1].values\n",
    "        x1,y1,t1 = dsx.lon[0,istep-1:istep].values,dsx.lat[0,istep-1:istep].values,dsx.time[0,istep-1:istep].values\n",
    "        if abs(dif_lat)>abs(dif_lon):\n",
    "            isign = np.sign(dif_lat)\n",
    "            if abs(dif_lat)>0.75:\n",
    "                ynew1 = np.arange(y[0], y[-1], isign.data*0.75)\n",
    "                f = interpolate.interp1d(y, x, assume_sorted=False)\n",
    "                xnew1 = f(ynew1)\n",
    "                f = interpolate.interp1d(y, t, assume_sorted=False)\n",
    "                tnew1 = f(ynew1)\n",
    "            else:\n",
    "                xnew1,ynew1,tnew1 = x1,y1,t1\n",
    "            xnew,ynew,tnew = np.append(xnew,xnew1),np.append(ynew,ynew1),np.append(tnew,tnew1) \n",
    "        else:\n",
    "            isign = np.sign(dif_lon)\n",
    "            if abs(dif_lon)>0.75:\n",
    "                iwrap_interp = 1\n",
    "                if (x[0]<-90) & (x[-1]>90):\n",
    "                    iwrap_interp = -1\n",
    "                    x[0]=x[0]+360\n",
    "                if (x[0]>90) & (x[-1]<-90):\n",
    "                    iwrap_interp = -1\n",
    "                    x[-1]=x[-1]+360\n",
    "                xnew1 = np.arange(x[0], x[-1], iwrap_interp*isign.data*0.75)\n",
    "                f = interpolate.interp1d(x, y, assume_sorted=False)\n",
    "                ynew1 = f(xnew1)\n",
    "                f = interpolate.interp1d(x, t, assume_sorted=False)\n",
    "                tnew1 = f(xnew1)\n",
    "                xnew1 = (xnew1 - 180) % 360 - 180 #put -180 to 180\n",
    "            else:\n",
    "                xnew1,ynew1,tnew1 = x1,y1,t1\n",
    "            xnew,ynew,tnew = np.append(xnew,xnew1),np.append(ynew,ynew1),np.append(tnew,tnew1) \n",
    "#remove any repeated points\n",
    "    ilen=xnew.size\n",
    "    outputx,outputy,outputt=[],[],[]\n",
    "    for i in range(ilen-1):\n",
    "        if (xnew[i]==xnew[i+1]) and (ynew[i]==ynew[i+1]):\n",
    "            continue\n",
    "        else:\n",
    "            outputx,outputy,outputt = np.append(outputx,xnew[i]),np.append(outputy,ynew[i]),np.append(outputt,tnew[i])\n",
    "    xnew,ynew,tnew=outputx,outputy,outputt\n",
    "#put into xarray\n",
    "    i2,j2=xnew.shape[0],1\n",
    "    tem = np.expand_dims(xnew, axis=0)\n",
    "    xx = xr.DataArray(tem.T,dims=['i2','j2'])\n",
    "    tem = np.expand_dims(ynew, axis=0)\n",
    "    yy = xr.DataArray(tem.T,dims=['i2','j2'])\n",
    "    tem = np.expand_dims(tnew, axis=0)\n",
    "    tt = xr.DataArray(tem.T,dims=['i2','j2'])\n",
    "    dsx_new = xr.Dataset({'lon':xx.T,'lat':yy.T,'time':tt.T})\n",
    "    return dsx_new\n",
    "\n",
    "def get_dist_grid(lat_point,lon_point,lat_grid,lon_grid):\n",
    "    #this routine takes a point and finds distance to all points in a grid of lat and lon\n",
    "    #it is slowwwwwww\n",
    "    dist_grid = np.empty(lat_grid.shape)    \n",
    "    coords_1 = (lat_point, lon_point)  \n",
    "    for i in range(0,lat_grid.shape[0]):\n",
    "        for j in range(0,lat_grid.shape[1]):\n",
    "            coords_2 = (lat_grid[i,j], lon_grid[i,j])  \n",
    "            arclen_temp = geopy.distance.geodesic(coords_1, coords_2).km  #distance in km       \n",
    "            dist_grid[i,j]=arclen_temp\n",
    "    return dist_grid\n",
    "\n",
    "\n",
    "def closest_dist(ds_in,ds_storm): \n",
    "# m.garcia-reyes 2.4.2019, edited c.gentemann 2.4.2019\n",
    "# calculate distance closest storm point\n",
    "# point given as tla,tlo.... storm is in the program\n",
    "# initialize distances (in km)\n",
    " #   ds_storm['lon'] = (ds_storm.lon + 180) % 360 - 180\n",
    "    dsx_input = ds_storm.copy(deep=True)\n",
    "    ds_storm_new = interpolate_storm_path(dsx_input)       \n",
    "    tdim,xdim,ydim=ds_storm_new.lat.shape[1], ds_in.analysed_sst[0,:,0].shape[0], ds_in.analysed_sst[0,0,:].shape[0]\n",
    "    dx_save=np.zeros([tdim,xdim,ydim])\n",
    "    dx_grid,dy_grid = np.meshgrid(ds_in.lon.values,ds_in.lat.values)\n",
    "    lon_grid,lat_grid = np.meshgrid(ds_in.lon.values,ds_in.lat.values)\n",
    "    min_dist_save = np.zeros([xdim,ydim])*np.nan\n",
    "    min_index_save = np.zeros([xdim,ydim])*np.nan\n",
    "    min_time_save = np.zeros([xdim,ydim])*np.nan\n",
    "\n",
    "    position = np.zeros([xdim,ydim])*np.nan\n",
    "    #for each location of the storm calculate the difference for all values in box\n",
    "    for ipt in range(0,ds_storm_new.lat.shape[1]):  # all storm values\n",
    "        dist_tem_grid = get_dist_grid(ds_storm_new.lat[0,ipt].values,ds_storm_new.lon[0,ipt].values,lat_grid,lon_grid)\n",
    "        dx_save[ipt,:,:]=dist_tem_grid       \n",
    "    #now go through each value in box and find minimum storm location/day\n",
    "    ds_tem = ds_in.copy(deep=True)\n",
    "    for j in range(0,ds_in.lon.shape[0]):\n",
    "        for i in range(0,ds_in.lat.shape[0]):\n",
    "            imin = np.argmin(dx_save[:,i,j])\n",
    "            min_dist_save[i,j]=dx_save[imin,i,j]\n",
    "            min_index_save[i,j]=imin\n",
    "            min_time_save[i,j]=ds_storm_new.time[0,imin]\n",
    "            i1,i2=imin,imin+1\n",
    "            if i2>=ds_storm_new.lat.shape[1]:\n",
    "                i1,i2=imin-1,imin\n",
    "            lonx,laty=ds_in.lon[j],ds_in.lat[i]\n",
    "    #                sign((Bx - Ax) * (Y - Ay) - (By - Ay) * (X - Ax))\n",
    "            if (ds_storm_new.lon[0,i2]<0) and (lonx>180):\n",
    "                lonx=lonx-360\n",
    "            position[i,j] = np.sign((ds_storm_new.lon[0,i2] - ds_storm_new.lon[0,i1]) * (laty - ds_storm_new.lat[0,i1]) \n",
    "                                 - (ds_storm_new.lat[0,i2] - ds_storm_new.lat[0,i1]) * (lonx - ds_storm_new.lon[0,i1]))\n",
    "\n",
    "    return min_dist_save,min_index_save,min_time_save,position,ds_storm_new\n",
    "\n",
    "def calculate_storm_mask(ds_sst,lats,lons):\n",
    "#make a mask for the storm and only keep data within -4 and 10 degrees of storm track\n",
    "#this was written before I had calculated the closest_dist which is probably a better mask to use\n",
    "    iwrap_mask = 0\n",
    "    if (ds_sst.lon.max().values>170) & (ds_sst.lon.min().values<-170):\n",
    "        iwrap_mask=1\n",
    "    print(ds_sst.lon.min().values,ds_sst.lon.max().values)\n",
    "#okay, now ds_storm is array with right lat/lon for storm so create mask now\n",
    "    ds_mask = ds_sst.copy(deep=True)\n",
    "    ds_mask['storm_mask']=ds_mask.analysed_sst*0\n",
    "    ds_mask = ds_mask.fillna(0)\n",
    "    ds_mask['storm_mask'] = ds_mask['storm_mask'].astype(int,copy=True)\n",
    "    for i in range(0,lats.shape[0]):\n",
    "        if lats[i]>0:   #northern hemi on right, southers on left\n",
    "            lons1,lons2=lons[i]-4,lons[i]+10\n",
    "        else:\n",
    "            lons1,lons2=lons[i]-10,lons[i]+4\n",
    "        lats1,lats2=lats[i]-10,lats[i]+10\n",
    "        if i==0:\n",
    "            print('lons1,lons2:',iwrap_mask,lons1.data,lons2.data)\n",
    "        if lons1<-180:\n",
    "            ds_mask['storm_mask'].loc[dict(lon=(ds_mask.lon < lons2) | (ds_mask.lon > lons1+360), lat=slice(lats1,lats2))] = -1\n",
    "        elif lons2>180:\n",
    "            ds_mask['storm_mask'].loc[dict(lon=(ds_mask.lon < lons2-360) | (ds_mask.lon > lons1), lat=slice(lats1,lats2))] = -1\n",
    "        else:\n",
    "            if iwrap_mask==1:\n",
    "                ds_mask.coords['lon'] = np.mod(ds_mask['lon'], 360)\n",
    "                ds_mask = ds_mask.sortby(ds_mask.lon)\n",
    "                ds_mask['storm_mask'].loc[dict(lon=slice(lons1+360,lons2+360), lat=slice(lats1,lats2))] = -1\n",
    "                ds_mask.coords['lon'] = (ds_mask.coords['lon'] + 180) % 360 - 180\n",
    "            else:\n",
    "                ds_mask['storm_mask'].loc[dict(lon=slice(lons1,lons2), lat=slice(lats1,lats2))] = -1\n",
    "    return ds_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_year=int(str(sys.argv[1]))\n",
    "print ('processing year:', input_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "isave_mld_year = 0 #init MLD monthly data read flag\n",
    "for root, dirs, files in os.walk(dir_storm_info, topdown=False):\n",
    "    if root[len(dir_storm_info):len(dir_storm_info)+1]=='.':\n",
    "        continue\n",
    "    for name in files:\n",
    "        if not name.endswith('.nc'):\n",
    "            continue\n",
    "        filename=os.path.join(root, name)\n",
    "        print(filename[36:39],filename[31:35])\n",
    "        inum_storm=int(filename[36:39])\n",
    "        iyr_storm=int(filename[31:35])\n",
    "\n",
    "        \n",
    "        if iyr_storm!=input_year:\n",
    "            continue\n",
    "#        if iyr_storm==2002 and inum_storm<9:\n",
    "#            continue\n",
    "        \n",
    "        \n",
    "#        if iyr_storm!=2007: # or iyr_storm<2003:\n",
    "#            continue\n",
    "        print(name,filename)\n",
    "        ds_storm_info = xr.open_dataset(filename)\n",
    "        lats = ds_storm_info.lat[0,:]\n",
    "        lons = ds_storm_info.lon[0,:]  #lons goes from 0 to 360\n",
    "        lons = (lons + 180) % 360 - 180 #put -180 to 180\n",
    "        dysince = ds_storm_info.time\n",
    "        ds_storm_info.close()\n",
    "        \n",
    "#make lat and lon of storm onto 25 km grid for below\n",
    "        lons = (((lons - .125)/.25+1).astype(int)-1)*.25+.125\n",
    "        lats = (((lats + 89.875)/.25+1).astype(int)-1)*.25-89.875\n",
    "        \n",
    "        iwrap=0\n",
    "#calculate size of box to get data in\n",
    "        minlon,maxlon = min(lons.values)-10, max(lons.values)+10\n",
    "        minlat,maxlat = min(lats.values)-10, max(lats.values)+10\n",
    "\n",
    "        ydim_storm = round((maxlat - minlat)/.25).astype(int)\n",
    "        new_lat_storm = np.linspace(minlat, maxlat, ydim_storm)\n",
    "        if (minlon<-90 and maxlon>=90) or (minlon<-180 and maxlon<0):  #this storm wraps  keep everythig 0 to 360 then wrap data at very end\n",
    "            iwrap = 1\n",
    "            lons2 = np.mod(lons, 360)\n",
    "            minlon, maxlon = min(lons2.values)-10, max(lons2.values)+10\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "        else:\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "\n",
    "        print(iwrap,minlon,maxlon)\n",
    "        print(iwrap,xdim_storm, new_lon_storm[:5],new_lon_storm[-5:])\n",
    "\n",
    "        \n",
    "        date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "        dims=lats.shape\n",
    "        tdim=dims[0]\n",
    "        tem_date=[0]*tdim #print(dysince.values)\n",
    "        for i in range(0,tdim):\n",
    "            tem_date[i]=date_1858+dt.timedelta(days=float(dysince[0,i].values))  #create new time array that can be queried for year etc\n",
    "        min_date = min(tem_date)+dt.timedelta(days=-5)\n",
    "        max_date = max(tem_date)+dt.timedelta(days=5)\n",
    "        minjdy = min_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        minyear =min_date.year #create new time array that can be queried for year etc\n",
    "        minmon =min_date.month #create new time array that can be queried for year etc\n",
    "        minday =min_date.day #create new time array that can be queried for year etc\n",
    "        maxjdy = max_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        maxyear =max_date.year  #create new time array that can be queried for year etc\n",
    "        print(minyear,minjdy,maxyear,maxjdy)\n",
    "        \n",
    "        dif = max(tem_date)-min(tem_date)\n",
    "        tdim=int(dif.days)+30             #calculate ssts for 30 days after storm\n",
    "\n",
    "        #print(tdim,xdim,ydim)            \n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "        ds_all = xr.open_dataset(filename,drop_variables=['uwnd','vwnd','dbss_obml','lhtfl','shtfl','tmp2m ','hum2m','analysed_sst_clim'])\n",
    "        ds_all.close()\n",
    "        \n",
    "#        ds_all = xr.merge([ds_storm_ccmp, ds_storm_mld, ds_storm_lhf, ds_storm_shf, ds_storm_ta, ds_storm_qa, ds_storm_sst])\n",
    "        if iwrap==1:\n",
    "            ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "            ds_storm_info['lon'] = np.mod(ds_storm_info['lon'], 360)\n",
    "\n",
    "        #calculate mask\n",
    "        print('caluculating mask')\n",
    "        ds_mask = calculate_storm_mask(ds_all,lats,lons)\n",
    "        ds_all['storm_mask']=ds_mask['storm_mask']\n",
    "        #dist to storm\n",
    "        print('calculating dist')\n",
    "        dist,index,stime,position,ds_storm_interp = closest_dist(ds_all,ds_storm_info)\n",
    "        dtem=xr.DataArray(dist, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['dist_from_storm_km']=dtem\n",
    "        dtem=xr.DataArray(index, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['closest_storm_index']=dtem\n",
    "        dtem=xr.DataArray(stime, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['closest_storm_time']=dtem\n",
    "        dtem=xr.DataArray(position, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['side_of_storm']=dtem\n",
    "\n",
    "        \n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined__masking_data.nc'\n",
    "        ds_all.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "        ds_storm_interp.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
