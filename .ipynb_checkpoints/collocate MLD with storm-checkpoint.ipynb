{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sin, pi\n",
    "from scipy import interpolate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INSTALLED VERSIONS\n",
      "------------------\n",
      "commit: None\n",
      "python: 3.7.1 | packaged by conda-forge | (default, Mar 13 2019, 13:32:59) [MSC v.1900 64 bit (AMD64)]\n",
      "python-bits: 64\n",
      "OS: Windows\n",
      "OS-release: 7\n",
      "machine: AMD64\n",
      "processor: Intel64 Family 6 Model 63 Stepping 2, GenuineIntel\n",
      "byteorder: little\n",
      "LC_ALL: None\n",
      "LANG: None\n",
      "LOCALE: None.None\n",
      "libhdf5: 1.10.4\n",
      "libnetcdf: 4.6.2\n",
      "\n",
      "xarray: 0.12.1\n",
      "pandas: 0.24.2\n",
      "numpy: 1.16.2\n",
      "scipy: 1.2.1\n",
      "netCDF4: 1.5.0.1\n",
      "pydap: None\n",
      "h5netcdf: None\n",
      "h5py: 2.9.0\n",
      "Nio: None\n",
      "zarr: None\n",
      "cftime: 1.0.3.4\n",
      "nc_time_axis: None\n",
      "PseudonetCDF: None\n",
      "rasterio: None\n",
      "cfgrib: None\n",
      "iris: None\n",
      "bottleneck: 1.2.1\n",
      "dask: 1.2.0\n",
      "distributed: 1.27.0\n",
      "matplotlib: 3.0.3\n",
      "cartopy: 0.17.0\n",
      "seaborn: 0.9.0\n",
      "setuptools: 41.0.0\n",
      "pip: 19.0.3\n",
      "conda: None\n",
      "pytest: None\n",
      "IPython: 7.4.0\n",
      "sphinx: None\n"
     ]
    }
   ],
   "source": [
    "xr.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "iyr_storm=2003\n",
    "inum_storm=32\n",
    "filename_out = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_MLD_data_v2.nc'\n",
    "print(os.path.exists(filename_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import cartopy.crs as ccrs\n",
    "dir_storm_wmo='F:/data/tc_wakes/ibtracks/year/'\n",
    "\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input directories\n",
    "dir_storm_info='f:/data/tc_wakes/database/info/'\n",
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "dir_flux = 'F:/data/model_data/oaflux/data_v3/daily/turbulence/'\n",
    "dir_cmc = 'F:/data/sst/cmc/CMC0.2deg/v2/'\n",
    "dir_ccmp='F:/data/sat_data/ccmp/v02.0/Y'\n",
    "##where to get the data through opendap, use these directories instead\n",
    "#dir_cmc = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/CMC/CMC0.1deg/v3/'\n",
    "#dir_flux = 'http://apdrc.soest.hawaii.edu:80/dods/public_data/WHOI_OAFlux/version3/daily/lh_oaflux/'\n",
    "#the latest ccmp is from www.remss.com but they do not have an opendap server so you can use this instead:\n",
    "#dir_ccmp='https://podaac-opendap.jpl.nasa.gov/opendap/allData/ccmp/L3.0/flk/'\n",
    "\n",
    "#################################################################################\n",
    "import geopy.distance\n",
    "from math import sin, pi\n",
    "from scipy import interpolate\n",
    "\n",
    "#functions for running storm data\n",
    "import sys\n",
    "sys.path.append('./subroutines/')\n",
    "from storm_masking_routines import interpolate_storm_path\n",
    "from storm_masking_routines import get_dist_grid\n",
    "from storm_masking_routines import closest_dist\n",
    "from storm_masking_routines import calculate_storm_mask\n",
    "\n",
    "input_year=int(str(sys.argv[1]))\n",
    "print ('processing year:', input_year)\n",
    "\n",
    "#this is a special version of the code above that just re-caluclates the MLD\n",
    "### SPECIAL VERSION ####\n",
    "date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "isave_mld_year = 0 #init MLD monthly data read flag\n",
    "for root, dirs, files in os.walk(dir_storm_info, topdown=False):\n",
    "    if root[len(dir_storm_info):len(dir_storm_info)+1]=='.':\n",
    "        continue\n",
    "    for name in files:\n",
    "        if not name.endswith('.nc'):\n",
    "            continue\n",
    "        filename=os.path.join(root, name)\n",
    "        print(filename[36:39],filename[31:35])\n",
    "        inum_storm=int(filename[36:39])\n",
    "        iyr_storm=int(filename[31:35])\n",
    "\n",
    "#        if iyr_storm!=input_year:\n",
    "#            continue\n",
    "#        if input_storm!=inum_storm:\n",
    "#            continue\n",
    "\n",
    "#        if iyr_storm!=2007: # or iyr_storm<2003:\n",
    "#            continue\n",
    "        print(name,filename)\n",
    "        ds_storm_info = xr.open_dataset(filename)\n",
    "        lats = ds_storm_info.lat[0,:]\n",
    "        lons = ds_storm_info.lon[0,:]  #lons goes from 0 to 360\n",
    "        lons = (lons + 180) % 360 - 180 #put -180 to 180\n",
    "        dysince = ds_storm_info.time\n",
    "        ds_storm_info.close()\n",
    "#        print(ds_storm_info)\n",
    "#        break\n",
    "#        ds_storm_interp = interpolate_storm_path(ds_storm_info)\n",
    "#        print(ds_storm_interp)\n",
    "#        break\n",
    "\n",
    "#make lat and lon of storm onto 25 km grid for below\n",
    "        lons = (((lons - .125)/.25+1).astype(int)-1)*.25+.125\n",
    "        lats = (((lats + 89.875)/.25+1).astype(int)-1)*.25-89.875\n",
    "        \n",
    "        iwrap=0\n",
    "#calculate size of box to get data in\n",
    "        minlon,maxlon = min(lons.values)-10, max(lons.values)+10\n",
    "        minlat,maxlat = min(lats.values)-10, max(lats.values)+10\n",
    "\n",
    "        ydim_storm = round((maxlat - minlat)/.25).astype(int)\n",
    "        new_lat_storm = np.linspace(minlat, maxlat, ydim_storm)\n",
    "        if (minlon<-90 and maxlon>=90) or (minlon<-180 and maxlon<0):  #this storm wraps  keep everythig 0 to 360 then wrap data at very end\n",
    "            iwrap = 1\n",
    "            lons2 = np.mod(lons, 360)\n",
    "            minlon, maxlon = min(lons2.values)-10, max(lons2.values)+10\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "        else:\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "\n",
    "        print(iwrap,minlon,maxlon)\n",
    "        print(iwrap,xdim_storm, new_lon_storm[:5],new_lon_storm[-5:])\n",
    "\n",
    "        \n",
    "        dims=lats.shape\n",
    "        tdim=dims[0]\n",
    "        tem_date=[0]*tdim #print(dysince.values)\n",
    "        for i in range(0,tdim):\n",
    "            tem_date[i]=date_1858+dt.timedelta(days=float(dysince[0,i].values))  #create new time array that can be queried for year etc\n",
    "        min_date = min(tem_date)+dt.timedelta(days=-5)\n",
    "#        max_date = max(tem_date)+dt.timedelta(days=5)\n",
    "        minjdy = min_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        minyear =min_date.year #create new time array that can be queried for year etc\n",
    "        minmon =min_date.month #create new time array that can be queried for year etc\n",
    "        minday =min_date.day #create new time array that can be queried for year etc\n",
    "#        maxjdy = max_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "#        maxyear =max_date.year  #create new time array that can be queried for year etc\n",
    "        print(minyear,minjdy)#,maxyear,maxjdy)\n",
    "        \n",
    "        dif = max(tem_date)-min(tem_date)\n",
    "        tdim=int(dif.days)+45             #calculate ssts for 30 days after storm\n",
    "        \n",
    "        max_date = dt.datetime(minyear,minmon,minday)+dt.timedelta(days=tdim)+dt.timedelta(hours=12)\n",
    "\n",
    "        #print(tdim,xdim,ydim)            \n",
    "\n",
    "        #special read in of ECCO data and subset in time \n",
    "        filename = 'http://apdrc.soest.hawaii.edu:80/dods/public_data/ECCO2/cube92/mxldepth'\n",
    "        ds_ecco = xr.open_dataset(filename)\n",
    "        ds_ecco.close()\n",
    "        ds_ecco = ds_ecco.sel(time=slice(min_date,max_date))\n",
    "        if iwrap==0:\n",
    "            ds_ecco.coords['lon'] = (ds_ecco.coords['lon'] + 180) % 360 - 180\n",
    "            ds_ecco = ds_ecco.sortby(ds_ecco.lon)\n",
    "        \n",
    "        #print('sst_out_sv',sst_out_sv.shape)\n",
    "        for i in range(0,tdim):\n",
    "            storm_date = dt.datetime(minyear,minmon,minday)+dt.timedelta(days=i)+dt.timedelta(hours=12)\n",
    "            #print(storm_date)\n",
    "            \n",
    "            syr=str(storm_date.year)\n",
    "            smon=str(storm_date.month)\n",
    "            sdym=str(storm_date.day)\n",
    "            sjdy=str(storm_date.timetuple().tm_yday)\n",
    "\n",
    "             \n",
    "#ocean mixed layer depth from monthly data GODAS NOAA, lon 0 to 360, monthly data so interp to day\n",
    "#this is monthly data (all other data daily) so need to read in year before and year after\n",
    "#so any storms <1/15 or greater than 12/15 in the year still get data\n",
    "#dir_godas='https://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/godas/'\n",
    "            dir_godas = 'f:/data/model_data/godas/'\n",
    "            if isave_mld_year != storm_date.year:\n",
    "                filename = dir_godas + 'dbss_obml.' + str(storm_date.year-1) + '.nc'\n",
    "                ds_day_mld=xr.open_dataset(filename)\n",
    "                ds_day_mld['time']=ds_day_mld.time+np.timedelta64(14,'D')  #data provider gave 1st day of ave in time \n",
    "                ds_day_mld.close()\n",
    "                filename = dir_godas + 'dbss_obml.' + str(storm_date.year) + '.nc'\n",
    "                ds_day_mld2=xr.open_dataset(filename)\n",
    "                ds_day_mld2['time']=ds_day_mld2.time+np.timedelta64(14,'D')  #data provider gave 1st day of ave in time \n",
    "                ds_day_mld2.close()\n",
    "                ds_day_mld = xr.concat([ds_day_mld,ds_day_mld2],dim='time')\n",
    "                filename = dir_godas + 'dbss_obml.' + str(storm_date.year+1) + '.nc'\n",
    "                ds_day_mld2=xr.open_dataset(filename)\n",
    "                ds_day_mld2['time']=ds_day_mld2.time+np.timedelta64(14,'D')  #data provider gave 1st day of ave in time \n",
    "                ds_day_mld2.close()\n",
    "                ds_day_mld = xr.concat([ds_day_mld,ds_day_mld2],dim='time')\n",
    "                if iwrap==0:\n",
    "                    ds_day_mld.coords['lon'] = (ds_day_mld.coords['lon'] + 180) % 360 - 180\n",
    "                    ds_day_mld = ds_day_mld.sortby(ds_day_mld.lon)\n",
    "                isave_mld_year = storm_date.year\n",
    "            if i==0:\n",
    "                tem_storm_date = dt.datetime(minyear,minmon,minday)+dt.timedelta(days=-30)\n",
    "                ds_storm = ds_day_mld.interp(time = tem_storm_date, lat = new_lat_storm,lon = new_lon_storm)\n",
    "            else:\n",
    "                ds_storm = ds_day_mld.interp(time = storm_date, lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if i==0:\n",
    "                ds_storm_mld = ds_storm\n",
    "            else:\n",
    "                ds_storm_mld = xr.concat([ds_storm_mld,ds_storm],dim='time')            \n",
    "\n",
    "            ds_storm = ds_ecco.interp(time = storm_date, lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if i==0:\n",
    "                ds_storm_mld_ecco = ds_storm\n",
    "            else:\n",
    "                ds_storm_mld_ecco = xr.concat([ds_storm_mld_ecco,ds_storm],dim='time')            \n",
    "                \n",
    "#        ds_all = xr.merge([ds_storm_ccmp, ds_storm_mld, ds_storm_lhf, ds_storm_shf, ds_storm_ta, ds_storm_qa, ds_storm_sst, ds_storm_sst_clim])\n",
    "        ds_all = xr.merge([ds_storm_mld,ds_storm_mld_ecco])\n",
    "\n",
    "        if iwrap==1:\n",
    "            ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "            ds_storm_info['lon'] = np.mod(ds_storm_info['lon'], 360)     \n",
    "         \n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_MLD_data_v2.nc'\n",
    "        ds_all.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
