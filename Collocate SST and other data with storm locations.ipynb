{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import cartopy.crs as ccrs\n",
    "dir_storm_wmo='F:/data/tc_wakes/ibtracks/year/'\n",
    "\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input directories\n",
    "dir_storm_info='f:/data/tc_wakes/database/info/'\n",
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "dir_flux = 'F:/data/model_data/oaflux/data_v3/daily/turbulence/'\n",
    "dir_cmc = 'F:/data/sst/cmc/CMC0.2deg/v2/'\n",
    "dir_ccmp='F:/data/sat_data/ccmp/v02.0/Y'\n",
    "##where to get the data through opendap, use these directories instead\n",
    "#dir_cmc = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/CMC/CMC0.1deg/v3/'\n",
    "#dir_flux = 'http://apdrc.soest.hawaii.edu:80/dods/public_data/WHOI_OAFlux/version3/daily/lh_oaflux/'\n",
    "#the latest ccmp is from www.remss.com but they do not have an opendap server so you can use this instead:\n",
    "#dir_ccmp='https://podaac-opendap.jpl.nasa.gov/opendap/allData/ccmp/L3.0/flk/'\n",
    "\n",
    "#################################################################################\n",
    "import geopy.distance\n",
    "from math import sin, pi\n",
    "from scipy import interpolate\n",
    "\n",
    "#functions for running storm data\n",
    "import sys\n",
    "sys.path.append('./subroutines/')\n",
    "from storm_masking_routines import interpolate_storm_path\n",
    "from storm_masking_routines import get_dist_grid\n",
    "from storm_masking_routines import closest_dist\n",
    "from storm_masking_routines import calculate_storm_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_year=int(str(sys.argv[1]))\n",
    "print ('processing year:', input_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_year = 2010\n",
    "input_storm = 20\n",
    "date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "isave_mld_year = 0 #init MLD monthly data read flag\n",
    "for root, dirs, files in os.walk(dir_storm_info, topdown=False):\n",
    "    if root[len(dir_storm_info):len(dir_storm_info)+1]=='.':\n",
    "        continue\n",
    "    for name in files:\n",
    "        if not name.endswith('.nc'):\n",
    "            continue\n",
    "        filename=os.path.join(root, name)\n",
    "        print(filename[36:39],filename[31:35])\n",
    "        inum_storm=int(filename[36:39])\n",
    "        iyr_storm=int(filename[31:35])\n",
    "\n",
    "        if iyr_storm!=input_year:\n",
    "            continue\n",
    "        if input_storm!=inum_storm:\n",
    "            continue\n",
    "\n",
    "#        if iyr_storm!=2007: # or iyr_storm<2003:\n",
    "#            continue\n",
    "        print(name,filename)\n",
    "        ds_storm_info = xr.open_dataset(filename)\n",
    "        lats = ds_storm_info.lat[0,:]\n",
    "        lons = ds_storm_info.lon[0,:]  #lons goes from 0 to 360\n",
    "        lons = (lons + 180) % 360 - 180 #put -180 to 180\n",
    "        dysince = ds_storm_info.time\n",
    "        ds_storm_info.close()\n",
    "#        print(ds_storm_info)\n",
    "#        break\n",
    "#        ds_storm_interp = interpolate_storm_path(ds_storm_info)\n",
    "#        print(ds_storm_interp)\n",
    "#        break\n",
    "\n",
    "#make lat and lon of storm onto 25 km grid for below\n",
    "        lons = (((lons - .125)/.25+1).astype(int)-1)*.25+.125\n",
    "        lats = (((lats + 89.875)/.25+1).astype(int)-1)*.25-89.875\n",
    "        \n",
    "        iwrap=0\n",
    "#calculate size of box to get data in\n",
    "        minlon,maxlon = min(lons.values)-10, max(lons.values)+10\n",
    "        minlat,maxlat = min(lats.values)-10, max(lats.values)+10\n",
    "\n",
    "        ydim_storm = round((maxlat - minlat)/.25).astype(int)\n",
    "        new_lat_storm = np.linspace(minlat, maxlat, ydim_storm)\n",
    "        if (minlon<-90 and maxlon>=90) or (minlon<-180 and maxlon<0):  #this storm wraps  keep everythig 0 to 360 then wrap data at very end\n",
    "            iwrap = 1\n",
    "            lons2 = np.mod(lons, 360)\n",
    "            minlon, maxlon = min(lons2.values)-10, max(lons2.values)+10\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "        else:\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "\n",
    "        print(iwrap,minlon,maxlon)\n",
    "        print(iwrap,xdim_storm, new_lon_storm[:5],new_lon_storm[-5:])\n",
    "\n",
    "        \n",
    "        dims=lats.shape\n",
    "        tdim=dims[0]\n",
    "        tem_date=[0]*tdim #print(dysince.values)\n",
    "        for i in range(0,tdim):\n",
    "            tem_date[i]=date_1858+dt.timedelta(days=float(dysince[0,i].values))  #create new time array that can be queried for year etc\n",
    "        min_date = min(tem_date)+dt.timedelta(days=-5)\n",
    "#        max_date = max(tem_date)+dt.timedelta(days=5)\n",
    "        minjdy = min_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        minyear =min_date.year #create new time array that can be queried for year etc\n",
    "        minmon =min_date.month #create new time array that can be queried for year etc\n",
    "        minday =min_date.day #create new time array that can be queried for year etc\n",
    "#        maxjdy = max_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "#        maxyear =max_date.year  #create new time array that can be queried for year etc\n",
    "        print(minyear,minjdy)#,maxyear,maxjdy)\n",
    "        \n",
    "        dif = max(tem_date)-min(tem_date)\n",
    "        tdim=int(dif.days)+45             #calculate ssts for 30 days after storm\n",
    "        \n",
    "        #print(tdim,xdim,ydim)            \n",
    "        \n",
    "        #print('sst_out_sv',sst_out_sv.shape)\n",
    "        for i in range(0,tdim):\n",
    "            storm_date = dt.datetime(minyear,minmon,minday)+dt.timedelta(days=i)+dt.timedelta(hours=12)\n",
    "            #print(storm_date)\n",
    "            \n",
    "            syr=str(storm_date.year)\n",
    "            smon=str(storm_date.month)\n",
    "            sdym=str(storm_date.day)\n",
    "            sjdy=str(storm_date.timetuple().tm_yday)\n",
    "\n",
    "#sst data   \n",
    "            fname_tem=syr + smon.zfill(2) + sdym.zfill(2) + '120000-CMC-L4_GHRSST-SSTfnd-CMC0.2deg-GLOB-v02.0-fv02.0.nc'\n",
    "            filename = dir_cmc + syr + '/' + sjdy.zfill(3) + '/' + fname_tem\n",
    "            ds_day=xr.open_dataset(filename,drop_variables=['analysis_error','sea_ice_fraction'])\n",
    "            if iwrap==1:  #data is -180 to 180 for sst, so need to bring to 0 to 360 when wrapped\n",
    "                ds_day.coords['lon'] = np.mod(ds_day['lon'], 360)\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds_day.close()\n",
    "            ds_day = ds_day.where(ds_day['mask'] == 1.) \n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            #ds_storm['time']=storm_date\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            if i==0:\n",
    "                ds_storm_sst = ds_storm\n",
    "            else:\n",
    "                ds_storm_sst = xr.concat([ds_storm_sst,ds_storm],dim='time')\n",
    "\n",
    "#sst climatology  \n",
    "            if storm_date.timetuple().tm_yday==366:\n",
    "                sjdy = '365'\n",
    "            filename='F:/data/sst/cmc/CMC0.2deg/v2/climatology/clim1993_2016' + sjdy.zfill(3) + '-CMC-L4_GHRSST-SSTfnd-CMC0.2deg-GLOB-v02.0-fv02.0.nc'\n",
    "            ds_day=xr.open_dataset(filename,drop_variables=['analysis_error','sea_ice_fraction','sq_sst'])\n",
    "            ds_day = ds_day.rename({'analysed_sst':'analysed_sst_clim','mask':'mask_clim'}) #, inplace = True)            \n",
    "            if iwrap==1:  #data is -180 to 180 for sst, so need to bring to 0 to 360 when wrapped\n",
    "                ds_day.coords['lon'] = np.mod(ds_day['lon'], 360)\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds_day.close()\n",
    "            ds_day = ds_day.where(ds_day['mask_clim'] == 1.) \n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            if i==0:\n",
    "                ds_storm_sst_clim = ds_storm\n",
    "            else:\n",
    "                ds_storm_sst_clim = xr.concat([ds_storm_sst_clim,ds_storm],dim='time')           \n",
    "            \n",
    "#ccmp wind data, no masked data, a complete field\n",
    "#            lyr, idyjl = 2015,1\n",
    "#            storm_date = dt.datetime(2015,1,1)\n",
    "            syr, smon, sdym, sjdy=str(storm_date.year),str(storm_date.month),str(storm_date.day),str(storm_date.timetuple().tm_yday)\n",
    "            fname_tem='/CCMP_Wind_Analysis_' + syr + smon.zfill(2) + sdym.zfill(2) + '_V02.0_L3.0_RSS.nc'\n",
    "            ccmp_filename = dir_ccmp + syr + '/M' + smon.zfill(2) + fname_tem      \n",
    "            ds=xr.open_dataset(ccmp_filename,drop_variables=['nobs'])\n",
    "            ds_day = ds.mean(dim='time')     #take average across all 6 hourly data fields\n",
    "            ds_day = ds_day.rename({'longitude':'lon','latitude':'lat'}) #, inplace = True)            \n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if i==0:\n",
    "                ds_storm_ccmp = ds_storm\n",
    "            else:\n",
    "                ds_storm_ccmp = xr.concat([ds_storm_ccmp,ds_storm],dim='time')\n",
    "              \n",
    "#ocean mixed layer depth from monthly data GODAS NOAA, lon 0 to 360, monthly data so interp to day\n",
    "#this is monthly data (all other data daily) so need to read in year before and year after\n",
    "#so any storms <1/15 or greater than 12/15 in the year still get data\n",
    "#dir_godas='https://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/godas/'\n",
    "            dir_godas = 'f:/data/model_data/godas/'\n",
    "            if isave_mld_year != storm_date.year:\n",
    "                filename = dir_godas + 'dbss_obml.' + str(storm_date.year-1) + '.nc'\n",
    "                ds_day_mld=xr.open_dataset(filename)\n",
    "                ds_day_mld['time']=ds_day_mld.time+np.timedelta64(14,'D')  #data provider gave 1st day of ave in time \n",
    "                ds_day_mld.close()\n",
    "                filename = dir_godas + 'dbss_obml.' + str(storm_date.year) + '.nc'\n",
    "                ds_day_mld2=xr.open_dataset(filename)\n",
    "                ds_day_mld2['time']=ds_day_mld2.time+np.timedelta64(14,'D')  #data provider gave 1st day of ave in time \n",
    "                ds_day_mld2.close()\n",
    "                ds_day_mld = xr.concat([ds_day_mld,ds_day_mld2],dim='time')\n",
    "                filename = dir_godas + 'dbss_obml.' + str(storm_date.year+1) + '.nc'\n",
    "                ds_day_mld2=xr.open_dataset(filename)\n",
    "                ds_day_mld2['time']=ds_day_mld2.time+np.timedelta64(14,'D')  #data provider gave 1st day of ave in time \n",
    "                ds_day_mld2.close()\n",
    "                ds_day_mld = xr.concat([ds_day_mld,ds_day_mld2],dim='time')\n",
    "                if iwrap==0:\n",
    "                    ds_day_mld.coords['lon'] = (ds_day_mld.coords['lon'] + 180) % 360 - 180\n",
    "                    ds_day_mld = ds_day_mld.sortby(ds_day_mld.lon)\n",
    "                isave_mld_year = storm_date.year\n",
    "            ds_storm = ds_day_mld.interp(time = storm_date, lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if i==0:\n",
    "                ds_storm_mld = ds_storm\n",
    "            else:\n",
    "                ds_storm_mld = xr.concat([ds_storm_mld,ds_storm],dim='time')            \n",
    "            \n",
    "#latent heat flux data, masked already set to NaN                \n",
    "            filename = dir_flux + 'lh_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_lhf = ds_storm\n",
    "            else:\n",
    "                ds_storm_lhf = xr.concat([ds_storm_lhf,ds_storm],dim='time')\n",
    "\n",
    "#sensible heat flux data , masked already set to NaN                \n",
    "            filename = dir_flux + 'sh_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_shf = ds_storm\n",
    "            else:\n",
    "                ds_storm_shf = xr.concat([ds_storm_shf,ds_storm],dim='time')\n",
    "\n",
    "#surface humid flux data   , masked already set to NaN              \n",
    "            filename = dir_flux + 'qa_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_qa = ds_storm\n",
    "            else:\n",
    "                ds_storm_qa = xr.concat([ds_storm_qa,ds_storm],dim='time')\n",
    "\n",
    "#air temp flux data   , masked already set to NaN              \n",
    "            filename = dir_flux + 'ta_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_ta = ds_storm\n",
    "            else:\n",
    "                ds_storm_ta = xr.concat([ds_storm_ta,ds_storm],dim='time')\n",
    "                \n",
    "#        ds_all = xr.merge([ds_storm_ccmp, ds_storm_mld, ds_storm_lhf, ds_storm_shf, ds_storm_ta, ds_storm_qa, ds_storm_sst, ds_storm_sst_clim])\n",
    "        ds_all = xr.merge([ds_storm_ccmp, ds_storm_mld, ds_storm_lhf, ds_storm_shf, ds_storm_ta, ds_storm_qa, ds_storm_sst,ds_storm_sst_clim])\n",
    "\n",
    "        #calculate mask\n",
    "#        print('caluculating mask')\n",
    "#        ds_mask = calculate_storm_mask(ds_all,lats,lons)\n",
    "#        ds_all['storm_mask']=ds_mask['storm_mask']\n",
    "#        #dist to storm\n",
    "#        print('calculating dist')\n",
    "#        dist,index,stime,position = closest_dist(ds_all,ds_storm_info)\n",
    "#        dtem=xr.DataArray(dist, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "#        ds_all['dist_from_storm_km']=dtem\n",
    "#        dtem=xr.DataArray(index, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "#        ds_all['closest_storm_index']=dtem\n",
    "#        dtem=xr.DataArray(stime, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "#        ds_all['closest_storm_time']=dtem\n",
    "#        dtem=xr.DataArray(position, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "#        ds_all['side_of_storm']=dtem\n",
    "\n",
    "        if iwrap==1:\n",
    "            ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "            ds_storm_info['lon'] = np.mod(ds_storm_info['lon'], 360)\n",
    "\n",
    "        #calculate mask\n",
    "        print('caluculating mask')\n",
    "        ds_mask = calculate_storm_mask(ds_all,lats,lons)\n",
    "        ds_all['storm_mask']=ds_mask['storm_mask']\n",
    "        #dist to storm\n",
    "        print('calculating dist')\n",
    "        dist,index,stime,position,ds_storm_interp = closest_dist(ds_all,ds_storm_info)\n",
    "        dtem=xr.DataArray(dist, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['dist_from_storm_km']=dtem\n",
    "        dtem=xr.DataArray(index, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['closest_storm_index']=dtem\n",
    "        dtem=xr.DataArray(stime, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['closest_storm_time']=dtem\n",
    "        dtem=xr.DataArray(position, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['side_of_storm']=dtem\n",
    "       \n",
    "        xdim,ydim,tdim = ds_all.lon.shape[0],ds_all.lat.shape[0],ds_all.time.shape[0]\n",
    "        wtem=np.empty([ydim,xdim])\n",
    "        ptem=np.empty([ydim,xdim])\n",
    "        stem=np.empty([ydim,xdim])\n",
    "        for i in range(0,xdim):\n",
    "            for j in range(0,ydim):\n",
    "                storm_index = ds_all.closest_storm_index[j,i].data\n",
    "                wtem[j,i]=ds_storm_interp.wind[0,int(storm_index)].data\n",
    "                ptem[j,i]=ds_storm_interp.pres[0,int(storm_index)].data\n",
    "                stem[j,i]=ds_storm_interp.storm_speed_kmhr[0,int(storm_index)].data\n",
    "        xrtem=xr.DataArray(wtem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n",
    "        ds_all['wmo_storm_wind']=xrtem\n",
    "        xrtem=xr.DataArray(ptem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n",
    "        ds_all['wmo_storm_pres']=xrtem\n",
    "        xrtem=xr.DataArray(stem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n",
    "        ds_all['wmo_storm_speed_kmhr']=xrtem\n",
    "        \n",
    "        #find max sst 5 days before storm location\n",
    "        #first create an array with the storm crossover time (from nearest point) as an array\n",
    "        sdate = np.empty([ydim,xdim], dtype=dt.datetime)    \n",
    "        for i in range(0,xdim):\n",
    "            for j in range(0,ydim):\n",
    "                tem=date_1858+dt.timedelta(days=float(ds_all.closest_storm_time[j,i])) \n",
    "                sdate[j,i]=np.datetime64(tem)\n",
    "        xsdate=xr.DataArray(sdate, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))    \n",
    "        ds_all['closest_storm_time_np64']=xsdate\n",
    "        #now use array of storm time to calculate prestorm sst\n",
    "        sst0 = ds_all.dist_from_storm_km.copy(deep=True)\n",
    "        sst0_clim_anom = ds_all.dist_from_storm_km.copy(deep=True)\n",
    "        for i in range(0,xdim):\n",
    "            for j in range(0,ydim):\n",
    "                #sst0[j,i] = ds_data.analysed_sst[:,j,i].interp(time=xsdate[j,i])\n",
    "                sst0[j,i] = ds_all.analysed_sst[:,j,i].sel(time=slice(xsdate[j,i]-np.timedelta64(5,'D'),xsdate[j,i])).max()\n",
    "                sst0_clim_anom[j,i] = (ds_all.analysed_sst[:,j,i]-ds_all.analysed_sst_clim[:,j,i]).sel(time=slice(xsdate[j,i]-np.timedelta64(5,'D'),xsdate[j,i])).max()\n",
    "        ds_all['sst_prestorm']=sst0\n",
    "        ds_all['sst_prestorm_clim']=sst0_clim_anom\n",
    "#create sst anomalies\n",
    "        ds_all['sst_anom']=ds_all.analysed_sst-ds_all.analysed_sst_clim\n",
    "#now calculate coldwake information\n",
    "        if abs(ds_all.lon[-1]-ds_all.lon[0])>180:\n",
    "            ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "            ds_storm_interp['lon'] = np.mod(ds_storm_interp['lon'], 360)\n",
    "        max_lat = ds_storm_interp.lat.max()\n",
    "\n",
    "    #remove all data outsice 100km/800km or cold wake >0 or <-10\n",
    "        if max_lat<0:\n",
    "            cond = ((((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm<=0)) | \n",
    "            ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm>0))) )       \n",
    "        else:\n",
    "            cond = ((((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm<0)) | \n",
    "            ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm>=0))))            \n",
    "        subset = ds_all.where(cond)\n",
    "          \n",
    "        xdim,ydim,tdim = ds_all.lon.shape[0],ds_all.lat.shape[0],ds_all.time.shape[0]\n",
    "        date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "        coldwake_max=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "        coldwake_maxindex=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "        coldwake_hrtomaxcold=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "        coldwake_recovery=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "#go through entire array lat/lon dims\n",
    "        for i in range(0,xdim):\n",
    "            for j in range(0,ydim):\n",
    "                 #calculate the storm time for the closest collocated storm point then find the combined data index for closest time\n",
    "                #this gives you the combined data storm index cross over\n",
    "                storm_date64 = ds_all.closest_storm_time_np64[j,i]\n",
    "                if np.isnan(subset.analysed_sst[0,j,i]):  #don't process masked values\n",
    "                    continue\n",
    "                time_diff = subset.time-storm_date64\n",
    "                storm_index = np.argmin(abs(time_diff)).data\n",
    "                #now look for cold wake for 1 day before strom to 5 days after strom\n",
    "                #caluclate hours to cold wake, maximum cold wake, hours until it returns to prestorm sst\n",
    "                #there is NO filter on wheither coldwake large enough here, just does all points\n",
    "                istart,iend = int(storm_index),int(storm_index)+8\n",
    "                if iend>tdim:\n",
    "                    iend=tdim\n",
    "                if np.isnan(subset.sst_prestorm_clim[j,i]):\n",
    "                    continue\n",
    "                #coldwake_max[j,i] = (subset.analysed_sst[istart:iend,j,i]-subset.sst_prestorm[j,i]).min()\n",
    "                coldwake_max[j,i] = (subset.sst_anom[istart:iend,j,i]-subset.sst_prestorm_clim[j,i]).min()\n",
    "                if all(np.isnan(subset.sst_anom[istart:iend,j,i])):\n",
    "                    continue\n",
    "                itmp = np.argmin(subset.sst_anom[istart:iend,j,i]-subset.sst_prestorm_clim[j,i]).data\n",
    "                coldwake_maxindex[j,i]=istart+itmp\n",
    "                delay = subset.time[istart+itmp].values-subset.time[istart].values\n",
    "                coldwake_hrtomaxcold[j,i]=delay / np.timedelta64(1, 'h')\n",
    "                for k in range(istart+itmp,tdim):\n",
    "                    sst_change = subset.sst_anom[k,j,i]-subset.sst_prestorm_clim[j,i]\n",
    "                   # sst_change_clim = subset.analysed_sst_clim[k,j,i]-subset.sst_prestorm_clim[j,i]\n",
    "            #NEED TO ADD CLIMATOLOGY SST CHANGE HERE\n",
    "                    if sst_change>-0.1:  #changed 2/27 based on dare and mcbride paper criteria\n",
    "                        break\n",
    "                delay = subset.time[k].values-subset.time[istart].values\n",
    "                coldwake_recovery[j,i]=delay / np.timedelta64(1, 'D')\n",
    "\n",
    "        ds_all['coldwake_max']=coldwake_max\n",
    "        ds_all['coldwake_maxindex']=coldwake_maxindex\n",
    "        ds_all['coldwake_hrtomaxcold']=coldwake_hrtomaxcold\n",
    "        ds_all['coldwake_dytorecovery']=coldwake_recovery\n",
    "        \n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "        ds_all.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "        ds_storm_interp.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all.dbss_obml[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all.analysed_sst[0,:,:].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iyr_storm=2010\n",
    "inum_storm=20\n",
    "filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "ds_old = xr.open_dataset(filename)\n",
    "ds_old.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_old.dbss_obml[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "for iyr_storm in range(2010,2017):\n",
    "#inum_storm = 55\n",
    "#for iyr_storm in range(2003,2011):\n",
    "    filename=dir_storm_wmo+'Year.'+str(iyr_storm)+'.ibtracs_wmo.v03r10.nc'\n",
    "    ds_ibtrak = xr.open_dataset(filename)\n",
    "    ds_ibtrak.close()\n",
    "    for inum_storm in range(0,100): #0,100):#100): #(0,100): #100):\n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "        exists = os.path.isfile(filename)\n",
    "        if exists:       \n",
    "            print(filename)\n",
    "            ds_storm_info=xr.open_dataset(filename)\n",
    "            ds_storm_info = ds_storm_info.sel(j2=0)\n",
    "            ds_storm_info.close()\n",
    "            filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "            ds_all = xr.open_dataset(filename)\n",
    "            ds_all['spd']=np.sqrt(ds_all.uwnd**2+ds_all.vwnd**2)\n",
    "            ds_all.close()\n",
    "            icenter=0\n",
    "\n",
    "    #        if ds_all.lon.max()>180:\n",
    "    #            ds_all.coords['lon'] = (ds_all.coords['lon'] + 180) % 360 - 180\n",
    "\n",
    "            if ((abs(ds_storm_info.lon[-1]-ds_storm_info.lon[0])>180) | ((ds_all.lon[-1].data>180) & (ds_all.lon[0].data<180))):\n",
    "    #            ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "    #            ds_storm_info['lon'] = np.mod(ds_storm_info['lon'], 360)\n",
    "                icenter=-180\n",
    "            max_lat = ds_storm_info.lat.max()\n",
    "            #remove all data outsice 100km/800km or cold wake >0 or <-10\n",
    "            if max_lat<0:\n",
    "                cond = ((((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm<=0)) | \n",
    "                ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm>0))) \n",
    "                & (ds_all.coldwake_max<=-.1) & (ds_all.coldwake_max>=-10))\n",
    "            else:\n",
    "                cond = ((((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm<0)) | \n",
    "                ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm>=0))) \n",
    "                & (ds_all.coldwake_max<=-.1) & (ds_all.coldwake_max>=-10))\n",
    "            subset = ds_all.where(cond)\n",
    "\n",
    "            #create coldwake anomaly with nan for all values before wmo storm time\n",
    "            subset['sst_anomaly']=subset.analysed_sst-subset.sst_prestorm\n",
    "\n",
    "            frac = ds_all.lon.size/ds_all.lat.size*1.2\n",
    "\n",
    "            #create array with day.frac since closest storm passage\n",
    "            tdif_dy = (subset.time-subset.closest_storm_time_np64)/np.timedelta64(1, 'D')\n",
    "            subset['tdif_dy']=tdif_dy\n",
    "\n",
    "            plt.figure(1,figsize=(4, 3),dpi=100)\n",
    "            gs1 = gridspec.GridSpec(1, 3)\n",
    "            gs1.update(wspace=0.025, hspace=0.05) # set the spacing between axes. \n",
    "\n",
    "            ax = plt.subplot(gs1[0],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "            ax.set_global(), ax.coastlines()\n",
    "            cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, (ds_all.analysed_sst-ds_all.sst_prestorm).min('time'),vmin=-5,vmax=5,cmap='seismic')  \n",
    "            ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'g.',markersize=.5)\n",
    "            ax.plot(ds_storm_info.lon+icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "     #       plt.arrow(ds_storm_info.lon[3], ds_storm_info.lat[3], ds_storm_info.lon[4]-ds_storm_info.lon[3], \n",
    "     #                 ds_storm_info.lat[4]-ds_storm_info.lat[3],color='g',width=.01,head_width=1)\n",
    "     #       ax.annotate(\"\", xy=(0.5, 0.5), xytext=(0, 0),arrowprops=dict(arrowstyle=\"->\"))\n",
    "            ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "            cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "            cb.set_label('SST (K)',size=8),cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "            ax = plt.subplot(gs1[1],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "            ax.set_global(), ax.coastlines()\n",
    "            cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, ds_all.spd.max('time'))  \n",
    "            ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.plot(ds_storm_info.lon+icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.set_title(str(ds_ibtrak.name[inum_storm-1].data)[2:-1])      \n",
    "            ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "            cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "            cb.set_label('wind speed (m/s)',size=8),cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "            ax = plt.subplot(gs1[2],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "            ax.set_global(), ax.coastlines()\n",
    "            cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, ds_all.dbss_obml[0,:,:])  \n",
    "            ax.plot(ds_storm_info.lon+icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "            cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "            cb.set_label('MLD (m)',size=8),cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "            dir_figs = 'f:/data/tc_wakes/database/figs/data_images/'\n",
    "            plt.savefig(dir_figs+str(iyr_storm)+'_'+str(inum_storm)+'data.png', bbox_inches='tight',pad_inches = 0, dpi = 200)\n",
    "            plt.clf()\n",
    "\n",
    "            plt.figure(2,figsize=(4, 3),dpi=100)\n",
    "            gs1 = gridspec.GridSpec(1, 3)\n",
    "            gs1.update(wspace=0.025, hspace=0.05) # set the spacing between axes. \n",
    "\n",
    "            ax = plt.subplot(gs1[0],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "            ax.set_global(), ax.coastlines()\n",
    "            cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, (ds_all.dist_from_storm_km),cmap='seismic')  \n",
    "            ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'g.',markersize=.5)\n",
    "            ax.plot(ds_storm_info.lon+icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "            cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "            cb.set_label('dist. from storm (km)',size=8),cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "            ax = plt.subplot(gs1[1],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "            ax.set_global(), ax.coastlines()\n",
    "            cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, ds_all.side_of_storm)  \n",
    "            ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.plot(ds_storm_info.lon+icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "            ax.set_title(str(ds_ibtrak.name[inum_storm-1].data)[2:-1])      \n",
    "            cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "    #        cb = plt.colorbar(cs, ax=ax, shrink=frac,orientation=\"horizontal\",pad=0.02)\n",
    "            cb.set_label('side of storm',size=8),cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "            ax = plt.subplot(gs1[2],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "            ax.set_global(), ax.coastlines()\n",
    "            cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, ds_all.coldwake_max,vmin=-3,vmax=0)  \n",
    "            ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.plot(ds_storm_info.lon+icenter,ds_storm_info.lat,'r.',markersize=.5)\n",
    "            ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "    #        ax.set_xlim([ds_all.lon[0]+icenter*2-icenter/180*20,ds_all.lon[-1]+icenter*2]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "            cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "            cb.set_label('max coldwake (K)',size=8),cb.ax.tick_params(labelsize=6)\n",
    "            #plt.colorbar(cs,fraction=0.046, pad=0.04,orientation='horizontal')\n",
    "\n",
    "            dir_figs = 'f:/data/tc_wakes/database/figs/data_images/'\n",
    "            plt.savefig(dir_figs+str(iyr_storm)+'_'+str(inum_storm)+'info.png', bbox_inches='tight',pad_inches = 0, dpi = 200)\n",
    "            plt.clf()\n",
    "\n",
    "            print(str(ds_ibtrak.name[inum_storm-1].data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        plt.figure(1,figsize=(4, 3),dpi=100)\n",
    "        gs1 = gridspec.GridSpec(1, 3)\n",
    "        gs1.update(wspace=0.025, hspace=0.05) # set the spacing between axes. \n",
    "\n",
    "        ax = plt.subplot(gs1[0],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "        ax.set_global(), ax.coastlines()\n",
    "        cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, (ds_all.analysed_sst-ds_all.sst_prestorm).min('time'),vmin=-5,vmax=5,cmap='seismic')  \n",
    "        ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'g.',markersize=1)\n",
    " #       plt.arrow(ds_storm_info.lon[3], ds_storm_info.lat[3], ds_storm_info.lon[4]-ds_storm_info.lon[3], \n",
    " #                 ds_storm_info.lat[4]-ds_storm_info.lat[3],color='g',width=.01,head_width=1)\n",
    " #       ax.annotate(\"\", xy=(0.5, 0.5), xytext=(0, 0),arrowprops=dict(arrowstyle=\"->\"))\n",
    "        ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "        cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "        cb.set_label('SST (K)',size=8),cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "        ax = plt.subplot(gs1[1],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "        ax.set_global(), ax.coastlines()\n",
    "        cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, ds_all.spd.max('time'))  \n",
    "        ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'r.',markersize=1)\n",
    "        ax.set_title(str(ds_ibtrak.name[inum_storm-1].data)[2:-1])      \n",
    "        ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "        cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "        cb.set_label('wind speed (m/s)',size=8),cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "        ax = plt.subplot(gs1[2],projection=ccrs.PlateCarree(central_longitude=icenter))\n",
    "        ax.set_global(), ax.coastlines()\n",
    "        cs=ax.pcolormesh(ds_all.lon+icenter,ds_all.lat, ds_all.dbss_obml[0,:,:])  \n",
    "        ax.plot(ds_storm_info.lon-icenter,ds_storm_info.lat,'r.',markersize=1)\n",
    "        ax.plot(ds_storm_info.lon+icenter,ds_storm_info.lat,'r.',markersize=1)\n",
    "        ax.set_xlim([ds_all.lon[0]+icenter,ds_all.lon[-1]+icenter]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "        cb = plt.colorbar(cs, ax=ax,orientation=\"horizontal\",pad=0.02)\n",
    "        cb.set_label('MLD (m)',size=8),cb.ax.tick_params(labelsize=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all.coldwake_max.plot()\n",
    "plt.plot(ds_storm_info.lon,ds_storm_info.lat,'r.',markersize=.5)\n",
    "print(icenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_all.lon[-1].data,ds_all.lon[0].data,(ds_all.lon[-1]-ds_all.lon[0]).data)\n",
    "if ((ds_all.lon[-1].data>180) & (ds_all.lon[0].data<180)):\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = plt.axes(projection=ccrs.Orthographic(-80, 35))\n",
    "ds_all.analysed_sst[0,:,:].plot.contourf(ax=ax, transform=ccrs.PlateCarree());\n",
    "ax.set_global(); ax.coastlines();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(12, 3),dpi=100)\n",
    "\n",
    "ax = plt.subplot(131,projection=ccrs.PlateCarree())\n",
    "ax.set_global(), ax.coastlines()\n",
    "cs=ax.pcolormesh(ds_all.lon,ds_all.lat, (ds_all.dist_from_storm_km),cmap='seismic')  \n",
    "ax.plot(ds_storm_info.lon,ds_storm_info.lat,'g')\n",
    "ax.set_xlim([ds_all.lon[0],ds_all.lon[-1]]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "cb = fig.colorbar(cs, ax=ax, shrink=0.9)\n",
    "cb.set_label('distance from storm')\n",
    "\n",
    "ax = plt.subplot(132,projection=ccrs.PlateCarree())\n",
    "ax.set_global(), ax.coastlines()\n",
    "cs=ax.pcolormesh(ds_all.lon,ds_all.lat, ds_all.side_of_storm)  \n",
    "ax.plot(ds_storm_info.lon,ds_storm_info.lat,'r')\n",
    "ax.set_xlim([ds_all.lon[0],ds_all.lon[-1]]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "cb = fig.colorbar(cs, ax=ax, shrink=0.9)\n",
    "cb.set_label('side of storm')\n",
    "\n",
    "ax = plt.subplot(133,projection=ccrs.PlateCarree())\n",
    "ax.set_global(), ax.coastlines()\n",
    "cs=ax.pcolormesh(ds_all.lon,ds_all.lat, ds_all.coldwake_max,vmin=-3,vmax=0)  \n",
    "ax.plot(ds_storm_info.lon,ds_storm_info.lat,'r')\n",
    "ax.set_xlim([ds_all.lon[0],ds_all.lon[-1]]), ax.set_ylim([ds_all.lat[0],ds_all.lat[-1]])\n",
    "cb = fig.colorbar(cs, ax=ax, shrink=0.9)\n",
    "cb.set_label('Max coldwake (K)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.analysed_sst[:,60,210].plot()\n",
    "plt.plot(subset.time[istart],subset.analysed_sst[istart,60,240],'r*')\n",
    "plt.plot(subset.time[istart],subset.analysed_sst[istart,60,240],'r*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.time[istart]\n",
    "print(subset.lon[240].data,subset.lat[60].data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        print(name,filename)\n",
    "        ds_storm_info = xr.open_dataset(filename)\n",
    "        lats = ds_storm_info.lat[0,:]\n",
    "        lons = ds_storm_info.lon[0,:]  #lons goes from 0 to 360\n",
    "        lons = (lons + 180) % 360 - 180 #put -180 to 180\n",
    "        dysince = ds_storm_info.time\n",
    "        ds_storm_info.close()\n",
    "#        print(ds_storm_info)\n",
    "#        break\n",
    "#        ds_storm_interp = interpolate_storm_path(ds_storm_info)\n",
    "#        print(ds_storm_interp)\n",
    "#        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
