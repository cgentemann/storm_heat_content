{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input directories\n",
    "dir_storm_info='f:/data/tc_wakes/database/info/'\n",
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "dir_flux = 'F:/data/model_data/oaflux/data_v3/daily/turbulence/'\n",
    "dir_cmc = 'F:/data/sst/cmc/CMC0.2deg/v2/'\n",
    "dir_ccmp='F:/data/sat_data/ccmp/v02.0/Y'\n",
    "##where to get the data through opendap, use these directories instead\n",
    "#dir_cmc = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/CMC/CMC0.1deg/v3/'\n",
    "#dir_flux = 'http://apdrc.soest.hawaii.edu:80/dods/public_data/WHOI_OAFlux/version3/daily/lh_oaflux/'\n",
    "#the latest ccmp is from www.remss.com but they do not have an opendap server so you can use this instead:\n",
    "#dir_ccmp='https://podaac-opendap.jpl.nasa.gov/opendap/allData/ccmp/L3.0/flk/'\n",
    "\n",
    "#################################################################################\n",
    "import geopy.distance\n",
    "from math import sin, pi\n",
    "from scipy import interpolate\n",
    "\n",
    "#functions for running storm data\n",
    "import sys\n",
    "sys.path.append('C:/Users/gentemann/Google Drive/d_drive/python/storm_heat_content/subroutines/')\n",
    "from storm_masking_routines import interpolate_storm_path\n",
    "from storm_masking_routines import get_dist_grid\n",
    "from storm_masking_routines import closest_dist\n",
    "from storm_masking_routines import calculate_storm_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_year=int(str(sys.argv[1]))\n",
    "print ('processing year:', input_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001 2002\n",
      "002 2002\n",
      "005 2002\n",
      "007 2002\n",
      "009 2002\n",
      "010 2002\n",
      "011 2002\n",
      "012 2002\n",
      "013 2002\n",
      "014 2002\n",
      "015 2002\n",
      "017 2002\n",
      "018 2002\n",
      "019 2002\n",
      "020 2002\n",
      "021 2002\n",
      "022 2002\n",
      "023 2002\n",
      "025 2002\n",
      "027 2002\n",
      "028 2002\n",
      "030 2002\n",
      "031 2002\n",
      "033 2002\n",
      "034 2002\n",
      "035 2002\n",
      "036 2002\n",
      "037 2002\n",
      "039 2002\n",
      "040 2002\n",
      "041 2002\n",
      "042 2002\n",
      "043 2002\n",
      "046 2002\n",
      "047 2002\n",
      "048 2002\n",
      "049 2002\n",
      "050 2002\n",
      "051 2002\n",
      "052 2002\n",
      "053 2002\n",
      "054 2002\n",
      "055 2002\n",
      "056 2002\n",
      "058 2002\n",
      "060 2002\n",
      "061 2002\n",
      "063 2002\n",
      "064 2002\n",
      "066 2002\n",
      "067 2002\n",
      "068 2002\n",
      "069 2002\n",
      "070 2002\n",
      "072 2002\n",
      "075 2002\n",
      "077 2002\n",
      "081 2002\n",
      "082 2002\n",
      "083 2002\n",
      "084 2002\n",
      "002 2003\n",
      "003 2003\n",
      "004 2003\n",
      "005 2003\n",
      "006 2003\n",
      "008 2003\n",
      "009 2003\n",
      "010 2003\n",
      "011 2003\n",
      "012 2003\n",
      "013 2003\n",
      "015 2003\n",
      "016 2003\n",
      "017 2003\n",
      "018 2003\n",
      "019 2003\n",
      "021 2003\n",
      "022 2003\n",
      "023 2003\n",
      "024 2003\n",
      "025 2003\n",
      "026 2003\n",
      "027 2003\n",
      "028 2003\n",
      "031 2003\n",
      "032 2003\n",
      "033 2003\n",
      "034 2003\n",
      "035 2003\n",
      "036 2003\n",
      "037 2003\n",
      "038 2003\n",
      "040 2003\n",
      "042 2003\n",
      "044 2003\n",
      "046 2003\n",
      "047 2003\n",
      "048 2003\n",
      "049 2003\n",
      "050 2003\n",
      "051 2003\n",
      "055 2003\n",
      "057 2003\n",
      "059 2003\n",
      "060 2003\n",
      "064 2003\n",
      "065 2003\n",
      "066 2003\n",
      "067 2003\n",
      "070 2003\n",
      "071 2003\n",
      "072 2003\n",
      "073 2003\n",
      "075 2003\n",
      "076 2003\n",
      "077 2003\n",
      "078 2003\n",
      "079 2003\n",
      "080 2003\n",
      "081 2003\n",
      "082 2003\n",
      "083 2003\n",
      "086 2003\n",
      "087 2003\n",
      "088 2003\n",
      "089 2003\n",
      "091 2003\n",
      "092 2003\n",
      "093 2003\n",
      "094 2003\n",
      "095 2003\n",
      "097 2003\n",
      "001 2004\n",
      "002 2004\n",
      "003 2004\n",
      "004 2004\n",
      "005 2004\n",
      "006 2004\n",
      "007 2004\n",
      "008 2004\n",
      "009 2004\n",
      "011 2004\n",
      "012 2004\n",
      "013 2004\n",
      "014 2004\n",
      "015 2004\n",
      "016 2004\n",
      "017 2004\n",
      "018 2004\n",
      "019 2004\n",
      "021 2004\n",
      "022 2004\n",
      "023 2004\n",
      "024 2004\n",
      "025 2004\n",
      "026 2004\n",
      "028 2004\n",
      "029 2004\n",
      "030 2004\n",
      "031 2004\n",
      "032 2004\n",
      "033 2004\n",
      "034 2004\n",
      "037 2004\n",
      "038 2004\n",
      "039 2004\n",
      "040 2004\n",
      "041 2004\n",
      "043 2004\n",
      "044 2004\n",
      "045 2004\n",
      "046 2004\n",
      "047 2004\n",
      "048 2004\n",
      "049 2004\n",
      "051 2004\n",
      "052 2004\n",
      "053 2004\n",
      "054 2004\n",
      "055 2004\n",
      "058 2004\n",
      "059 2004\n",
      "060 2004\n",
      "062 2004\n",
      "063 2004\n",
      "064 2004\n",
      "065 2004\n",
      "066 2004\n",
      "068 2004\n",
      "069 2004\n",
      "071 2004\n",
      "072 2004\n",
      "073 2004\n",
      "074 2004\n",
      "075 2004\n",
      "076 2004\n",
      "081 2004\n",
      "084 2004\n",
      "087 2004\n",
      "089 2004\n",
      "091 2004\n",
      "092 2004\n",
      "093 2004\n",
      "002 2005\n",
      "003 2005\n",
      "004 2005\n",
      "005 2005\n",
      "006 2005\n",
      "007 2005\n",
      "009 2005\n",
      "010 2005\n",
      "011 2005\n",
      "012 2005\n",
      "014 2005\n",
      "015 2005\n",
      "016 2005\n",
      "017 2005\n",
      "018 2005\n",
      "019 2005\n",
      "020 2005\n",
      "021 2005\n",
      "022 2005\n",
      "023 2005\n",
      "024 2005\n",
      "025 2005\n",
      "026 2005\n",
      "027 2005\n",
      "028 2005\n",
      "029 2005\n",
      "031 2005\n",
      "034 2005\n",
      "035 2005\n",
      "038 2005\n",
      "042 2005\n",
      "043 2005\n",
      "044 2005\n",
      "045 2005\n",
      "047 2005\n",
      "048 2005\n",
      "049 2005\n",
      "053 2005\n",
      "055 2005\n",
      "056 2005\n",
      "057 2005\n",
      "058 2005\n",
      "060 2005\n",
      "061 2005\n",
      "062 2005\n",
      "063 2005\n",
      "065 2005\n",
      "066 2005\n",
      "067 2005\n",
      "068 2005\n",
      "069 2005\n",
      "070 2005\n",
      "071 2005\n",
      "072 2005\n",
      "073 2005\n",
      "074 2005\n",
      "078 2005\n",
      "079 2005\n",
      "081 2005\n",
      "082 2005\n",
      "083 2005\n",
      "084 2005\n",
      "085 2005\n",
      "086 2005\n",
      "087 2005\n",
      "088 2005\n",
      "093 2005\n",
      "095 2005\n",
      "096 2005\n",
      "097 2005\n",
      "099 2005\n",
      "101 2005\n",
      "102 2005\n",
      "103 2005\n",
      "104 2005\n",
      "106 2005\n",
      "107 2005\n",
      "108 2005\n",
      "109 2005\n",
      "110 2005\n",
      "111 2005\n",
      "001 2006\n",
      "002 2006\n",
      "003 2006\n",
      "006 2006\n",
      "007 2006\n",
      "008 2006\n",
      "009 2006\n",
      "010 2006\n",
      "012 2006\n",
      "014 2006\n",
      "015 2006\n",
      "016 2006\n",
      "017 2006\n",
      "018 2006\n",
      "019 2006\n",
      "021 2006\n",
      "022 2006\n",
      "023 2006\n",
      "026 2006\n",
      "027 2006\n",
      "028 2006\n",
      "029 2006\n",
      "030 2006\n",
      "031 2006\n",
      "032 2006\n",
      "033 2006\n",
      "034 2006\n",
      "037 2006\n",
      "038 2006\n",
      "040 2006\n",
      "041 2006\n",
      "042 2006\n",
      "043 2006\n",
      "044 2006\n",
      "045 2006\n",
      "047 2006\n",
      "049 2006\n",
      "050 2006\n",
      "052 2006\n",
      "053 2006\n",
      "054 2006\n",
      "055 2006\n",
      "056 2006\n",
      "057 2006\n",
      "058 2006\n",
      "060 2006\n",
      "061 2006\n",
      "062 2006\n",
      "064 2006\n",
      "065 2006\n",
      "066 2006\n",
      "067 2006\n",
      "068 2006\n",
      "070 2006\n",
      "072 2006\n",
      "073 2006\n",
      "075 2006\n",
      "076 2006\n",
      "077 2006\n",
      "078 2006\n",
      "079 2006\n",
      "080 2006\n",
      "083 2006\n",
      "086 2006\n",
      "087 2006\n",
      "088 2006\n",
      "089 2006\n",
      "001 2007\n",
      "002 2007\n",
      "003 2007\n",
      "004 2007\n",
      "005 2007\n",
      "007 2007\n",
      "008 2007\n",
      "009 2007\n",
      "010 2007\n",
      "011 2007\n",
      "012 2007\n",
      "013 2007\n",
      "014 2007\n",
      "015 2007\n",
      "016 2007\n",
      "017 2007\n",
      "018 2007\n",
      "020 2007\n",
      "021 2007\n",
      "022 2007\n",
      "024 2007\n",
      "026 2007\n",
      "027 2007\n",
      "028 2007\n",
      "030 2007\n",
      "031 2007\n",
      "035 2007\n",
      "037 2007\n",
      "039 2007\n",
      "040 2007\n",
      "041 2007\n",
      "043 2007\n",
      "045 2007\n",
      "046 2007\n",
      "047 2007\n",
      "048 2007\n",
      "049 2007\n",
      "050 2007\n",
      "052 2007\n",
      "053 2007\n",
      "055 2007\n",
      "056 2007\n",
      "058 2007\n",
      "059 2007\n",
      "060 2007\n",
      "066 2007\n",
      "067 2007\n",
      "068 2007\n",
      "069 2007\n",
      "070 2007\n",
      "071 2007\n",
      "072 2007\n",
      "073 2007\n",
      "074 2007\n",
      "075 2007\n",
      "076 2007\n",
      "078 2007\n",
      "080 2007\n",
      "081 2007\n",
      "082 2007\n",
      "083 2007\n",
      "002 2008\n",
      "003 2008\n",
      "004 2008\n",
      "005 2008\n",
      "006 2008\n",
      "007 2008\n",
      "008 2008\n",
      "009 2008\n",
      "010 2008\n",
      "012 2008\n",
      "013 2008\n",
      "014 2008\n",
      "015 2008\n",
      "016 2008\n",
      "017 2008\n",
      "018 2008\n",
      "019 2008\n",
      "020 2008\n",
      "021 2008\n",
      "022 2008\n",
      "023 2008\n",
      "024 2008\n",
      "027 2008\n",
      "028 2008\n",
      "029 2008\n",
      "031 2008\n",
      "036 2008\n",
      "037 2008\n",
      "038 2008\n",
      "040 2008\n",
      "042 2008\n",
      "043 2008\n",
      "044 2008\n",
      "046 2008\n",
      "047 2008\n",
      "048 2008\n",
      "049 2008\n",
      "051 2008\n",
      "052 2008\n",
      "055 2008\n",
      "056 2008\n",
      "057 2008\n",
      "058 2008\n",
      "060 2008\n",
      "061 2008\n",
      "062 2008\n",
      "063 2008\n",
      "065 2008\n",
      "066 2008\n",
      "068 2008\n",
      "069 2008\n",
      "070 2008\n",
      "071 2008\n",
      "072 2008\n",
      "074 2008\n",
      "075 2008\n",
      "077 2008\n",
      "079 2008\n",
      "086 2008\n",
      "087 2008\n",
      "088 2008\n",
      "093 2008\n",
      "001 2009\n",
      "002 2009\n",
      "003 2009\n",
      "004 2009\n",
      "005 2009\n",
      "006 2009\n",
      "008 2009\n",
      "009 2009\n",
      "011 2009\n",
      "013 2009\n",
      "014 2009\n",
      "016 2009\n",
      "017 2009\n",
      "018 2009\n",
      "020 2009\n",
      "023 2009\n",
      "025 2009\n",
      "026 2009\n",
      "027 2009\n",
      "028 2009\n",
      "029 2009\n",
      "030 2009\n",
      "032 2009\n",
      "038 2009\n",
      "040 2009\n",
      "041 2009\n",
      "045 2009\n",
      "046 2009\n",
      "047 2009\n",
      "048 2009\n",
      "049 2009\n",
      "050 2009\n",
      "052 2009\n",
      "053 2009\n",
      "054 2009\n",
      "055 2009\n",
      "057 2009\n",
      "060 2009\n",
      "061 2009\n",
      "062 2009\n",
      "065 2009\n",
      "066 2009\n",
      "068 2009\n",
      "071 2009\n",
      "072 2009\n",
      "073 2009\n",
      "074 2009\n",
      "076 2009\n",
      "078 2009\n",
      "079 2009\n",
      "080 2009\n",
      "081 2009\n",
      "083 2009\n",
      "084 2009\n",
      "085 2009\n",
      "086 2009\n",
      "087 2009\n",
      "089 2009\n",
      "090 2009\n",
      "001 2010\n",
      "002 2010\n",
      "003 2010\n",
      "004 2010\n",
      "005 2010\n",
      "007 2010\n",
      "008 2010\n",
      "010 2010\n",
      "012 2010\n",
      "015 2010\n",
      "016 2010\n",
      "017 2010\n",
      "018 2010\n",
      "019 2010\n",
      "020 2010\n",
      "021 2010\n",
      "023 2010\n",
      "024 2010\n",
      "025 2010\n",
      "026 2010\n",
      "027 2010\n",
      "028 2010\n",
      "029 2010\n",
      "031 2010\n",
      "032 2010\n",
      "034 2010\n",
      "035 2010\n",
      "036 2010\n",
      "038 2010\n",
      "040 2010\n",
      "042 2010\n",
      "043 2010\n",
      "044 2010\n",
      "045 2010\n",
      "048 2010\n",
      "049 2010\n",
      "050 2010\n",
      "051 2010\n",
      "052 2010\n",
      "054 2010\n",
      "055 2010\n",
      "056 2010\n",
      "058 2010\n",
      "059 2010\n",
      "060 2010\n",
      "061 2010\n",
      "062 2010\n",
      "063 2010\n",
      "064 2010\n",
      "065 2010\n",
      "069 2010\n",
      "071 2010\n",
      "072 2010\n",
      "074 2010\n",
      "075 2010\n",
      "076 2010\n",
      "077 2010\n",
      "079 2010\n",
      "001 2011\n",
      "002 2011\n",
      "005 2011\n",
      "006 2011\n",
      "007 2011\n",
      "008 2011\n",
      "009 2011\n",
      "010 2011\n",
      "011 2011\n",
      "012 2011\n",
      "015 2011\n",
      "016 2011\n",
      "017 2011\n",
      "018 2011\n",
      "019 2011\n",
      "020 2011\n",
      "021 2011\n",
      "022 2011\n",
      "023 2011\n",
      "024 2011\n",
      "025 2011\n",
      "026 2011\n",
      "027 2011\n",
      "030 2011\n",
      "031 2011\n",
      "033 2011\n",
      "035 2011\n",
      "036 2011\n",
      "038 2011\n",
      "039 2011\n",
      "042 2011\n",
      "043 2011\n",
      "045 2011\n",
      "046 2011\n",
      "047 2011\n",
      "050 2011\n",
      "051 2011\n",
      "053 2011\n",
      "054 2011\n",
      "055 2011\n",
      "058 2011\n",
      "061 2011\n",
      "062 2011\n",
      "063 2011\n",
      "064 2011\n",
      "065 2011\n",
      "066 2011\n",
      "067 2011\n",
      "068 2011\n",
      "069 2011\n",
      "071 2011\n",
      "072 2011\n",
      "074 2011\n",
      "075 2011\n",
      "076 2011\n",
      "077 2011\n",
      "078 2011\n",
      "080 2011\n",
      "081 2011\n",
      "082 2011\n",
      "083 2011\n",
      "084 2011\n",
      "085 2011\n",
      "086 2011\n",
      "087 2011\n",
      "001 2012\n",
      "002 2012\n",
      "003 2012\n",
      "004 2012\n",
      "005 2012\n",
      "006 2012\n",
      "007 2012\n",
      "008 2012\n",
      "009 2012\n",
      "010 2012\n",
      "011 2012\n",
      "013 2012\n",
      "014 2012\n",
      "015 2012\n",
      "016 2012\n",
      "017 2012\n",
      "018 2012\n",
      "019 2012\n",
      "020 2012\n",
      "021 2012\n",
      "022 2012\n",
      "023 2012\n",
      "024 2012\n",
      "025 2012\n",
      "026 2012\n",
      "027 2012\n",
      "030 2012\n",
      "032 2012\n",
      "033 2012\n",
      "034 2012\n",
      "035 2012\n",
      "036 2012\n",
      "037 2012\n",
      "038 2012\n",
      "039 2012\n",
      "040 2012\n",
      "041 2012\n",
      "042 2012\n",
      "043 2012\n",
      "044 2012\n",
      "045 2012\n",
      "046 2012\n",
      "047 2012\n",
      "048 2012\n",
      "049 2012\n",
      "050 2012\n",
      "051 2012\n",
      "053 2012\n",
      "054 2012\n",
      "055 2012\n",
      "056 2012\n",
      "057 2012\n",
      "058 2012\n",
      "059 2012\n",
      "060 2012\n",
      "061 2012\n",
      "062 2012\n",
      "063 2012\n",
      "064 2012\n",
      "066 2012\n",
      "067 2012\n",
      "069 2012\n",
      "073 2012\n",
      "074 2012\n",
      "076 2012\n",
      "077 2012\n",
      "078 2012\n",
      "080 2012\n",
      "081 2012\n",
      "083 2012\n",
      "085 2012\n",
      "001 2013\n",
      "002 2013\n",
      "003 2013\n",
      "004 2013\n",
      "005 2013\n",
      "007 2013\n",
      "008 2013\n",
      "009 2013\n",
      "010 2013\n",
      "011 2013\n",
      "012 2013\n",
      "013 2013\n",
      "014 2013\n",
      "016 2013\n",
      "017 2013\n",
      "018 2013\n",
      "019 2013\n",
      "020 2013\n",
      "021 2013\n",
      "022 2013\n",
      "023 2013\n",
      "025 2013\n",
      "026 2013\n",
      "031 2013\n",
      "032 2013\n",
      "033 2013\n",
      "035 2013\n",
      "036 2013\n",
      "037 2013\n",
      "038 2013\n",
      "039 2013\n",
      "042 2013\n",
      "043 2013\n",
      "044 2013\n",
      "045 2013\n",
      "047 2013\n",
      "049 2013\n",
      "050 2013\n",
      "051 2013\n",
      "052 2013\n",
      "053 2013\n",
      "054 2013\n",
      "055 2013\n",
      "057 2013\n",
      "060 2013\n",
      "061 2013\n",
      "062 2013\n",
      "063 2013\n",
      "066 2013\n",
      "067 2013\n",
      "068 2013\n",
      "069 2013\n",
      "070 2013\n",
      "071 2013\n",
      "072 2013\n",
      "073 2013\n",
      "074 2013\n",
      "076 2013\n",
      "078 2013\n",
      "079 2013\n",
      "080 2013\n",
      "081 2013\n",
      "083 2013\n",
      "084 2013\n",
      "085 2013\n",
      "086 2013\n",
      "087 2013\n",
      "088 2013\n",
      "089 2013\n",
      "091 2013\n",
      "094 2013\n",
      "095 2013\n",
      "096 2013\n",
      "098 2013\n",
      "001 2014\n",
      "002 2014\n",
      "003 2014\n",
      "004 2014\n",
      "005 2014\n",
      "006 2014\n",
      "008 2014\n",
      "009 2014\n",
      "010 2014\n",
      "011 2014\n",
      "014 2014\n",
      "016 2014\n",
      "017 2014\n",
      "018 2014\n",
      "019 2014\n",
      "020 2014\n",
      "021 2014\n",
      "022 2014\n",
      "023 2014\n",
      "024 2014\n",
      "025 2014\n",
      "026 2014\n",
      "028 2014\n",
      "029 2014\n",
      "030 2014\n",
      "031 2014\n",
      "033 2014\n",
      "034 2014\n",
      "036 2014\n",
      "038 2014\n",
      "039 2014\n",
      "042 2014\n",
      "043 2014\n",
      "044 2014\n",
      "046 2014\n",
      "048 2014\n",
      "049 2014\n",
      "050 2014\n",
      "053 2014\n",
      "054 2014\n",
      "055 2014\n",
      "056 2014\n",
      "057 2014\n",
      "058 2014\n",
      "059 2014\n",
      "061 2014\n",
      "062 2014\n",
      "063 2014\n",
      "064 2014\n",
      "066 2014\n",
      "067 2014\n",
      "068 2014\n",
      "069 2014\n",
      "070 2014\n",
      "071 2014\n",
      "072 2014\n",
      "073 2014\n",
      "074 2014\n",
      "075 2014\n",
      "076 2014\n",
      "077 2014\n",
      "078 2014\n",
      "079 2014\n",
      "081 2014\n",
      "082 2014\n",
      "084 2014\n",
      "085 2014\n",
      "086 2014\n",
      "087 2014\n",
      "090 2014\n",
      "001 2015\n",
      "002 2015\n",
      "003 2015\n",
      "004 2015\n",
      "005 2015\n",
      "006 2015\n",
      "008 2015\n",
      "009 2015\n",
      "010 2015\n",
      "011 2015\n",
      "012 2015\n",
      "013 2015\n",
      "014 2015\n",
      "015 2015\n",
      "016 2015\n",
      "017 2015\n",
      "018 2015\n",
      "019 2015\n",
      "020 2015\n",
      "022 2015\n",
      "024 2015\n",
      "027 2015\n",
      "028 2015\n",
      "029 2015\n",
      "030 2015\n",
      "031 2015\n",
      "032 2015\n",
      "033 2015\n",
      "034 2015\n",
      "035 2015\n",
      "036 2015\n",
      "039 2015\n",
      "041 2015\n",
      "042 2015\n",
      "043 2015\n",
      "044 2015\n",
      "046 2015\n",
      "048 2015\n",
      "049 2015\n",
      "050 2015\n",
      "051 2015\n",
      "053 2015\n",
      "054 2015\n",
      "056 2015\n",
      "057 2015\n",
      "058 2015\n",
      "059 2015\n",
      "061 2015\n",
      "062 2015\n",
      "065 2015\n",
      "066 2015\n",
      "067 2015\n",
      "068 2015\n",
      "070 2015\n",
      "073 2015\n",
      "075 2015\n",
      "076 2015\n",
      "077 2015\n",
      "078 2015\n",
      "080 2015\n",
      "082 2015\n",
      "084 2015\n",
      "085 2015\n",
      "087 2015\n",
      "088 2015\n",
      "089 2015\n",
      "090 2015\n",
      "092 2015\n",
      "093 2015\n",
      "095 2015\n",
      "096 2015\n",
      "097 2015\n",
      "098 2015\n",
      "099 2015\n",
      "001 2016\n",
      "002 2016\n",
      "003 2016\n",
      "004 2016\n",
      "005 2016\n",
      "006 2016\n",
      "007 2016\n",
      "007annual_storm_info_extended.nc f:/data/tc_wakes/database/info/2016\\007annual_storm_info_extended.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -85.125 -16.875\n",
      "0 273 [-85.125      -84.87408088 -84.62316176 -84.37224265 -84.12132353] [-17.87867647 -17.62775735 -17.37683824 -17.12591912 -16.875     ]\n",
      "2016 2\n",
      "caluculating mask\n",
      "-85.125 -16.875\n",
      "lons1,lons2: 0 -79.125 -65.125\n",
      "calculating dist\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All-NaN slice encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-93ac24479df4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    375\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m                 \u001b[0mcoldwake_max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalysed_sst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mistart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0miend\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msst_prestorm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m                 \u001b[0mitmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalysed_sst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mistart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0miend\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msst_prestorm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m                 \u001b[0mcoldwake_maxindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mistart\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mitmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[0mdelay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mistart\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mitmp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mistart\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmin\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m     \"\"\"\n\u001b[1;32m-> 1101\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\xarray\\core\\common.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[1;34m(self, dim, axis, skipna, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m                              **kwargs):\n\u001b[0;32m     24\u001b[0m                 return self.reduce(func, dim, axis,\n\u001b[1;32m---> 25\u001b[1;33m                                    skipna=skipna, allow_lazy=True, **kwargs)\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             def wrapped_func(self, dim=None, axis=None,\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\xarray\\core\\dataarray.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, func, dim, axis, keep_attrs, **kwargs)\u001b[0m\n\u001b[0;32m   1595\u001b[0m         \"\"\"\n\u001b[0;32m   1596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1597\u001b[1;33m         \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_attrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1598\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_replace_maybe_drop_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\xarray\\core\\variable.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, func, dim, axis, keep_attrs, allow_lazy, **kwargs)\u001b[0m\n\u001b[0;32m   1352\u001b[0m             \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_axis_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1353\u001b[0m         data = func(self.data if allow_lazy else self.values,\n\u001b[1;32m-> 1354\u001b[1;33m                     axis=axis, **kwargs)\n\u001b[0m\u001b[0;32m   1355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\xarray\\core\\duck_array_ops.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(values, axis, skipna, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdask_array_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\xarray\\core\\nanops.py\u001b[0m in \u001b[0;36mnanargmin\u001b[1;34m(a, axis)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"All-NaN slice encountered\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: All-NaN slice encountered"
     ]
    }
   ],
   "source": [
    "input_year = 2016\n",
    "input_storm = 7\n",
    "date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "isave_mld_year = 0 #init MLD monthly data read flag\n",
    "for root, dirs, files in os.walk(dir_storm_info, topdown=False):\n",
    "    if root[len(dir_storm_info):len(dir_storm_info)+1]=='.':\n",
    "        continue\n",
    "    for name in files:\n",
    "        if not name.endswith('.nc'):\n",
    "            continue\n",
    "        filename=os.path.join(root, name)\n",
    "        print(filename[36:39],filename[31:35])\n",
    "        inum_storm=int(filename[36:39])\n",
    "        iyr_storm=int(filename[31:35])\n",
    "\n",
    "        if iyr_storm!=input_year:\n",
    "            continue\n",
    "        if input_storm!=inum_storm:\n",
    "            continue\n",
    "\n",
    "#        if iyr_storm!=2007: # or iyr_storm<2003:\n",
    "#            continue\n",
    "        print(name,filename)\n",
    "        ds_storm_info = xr.open_dataset(filename)\n",
    "        lats = ds_storm_info.lat[0,:]\n",
    "        lons = ds_storm_info.lon[0,:]  #lons goes from 0 to 360\n",
    "        lons = (lons + 180) % 360 - 180 #put -180 to 180\n",
    "        dysince = ds_storm_info.time\n",
    "        ds_storm_info.close()\n",
    "#        print(ds_storm_info)\n",
    "#        break\n",
    "\n",
    "#make lat and lon of storm onto 25 km grid for below\n",
    "        lons = (((lons - .125)/.25+1).astype(int)-1)*.25+.125\n",
    "        lats = (((lats + 89.875)/.25+1).astype(int)-1)*.25-89.875\n",
    "        \n",
    "        iwrap=0\n",
    "#calculate size of box to get data in\n",
    "        minlon,maxlon = min(lons.values)-10, max(lons.values)+10\n",
    "        minlat,maxlat = min(lats.values)-10, max(lats.values)+10\n",
    "\n",
    "        ydim_storm = round((maxlat - minlat)/.25).astype(int)\n",
    "        new_lat_storm = np.linspace(minlat, maxlat, ydim_storm)\n",
    "        if (minlon<-90 and maxlon>=90) or (minlon<-180 and maxlon<0):  #this storm wraps  keep everythig 0 to 360 then wrap data at very end\n",
    "            iwrap = 1\n",
    "            lons2 = np.mod(lons, 360)\n",
    "            minlon, maxlon = min(lons2.values)-10, max(lons2.values)+10\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "        else:\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "\n",
    "        print(iwrap,minlon,maxlon)\n",
    "        print(iwrap,xdim_storm, new_lon_storm[:5],new_lon_storm[-5:])\n",
    "\n",
    "        \n",
    "        dims=lats.shape\n",
    "        tdim=dims[0]\n",
    "        tem_date=[0]*tdim #print(dysince.values)\n",
    "        for i in range(0,tdim):\n",
    "            tem_date[i]=date_1858+dt.timedelta(days=float(dysince[0,i].values))  #create new time array that can be queried for year etc\n",
    "        min_date = min(tem_date)+dt.timedelta(days=-5)\n",
    "#        max_date = max(tem_date)+dt.timedelta(days=5)\n",
    "        minjdy = min_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        minyear =min_date.year #create new time array that can be queried for year etc\n",
    "        minmon =min_date.month #create new time array that can be queried for year etc\n",
    "        minday =min_date.day #create new time array that can be queried for year etc\n",
    "#        maxjdy = max_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "#        maxyear =max_date.year  #create new time array that can be queried for year etc\n",
    "        print(minyear,minjdy)#,maxyear,maxjdy)\n",
    "        \n",
    "        dif = max(tem_date)-min(tem_date)\n",
    "        tdim=int(dif.days)+45             #calculate ssts for 30 days after storm\n",
    "        \n",
    "        #print(tdim,xdim,ydim)            \n",
    "        \n",
    "        #print('sst_out_sv',sst_out_sv.shape)\n",
    "        for i in range(0,tdim):\n",
    "            storm_date = dt.datetime(minyear,minmon,minday)+dt.timedelta(days=i)+dt.timedelta(hours=12)\n",
    "            #print(storm_date)\n",
    "            \n",
    "            syr=str(storm_date.year)\n",
    "            smon=str(storm_date.month)\n",
    "            sdym=str(storm_date.day)\n",
    "            sjdy=str(storm_date.timetuple().tm_yday)\n",
    "\n",
    "#sst data   \n",
    "            fname_tem=syr + smon.zfill(2) + sdym.zfill(2) + '120000-CMC-L4_GHRSST-SSTfnd-CMC0.2deg-GLOB-v02.0-fv02.0.nc'\n",
    "            filename = dir_cmc + syr + '/' + sjdy.zfill(3) + '/' + fname_tem\n",
    "            ds_day=xr.open_dataset(filename,drop_variables=['analysis_error','sea_ice_fraction'])\n",
    "            if iwrap==1:  #data is -180 to 180 for sst, so need to bring to 0 to 360 when wrapped\n",
    "                ds_day.coords['lon'] = np.mod(ds_day['lon'], 360)\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds_day.close()\n",
    "            ds_day = ds_day.where(ds_day['mask'] == 1.) \n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            #ds_storm['time']=storm_date\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            if i==0:\n",
    "                ds_storm_sst = ds_storm\n",
    "            else:\n",
    "                ds_storm_sst = xr.concat([ds_storm_sst,ds_storm],dim='time')\n",
    "\n",
    "#sst climatology  --- this isn't used, should remove from dataset in next round\n",
    "#            if storm_date.timetuple().tm_yday==366:\n",
    "#                sjdy = '365'\n",
    "#            filename='F:/data/sst/cmc/CMC0.2deg/v2/climatology/clim1993_2016' + sjdy.zfill(3) + '-CMC-L4_GHRSST-SSTfnd-CMC0.2deg-GLOB-v02.0-fv02.0.nc'\n",
    "#            ds_day=xr.open_dataset(filename,drop_variables=['analysis_error','sea_ice_fraction','sq_sst'])\n",
    "#            ds_day = ds_day.rename({'analysed_sst':'analysed_sst_clim','mask':'mask_clim'}) #, inplace = True)            \n",
    "#            if iwrap==1:  #data is -180 to 180 for sst, so need to bring to 0 to 360 when wrapped\n",
    "#                ds_day.coords['lon'] = np.mod(ds_day['lon'], 360)\n",
    "#                ds_day = ds_day.sortby(ds_day.lon)\n",
    "#            ds_day.close()\n",
    "#            ds_day = ds_day.where(ds_day['mask_clim'] == 1.) \n",
    "#            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "#            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "#            if iwrap==1:\n",
    "#                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "#            if i==0:\n",
    "#                ds_storm_sst_clim = ds_storm\n",
    "#            else:\n",
    "#                ds_storm_sst_clim = xr.concat([ds_storm_sst_clim,ds_storm],dim='time')           \n",
    "            \n",
    "#ccmp wind data, no masked data, a complete field\n",
    "#            lyr, idyjl = 2015,1\n",
    "#            storm_date = dt.datetime(2015,1,1)\n",
    "            syr, smon, sdym, sjdy=str(storm_date.year),str(storm_date.month),str(storm_date.day),str(storm_date.timetuple().tm_yday)\n",
    "            fname_tem='/CCMP_Wind_Analysis_' + syr + smon.zfill(2) + sdym.zfill(2) + '_V02.0_L3.0_RSS.nc'\n",
    "            ccmp_filename = dir_ccmp + syr + '/M' + smon.zfill(2) + fname_tem      \n",
    "            ds=xr.open_dataset(ccmp_filename,drop_variables=['nobs'])\n",
    "            ds_day = ds.mean(dim='time')     #take average across all 6 hourly data fields\n",
    "            ds_day = ds_day.rename({'longitude':'lon','latitude':'lat'}) #, inplace = True)            \n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if i==0:\n",
    "                ds_storm_ccmp = ds_storm\n",
    "            else:\n",
    "                ds_storm_ccmp = xr.concat([ds_storm_ccmp,ds_storm],dim='time')\n",
    "              \n",
    "#ocean mixed layer depth from monthly data GODAS NOAA, lon 0 to 360, monthly data so interp to day\n",
    "#this is monthly data (all other data daily) so need to read in year before and year after\n",
    "#so any storms <1/15 or greater than 12/15 in the year still get data\n",
    "#dir_godas='https://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/godas/'\n",
    "            dir_godas = 'f:/data/model_data/godas/'\n",
    "            if isave_mld_year != storm_date.year:\n",
    "                filename = dir_godas + 'dbss_obml.' + str(storm_date.year-1) + '.nc'\n",
    "                ds_day_mld=xr.open_dataset(filename)\n",
    "                ds_day_mld['time']=ds_day_mld.time+np.timedelta64(14,'D')  #data provider gave 1st day of ave in time \n",
    "                ds_day_mld.close()\n",
    "                filename = dir_godas + 'dbss_obml.' + str(storm_date.year) + '.nc'\n",
    "                ds_day_mld2=xr.open_dataset(filename)\n",
    "                ds_day_mld2['time']=ds_day_mld2.time+np.timedelta64(14,'D')  #data provider gave 1st day of ave in time \n",
    "                ds_day_mld2.close()\n",
    "                ds_day_mld = xr.concat([ds_day_mld,ds_day_mld2],dim='time')\n",
    "                filename = dir_godas + 'dbss_obml.' + str(storm_date.year+1) + '.nc'\n",
    "                ds_day_mld2=xr.open_dataset(filename)\n",
    "                ds_day_mld2['time']=ds_day_mld2.time+np.timedelta64(14,'D')  #data provider gave 1st day of ave in time \n",
    "                ds_day_mld2.close()\n",
    "                ds_day_mld = xr.concat([ds_day_mld,ds_day_mld2],dim='time')\n",
    "                if iwrap==0:\n",
    "                    ds_day_mld.coords['lon'] = (ds_day_mld.coords['lon'] + 180) % 360 - 180\n",
    "                    ds_day_mld = ds_day_mld.sortby(ds_day_mld.lon)\n",
    "                isave_mld_year = storm_date.year\n",
    "            ds_storm = ds_day_mld.interp(time = storm_date, lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if i==0:\n",
    "                ds_storm_mld = ds_storm\n",
    "            else:\n",
    "                ds_storm_mld = xr.concat([ds_storm_mld,ds_storm],dim='time')            \n",
    "            \n",
    "#latent heat flux data, masked already set to NaN                \n",
    "            filename = dir_flux + 'lh_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_lhf = ds_storm\n",
    "            else:\n",
    "                ds_storm_lhf = xr.concat([ds_storm_lhf,ds_storm],dim='time')\n",
    "\n",
    "#sensible heat flux data , masked already set to NaN                \n",
    "            filename = dir_flux + 'sh_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_shf = ds_storm\n",
    "            else:\n",
    "                ds_storm_shf = xr.concat([ds_storm_shf,ds_storm],dim='time')\n",
    "\n",
    "#surface humid flux data   , masked already set to NaN              \n",
    "            filename = dir_flux + 'qa_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_qa = ds_storm\n",
    "            else:\n",
    "                ds_storm_qa = xr.concat([ds_storm_qa,ds_storm],dim='time')\n",
    "\n",
    "#air temp flux data   , masked already set to NaN              \n",
    "            filename = dir_flux + 'ta_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_ta = ds_storm\n",
    "            else:\n",
    "                ds_storm_ta = xr.concat([ds_storm_ta,ds_storm],dim='time')\n",
    "                \n",
    "#        ds_all = xr.merge([ds_storm_ccmp, ds_storm_mld, ds_storm_lhf, ds_storm_shf, ds_storm_ta, ds_storm_qa, ds_storm_sst, ds_storm_sst_clim])\n",
    "        ds_all = xr.merge([ds_storm_ccmp, ds_storm_mld, ds_storm_lhf, ds_storm_shf, ds_storm_ta, ds_storm_qa, ds_storm_sst])\n",
    "\n",
    "        #calculate mask\n",
    "#        print('caluculating mask')\n",
    "#        ds_mask = calculate_storm_mask(ds_all,lats,lons)\n",
    "#        ds_all['storm_mask']=ds_mask['storm_mask']\n",
    "#        #dist to storm\n",
    "#        print('calculating dist')\n",
    "#        dist,index,stime,position = closest_dist(ds_all,ds_storm_info)\n",
    "#        dtem=xr.DataArray(dist, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "#        ds_all['dist_from_storm_km']=dtem\n",
    "#        dtem=xr.DataArray(index, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "#        ds_all['closest_storm_index']=dtem\n",
    "#        dtem=xr.DataArray(stime, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "#        ds_all['closest_storm_time']=dtem\n",
    "#        dtem=xr.DataArray(position, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "#        ds_all['side_of_storm']=dtem\n",
    "\n",
    "        if iwrap==1:\n",
    "            ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "            ds_storm_info['lon'] = np.mod(ds_storm_info['lon'], 360)\n",
    "\n",
    "        #calculate mask\n",
    "        print('caluculating mask')\n",
    "        ds_mask = calculate_storm_mask(ds_all,lats,lons)\n",
    "        ds_all['storm_mask']=ds_mask['storm_mask']\n",
    "        #dist to storm\n",
    "        print('calculating dist')\n",
    "        dist,index,stime,position,ds_storm_interp = closest_dist(ds_all,ds_storm_info)\n",
    "        dtem=xr.DataArray(dist, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['dist_from_storm_km']=dtem\n",
    "        dtem=xr.DataArray(index, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['closest_storm_index']=dtem\n",
    "        dtem=xr.DataArray(stime, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['closest_storm_time']=dtem\n",
    "        dtem=xr.DataArray(position, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['side_of_storm']=dtem\n",
    "\n",
    "#add storm translation speed to storm information\n",
    "        tdim_storm = ds_storm_interp.time.size\n",
    "        storm_speed = ds_storm_interp.time.copy(deep=True)*np.nan    \n",
    "        for i in range(0,tdim_storm-1):\n",
    "            coords_1 = (ds_storm_interp.lat[0,i], ds_storm_interp.lon[0,i])  \n",
    "            coords_2 = (ds_storm_interp.lat[0,i+1], ds_storm_interp.lon[0,i+1])  \n",
    "            arclen_temp = geopy.distance.geodesic(coords_1, coords_2).km  #distance in km  \n",
    "            storm_date1 = np.datetime64(date_1858 + dt.timedelta(days=float(ds_storm_interp.time[0,i])))  \n",
    "            storm_date2 = np.datetime64(date_1858 + dt.timedelta(days=float(ds_storm_interp.time[0,i+1])))  \n",
    "            arclen_time = storm_date2 - storm_date1\n",
    "            arclen_hr = arclen_time / np.timedelta64(1, 'h')\n",
    "            storm_speed[0,i]=arclen_temp/(arclen_hr)\n",
    "        storm_speed[0,-1]=storm_speed[0,-2]\n",
    "        ds_storm_interp['storm_speed']=storm_speed\n",
    "        \n",
    "        xdim,ydim,tdim = ds_all.lon.shape[0],ds_all.lat.shape[0],ds_all.time.shape[0]\n",
    "        wtem=np.empty([ydim,xdim])\n",
    "        ptem=np.empty([ydim,xdim])\n",
    "        stem=np.empty([ydim,xdim])\n",
    "        for i in range(0,xdim):\n",
    "            for j in range(0,ydim):\n",
    "                storm_index = ds_all.closest_storm_index[j,i].data\n",
    "                wtem[j,i]=ds_storm_interp.wind[0,int(storm_index)].data\n",
    "                ptem[j,i]=ds_storm_interp.pres[0,int(storm_index)].data\n",
    "                stem[j,i]=ds_storm_interp.storm_speed[0,int(storm_index)].data\n",
    "        xrtem=xr.DataArray(wtem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n",
    "        ds_all['wmo_storm_wind']=xrtem\n",
    "        xrtem=xr.DataArray(ptem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n",
    "        ds_all['wmo_storm_pres']=xrtem\n",
    "        xrtem=xr.DataArray(stem, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))        \n",
    "        ds_all['wmo_storm_speed']=xrtem\n",
    "        \n",
    "        #find max sst 5 days before storm location\n",
    "        #first create an array with the storm crossover time (from nearest point) as an array\n",
    "        sdate = np.empty([ydim,xdim], dtype=dt.datetime)    \n",
    "        for i in range(0,xdim):\n",
    "            for j in range(0,ydim):\n",
    "                tem=date_1858+dt.timedelta(days=float(ds_all.closest_storm_time[j,i])) \n",
    "                sdate[j,i]=np.datetime64(tem)\n",
    "        xsdate=xr.DataArray(sdate, coords={'lat': ds_all.lat.values, 'lon':ds_all.lon.values}, dims=('lat', 'lon'))    \n",
    "        ds_all['closest_storm_time_np64']=xsdate\n",
    "        #now use array of storm time to calculate prestorm sst\n",
    "        sst0 = ds_all.dist_from_storm_km.copy(deep=True)\n",
    "        for i in range(0,xdim):\n",
    "            for j in range(0,ydim):\n",
    "                #sst0[j,i] = ds_data.analysed_sst[:,j,i].interp(time=xsdate[j,i])\n",
    "                sst0[j,i] = ds_all.analysed_sst[:,j,i].sel(time=slice(xsdate[j,i]-np.timedelta64(5,'D'),xsdate[j,i])).max()\n",
    "        ds_all['sst_prestorm']=sst0\n",
    "\n",
    "#now calculate coldwake information\n",
    "        if abs(ds_all.lon[-1]-ds_all.lon[0])>180:\n",
    "            ds_all.coords['lon'] = np.mod(ds_all['lon'], 360)\n",
    "            ds_storm_interp['lon'] = np.mod(ds_storm_interp['lon'], 360)\n",
    "        max_lat = ds_storm_interp.lat.max()\n",
    "\n",
    "    #remove all data outsice 100km/800km or cold wake >0 or <-10\n",
    "        if max_lat<0:\n",
    "            cond = ((((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm<=0)) | \n",
    "            ((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm>0))) )       \n",
    "        else:\n",
    "            cond = ((((ds_all.dist_from_storm_km<800) & (ds_all.side_of_storm<0)) | \n",
    "            ((ds_all.dist_from_storm_km<100) & (ds_all.side_of_storm>=0))))            \n",
    "        subset = ds_all.where(cond)\n",
    "          \n",
    "        xdim,ydim,tdim = ds_all.lon.shape[0],ds_all.lat.shape[0],ds_all.time.shape[0]\n",
    "        date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "        coldwake_max=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "        coldwake_maxindex=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "        coldwake_hrtomaxcold=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "        coldwake_recovery=ds_all.sst_prestorm.copy(deep=True)*np.nan\n",
    "#go through entire array lat/lon dims\n",
    "        for i in range(0,xdim):\n",
    "            for j in range(0,ydim):\n",
    "                 #calculate the storm time for the closest collocated storm point then find the combined data index for closest time\n",
    "                #this gives you the combined data storm index cross over\n",
    "                storm_date64 = ds_all.closest_storm_time_np64[j,i]\n",
    "                if np.isnan(subset.analysed_sst[0,j,i]):  #don't process masked values\n",
    "                    continue\n",
    "                time_diff = subset.time-storm_date64\n",
    "                storm_index = np.argmin(abs(time_diff)).data\n",
    "                #now look for cold wake for 1 day before strom to 5 days after strom\n",
    "                #caluclate hours to cold wake, maximum cold wake, hours until it returns to prestorm sst\n",
    "                #there is NO filter on wheither coldwake large enough here, just does all points\n",
    "                istart,iend = int(storm_index),int(storm_index)+5\n",
    "                if iend>tdim:\n",
    "                    iend=tdim\n",
    "                if np.isnan(subset.sst_prestorm[j,i]):\n",
    "                    continue\n",
    "                coldwake_max[j,i] = (subset.analysed_sst[istart:iend,j,i]-subset.sst_prestorm[j,i]).min()\n",
    "                if all(np.isnan(subset.analysed_sst[istart:iend,j,i])):\n",
    "                    continue\n",
    "                itmp = np.argmin(subset.analysed_sst[istart:iend,j,i]-subset.sst_prestorm[j,i]).data\n",
    "                coldwake_maxindex[j,i]=istart+itmp\n",
    "                delay = subset.time[istart+itmp].values-subset.time[istart].values\n",
    "                coldwake_hrtomaxcold[j,i]=delay / np.timedelta64(1, 'h')\n",
    "                for k in range(istart+itmp,tdim):\n",
    "                    sst_change = subset.analysed_sst[k,j,i]-subset.sst_prestorm[j,i]\n",
    "                    if sst_change>-0.2:\n",
    "                        break\n",
    "                delay = subset.time[k].values-subset.time[istart].values\n",
    "                coldwake_recovery[j,i]=delay / np.timedelta64(1, 'D')\n",
    "\n",
    "        ds_all['coldwake_max']=coldwake_max\n",
    "        ds_all['coldwake_maxindex']=coldwake_maxindex\n",
    "        ds_all['coldwake_hrtomaxcold']=coldwake_hrtomaxcold\n",
    "        ds_all['coldwake_dytorecovery']=coldwake_recovery\n",
    "        \n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "        ds_all.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_interpolated_track.nc'\n",
    "        ds_storm_interp.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(istart)\n",
    "print(itmp)\n",
    "print(k)\n",
    "print(subset.time[k].data,subset.time[istart].data)\n",
    "print(coldwake_recovery[j,i].data)\n",
    "plt.plot(subset.analysed_sst[istart:,j,i]-subset.sst_prestorm[j,i],'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldwake_recovery.plot(vmin=0,vmax=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(coldwake_max,vmin=-1,vmax=1)\n",
    "#coldwake_max.plot()\n",
    "#plt.plot(ds_storm_interp.lon[0,:]+360,ds_storm_interp.lat[0,:],'r')\n",
    "#plt.plot(ds_storm_interp.lon[0,0]+360,ds_storm_interp.lat[0,0],'r*')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.analysed_sst[:,60,210].plot()\n",
    "plt.plot(subset.time[istart],subset.analysed_sst[istart,60,240],'r*')\n",
    "plt.plot(subset.time[istart],subset.analysed_sst[istart,60,240],'r*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.time[istart]\n",
    "print(subset.lon[240].data,subset.lat[60].data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_storm_interp.wind[0,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_storm_interp.time[0,25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_storm_interp.time[0,0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            dir_godas = 'f:/data/model_data/godas/'\n",
    "            if isave_mld_year != storm_date.year:\n",
    "                filename = dir_godas + 'dbss_obml.' + syr + '.nc'\n",
    "                ds_day_mld=xr.open_dataset(filename)\n",
    "                ds_day_mld['time']=ds_day_mld.time+np.timedelta64(14,'D')  #data provider gave 1st day of ave in time \n",
    "                if iwrap==0:\n",
    "                    ds_day_mld.coords['lon'] = (ds_day_mld.coords['lon'] + 180) % 360 - 180\n",
    "                    ds_day_mld = ds_day_mld.sortby(ds_day_mld.lon)\n",
    "                ds_day_mld.close()\n",
    "                isave_mld_year = storm_date.year\n",
    "            ds_storm = ds_day_mld.interp(time = storm_date, lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if i==0:\n",
    "                ds_storm_mld = ds_storm\n",
    "            else:\n",
    "                ds_storm_mld = xr.concat([ds_storm_mld,ds_storm],dim='time')            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_day_mld.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
