{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input directories\n",
    "dir_storm_info='f:/data/tc_wakes/database/info/'\n",
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "dir_flux = 'F:/data/model_data/oaflux/data_v3/daily/turbulence/'\n",
    "dir_cmc = 'F:/data/sst/cmc/CMC0.2deg/v2/'\n",
    "dir_ccmp='F:/data/sat_data/ccmp/v02.0/Y'\n",
    "##where to get the data through opendap, use these directories instead\n",
    "#dir_cmc = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/CMC/CMC0.1deg/v3/'\n",
    "#dir_flux = 'http://apdrc.soest.hawaii.edu:80/dods/public_data/WHOI_OAFlux/version3/daily/lh_oaflux/'\n",
    "#the latest ccmp is from www.remss.com but they do not have an opendap server so you can use this instead:\n",
    "#dir_ccmp='https://podaac-opendap.jpl.nasa.gov/opendap/allData/ccmp/L3.0/flk/'\n",
    "\n",
    "#################################################################################\n",
    "import geopy.distance\n",
    "from math import sin, pi\n",
    "from scipy import interpolate\n",
    "\n",
    "#functions for running storm data\n",
    "def interpolate_storm_path(dsx):\n",
    "    #after calculating the distance from the storm it became clear that the storm data is every 6 hours, no matter \n",
    "    #how much it may have moved.  So if the storm moved 300 km in 6 hr, when calculating the distance to the storm\n",
    "    #there were points on the storm track that showed large distances because of the separation to the 6hrly storm points\n",
    "    #this subroutine interpolates the storm path onto a higher spatial resolution\n",
    "    #the new storm dataset is carefully put into an identical format with i2 and j2 as dims to match the old format\n",
    "    ynew = []\n",
    "    tnew = []\n",
    "    xnew = []\n",
    "    dsx['lon'] = (dsx.lon-180) % 360 - 180 #put -180 to 180\n",
    "    for istep in range(1,dsx.lon.shape[1]):\n",
    "        dif_lat = dsx.lat[0,istep]-dsx.lat[0,istep-1]\n",
    "        dif_lon = dsx.lon[0,istep]-dsx.lon[0,istep-1]\n",
    "        x,y,t = dsx.lon[0,istep-1:istep+1].values,dsx.lat[0,istep-1:istep+1].values,dsx.time[0,istep-1:istep+1].values\n",
    "        if abs(dif_lat)>abs(dif_lon):\n",
    "            isign = np.sign(dif_lat)\n",
    "            if abs(dif_lat)>0.75:\n",
    "                ynew1 = np.arange(y[0], y[-1], isign.data*0.75)\n",
    "                f = interpolate.interp1d(y, x, assume_sorted=False)\n",
    "                xnew1 = f(ynew1)\n",
    "                f = interpolate.interp1d(y, t, assume_sorted=False)\n",
    "                tnew1 = f(ynew1)\n",
    "            else:\n",
    "                xnew1,ynew1,tnew1 = x,y,t\n",
    "            xnew,ynew,tnew = np.append(xnew,xnew1),np.append(ynew,ynew1),np.append(tnew,tnew1) \n",
    "        else:\n",
    "            isign = np.sign(dif_lon)\n",
    "            if abs(dif_lon)>0.75:\n",
    "                iwrap_interp = 1\n",
    "                if (x[0]<-90) & (x[-1]>90):\n",
    "                    iwrap_interp = -1\n",
    "                    x[0]=x[0]+360\n",
    "                if (x[0]>90) & (x[-1]<-90):\n",
    "                    iwrap_interp = -1\n",
    "                    x[-1]=x[-1]+360\n",
    "                xnew1 = np.arange(x[0], x[-1], iwrap_interp*isign.data*0.75)\n",
    "                f = interpolate.interp1d(x, y, assume_sorted=False)\n",
    "                ynew1 = f(xnew1)\n",
    "                f = interpolate.interp1d(x, t, assume_sorted=False)\n",
    "                tnew1 = f(xnew1)\n",
    "                xnew1 = (xnew1 - 180) % 360 - 180 #put -180 to 180\n",
    "            else:\n",
    "                xnew1,ynew1,tnew1 = x,y,t\n",
    "            xnew,ynew,tnew = np.append(xnew,xnew1),np.append(ynew,ynew1),np.append(tnew,tnew1) \n",
    "    i2,j2=xnew.shape[0],1\n",
    "    tem = np.expand_dims(xnew, axis=0)\n",
    "    xx = xr.DataArray(tem.T,dims=['i2','j2'])\n",
    "    tem = np.expand_dims(ynew, axis=0)\n",
    "    yy = xr.DataArray(tem.T,dims=['i2','j2'])\n",
    "    tem = np.expand_dims(tnew, axis=0)\n",
    "    tt = xr.DataArray(tem.T,dims=['i2','j2'])\n",
    "    dsx_new = xr.Dataset({'lon':xx.T,'lat':yy.T,'time':tt.T})\n",
    "    return dsx_new\n",
    "\n",
    "def get_dist_grid(lat_point,lon_point,lat_grid,lon_grid):\n",
    "    #this routine takes a point and finds distance to all points in a grid of lat and lon\n",
    "    #it is slowwwwwww\n",
    "    dist_grid = np.empty(lat_grid.shape)    \n",
    "    coords_1 = (lat_point, lon_point)  \n",
    "    for i in range(0,lat_grid.shape[0]):\n",
    "        for j in range(0,lat_grid.shape[1]):\n",
    "            coords_2 = (lat_grid[i,j], lon_grid[i,j])  \n",
    "            arclen_temp = geopy.distance.geodesic(coords_1, coords_2).km  #distance in km       \n",
    "            dist_grid[i,j]=arclen_temp\n",
    "    return dist_grid\n",
    "\n",
    "\n",
    "def closest_dist(ds_in,ds_storm): \n",
    "# m.garcia-reyes 2.4.2019, edited c.gentemann 2.4.2019\n",
    "# calculate distance closest storm point\n",
    "# point given as tla,tlo.... storm is in the program\n",
    "# initialize distances (in km)\n",
    "    ds_storm['lon'] = (ds_storm.lon + 180) % 360 - 180\n",
    "    dsx_input = ds_storm.copy(deep=True)\n",
    "    ds_storm_new = interpolate_storm_path(dsx_input)       \n",
    "    tdim,xdim,ydim=ds_storm_new.lat.shape[1], ds_in.analysed_sst[0,:,0].shape[0], ds_in.analysed_sst[0,0,:].shape[0]\n",
    "    dx_save=np.zeros([tdim,xdim,ydim])\n",
    "    dx_grid,dy_grid = np.meshgrid(ds_in.lon.values,ds_in.lat.values)\n",
    "    lon_grid,lat_grid = np.meshgrid(ds_in.lon.values,ds_in.lat.values)\n",
    "    min_dist_save = np.zeros([xdim,ydim])*np.nan\n",
    "    min_index_save = np.zeros([xdim,ydim])*np.nan\n",
    "    min_time_save = np.zeros([xdim,ydim])*np.nan\n",
    "    #for each location of the storm calculate the difference for all values in box\n",
    "    for ipt in range(0,ds_storm_new.lat.shape[1]):  # all storm values\n",
    "        dist_tem_grid = get_dist_grid(ds_storm_new.lat[0,ipt].values,ds_storm_new.lon[0,ipt].values,lat_grid,lon_grid)\n",
    "        dx_save[ipt,:,:]=dist_tem_grid       \n",
    "    #now go through each value in box and find minimum storm location/day\n",
    "    for j in range(0,ds_in.lon.shape[0]):\n",
    "        for i in range(0,ds_in.lat.shape[0]):\n",
    "            imin = np.argmin(dx_save[:,i,j])\n",
    "            min_dist_save[i,j]=dx_save[imin,i,j]\n",
    "            min_index_save[i,j]=imin\n",
    "            min_time_save[i,j]=ds_storm_new.time[0,imin]\n",
    "    return min_dist_save,min_index_save,min_time_save\n",
    "\n",
    "def calculate_storm_mask(ds_sst,lats,lons):\n",
    "#make a mask for the storm and only keep data within -4 and 10 degrees of storm track\n",
    "#this was written before I had calculated the closest_dist which is probably a better mask to use\n",
    "    iwrap_mask = 0\n",
    "    if (ds_sst.lon.max().values>170) & (ds_sst.lon.min().values<-170):\n",
    "        iwrap_mask=1\n",
    "    print(ds_sst.lon.min().values,ds_sst.lon.max().values)\n",
    "#okay, now ds_storm is array with right lat/lon for storm so create mask now\n",
    "    ds_mask = ds_sst.copy(deep=True)\n",
    "    ds_mask['storm_mask']=ds_mask.analysed_sst*0\n",
    "    ds_mask = ds_mask.fillna(0)\n",
    "    ds_mask['storm_mask'] = ds_mask['storm_mask'].astype(int,copy=True)\n",
    "    for i in range(0,lats.shape[0]):\n",
    "        if lats[i]>0:   #northern hemi on right, southers on left\n",
    "            lons1,lons2=lons[i]-4,lons[i]+10\n",
    "        else:\n",
    "            lons1,lons2=lons[i]-10,lons[i]+4\n",
    "        lats1,lats2=lats[i]-10,lats[i]+10\n",
    "        if i==0:\n",
    "            print('lons1,lons2:',iwrap_mask,lons1.data,lons2.data)\n",
    "        if lons1<-180:\n",
    "            ds_mask['storm_mask'].loc[dict(lon=(ds_mask.lon < lons2) | (ds_mask.lon > lons1+360), lat=slice(lats1,lats2))] = -1\n",
    "        elif lons2>180:\n",
    "            ds_mask['storm_mask'].loc[dict(lon=(ds_mask.lon < lons2-360) | (ds_mask.lon > lons1), lat=slice(lats1,lats2))] = -1\n",
    "        else:\n",
    "            if iwrap_mask==1:\n",
    "                ds_mask.coords['lon'] = np.mod(ds_mask['lon'], 360)\n",
    "                ds_mask = ds_mask.sortby(ds_mask.lon)\n",
    "                ds_mask['storm_mask'].loc[dict(lon=slice(lons1+360,lons2+360), lat=slice(lats1,lats2))] = -1\n",
    "                ds_mask.coords['lon'] = (ds_mask.coords['lon'] + 180) % 360 - 180\n",
    "            else:\n",
    "                ds_mask['storm_mask'].loc[dict(lon=slice(lons1,lons2), lat=slice(lats1,lats2))] = -1\n",
    "    return ds_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir_ccmp='F:/data/sat_data/ccmp/v02.0/Y'\n",
    "#date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "#dx=0.25\n",
    "#dy=0.25\n",
    "#dx_offset = -179.875\n",
    "#dy_offset = -78.3750\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "isave_mld_year = 0 #init MLD monthly data read flag\n",
    "for root, dirs, files in os.walk(dir_storm_info, topdown=False):\n",
    "    if root[len(dir_storm_info):len(dir_storm_info)+1]=='.':\n",
    "        continue\n",
    "    for name in files:\n",
    "        if not name.endswith('.nc'):\n",
    "            continue\n",
    "        filename=os.path.join(root, name)\n",
    "        print(filename[36:39],filename[31:35])\n",
    "        inum_storm=int(filename[36:39])\n",
    "        iyr_storm=int(filename[31:35])\n",
    "#        if iyr_storm!=2007: # or iyr_storm<2003:\n",
    "#            continue\n",
    "        print(name,filename)\n",
    "        ds_storm_info = xr.open_dataset(filename)\n",
    "        lats = ds_storm_info.lat[0,:]\n",
    "        lons = ds_storm_info.lon[0,:]  #lons goes from 0 to 360\n",
    "        lons = (lons + 180) % 360 - 180 #put -180 to 180\n",
    "        dysince = ds_storm_info.time\n",
    "        ds_storm_info.close()\n",
    "        \n",
    "#make lat and lon of storm onto 25 km grid for below\n",
    "        lons = (((lons - .125)/.25+1).astype(int)-1)*.25+.125\n",
    "        lats = (((lats + 89.875)/.25+1).astype(int)-1)*.25-89.875\n",
    "        \n",
    "        iwrap=0\n",
    "#calculate size of box to get data in\n",
    "        minlon,maxlon = min(lons.values)-10, max(lons.values)+10\n",
    "        minlat,maxlat = min(lats.values)-10, max(lats.values)+10\n",
    "\n",
    "        ydim_storm = round((maxlat - minlat)/.25).astype(int)\n",
    "        new_lat_storm = np.linspace(minlat, maxlat, ydim_storm)\n",
    "        if (minlon<-90 and maxlon>=90) or (minlon<-180 and maxlon<0):  #this storm wraps  keep everythig 0 to 360 then wrap data at very end\n",
    "            iwrap = 1\n",
    "            lons2 = np.mod(lons, 360)\n",
    "            minlon, maxlon = min(lons2.values)-10, max(lons2.values)+10\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "        else:\n",
    "            xdim_storm = round((maxlon - minlon)/.25).astype(int)\n",
    "            new_lon_storm = np.linspace(minlon, maxlon, xdim_storm)\n",
    "\n",
    "        print(iwrap,minlon,maxlon)\n",
    "        print(iwrap,xdim_storm, new_lon_storm[:5],new_lon_storm[-5:])\n",
    "\n",
    "        \n",
    "        dims=lats.shape\n",
    "        tdim=dims[0]\n",
    "        tem_date=[0]*tdim #print(dysince.values)\n",
    "        for i in range(0,tdim):\n",
    "            tem_date[i]=date_1858+dt.timedelta(days=float(dysince[0,i].values))  #create new time array that can be queried for year etc\n",
    "        min_date = min(tem_date)+dt.timedelta(days=-5)\n",
    "        max_date = max(tem_date)+dt.timedelta(days=5)\n",
    "        minjdy = min_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        minyear =min_date.year #create new time array that can be queried for year etc\n",
    "        minmon =min_date.month #create new time array that can be queried for year etc\n",
    "        minday =min_date.day #create new time array that can be queried for year etc\n",
    "        maxjdy = max_date.timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        maxyear =max_date.year  #create new time array that can be queried for year etc\n",
    "        print(minyear,minjdy,maxyear,maxjdy)\n",
    "        \n",
    "        dif = max(tem_date)-min(tem_date)\n",
    "        tdim=int(dif.days)+30             #calculate ssts for 30 days after storm\n",
    "\n",
    "        #print(tdim,xdim,ydim)            \n",
    "        \n",
    "        #print('sst_out_sv',sst_out_sv.shape)\n",
    "        for i in range(0,tdim):\n",
    "            storm_date = dt.datetime(minyear,minmon,minday)+dt.timedelta(days=i)+dt.timedelta(hours=12)\n",
    "            #print(storm_date)\n",
    "            \n",
    "            syr=str(storm_date.year)\n",
    "            smon=str(storm_date.month)\n",
    "            sdym=str(storm_date.day)\n",
    "            sjdy=str(storm_date.timetuple().tm_yday)\n",
    "\n",
    "#sst data   \n",
    "            fname_tem=syr + smon.zfill(2) + sdym.zfill(2) + '120000-CMC-L4_GHRSST-SSTfnd-CMC0.2deg-GLOB-v02.0-fv02.0.nc'\n",
    "            filename = dir_cmc + syr + '/' + sjdy.zfill(3) + '/' + fname_tem\n",
    "            ds_day=xr.open_dataset(filename,drop_variables=['analysis_error','sea_ice_fraction'])\n",
    "            if iwrap==1:  #data is -180 to 180 for sst, so need to bring to 0 to 360 when wrapped\n",
    "                ds_day.coords['lon'] = np.mod(ds_day['lon'], 360)\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds_day.close()\n",
    "            ds_day = ds_day.where(ds_day['mask'] == 1.) \n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            #ds_storm['time']=storm_date\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            if i==0:\n",
    "                ds_storm_sst = ds_storm\n",
    "            else:\n",
    "                ds_storm_sst = xr.concat([ds_storm_sst,ds_storm],dim='time')\n",
    "\n",
    "#sst climatology  --- this isn't used, should remove from dataset in next round\n",
    "#            if storm_date.timetuple().tm_yday==366:\n",
    "#                sjdy = '365'\n",
    "#            filename='F:/data/sst/cmc/CMC0.2deg/v2/climatology/clim1993_2016' + sjdy.zfill(3) + '-CMC-L4_GHRSST-SSTfnd-CMC0.2deg-GLOB-v02.0-fv02.0.nc'\n",
    "#            ds_day=xr.open_dataset(filename,drop_variables=['analysis_error','sea_ice_fraction','sq_sst'])\n",
    "#            ds_day = ds_day.rename({'analysed_sst':'analysed_sst_clim','mask':'mask_clim'}) #, inplace = True)            \n",
    "#            if iwrap==1:  #data is -180 to 180 for sst, so need to bring to 0 to 360 when wrapped\n",
    "#                ds_day.coords['lon'] = np.mod(ds_day['lon'], 360)\n",
    "#                ds_day = ds_day.sortby(ds_day.lon)\n",
    "#            ds_day.close()\n",
    "#            ds_day = ds_day.where(ds_day['mask_clim'] == 1.) \n",
    "#            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "#            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "#            if iwrap==1:\n",
    "#                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "#            if i==0:\n",
    "#                ds_storm_sst_clim = ds_storm\n",
    "#            else:\n",
    "#                ds_storm_sst_clim = xr.concat([ds_storm_sst_clim,ds_storm],dim='time')           \n",
    "            \n",
    "#ccmp wind data, no masked data, a complete field\n",
    "            lyr, idyjl = 2015,1\n",
    "#            storm_date = dt.datetime(2015,1,1)\n",
    "            syr, smon, sdym, sjdy=str(storm_date.year),str(storm_date.month),str(storm_date.day),str(storm_date.timetuple().tm_yday)\n",
    "            fname_tem='/CCMP_Wind_Analysis_' + syr + smon.zfill(2) + sdym.zfill(2) + '_V02.0_L3.0_RSS.nc'\n",
    "            ccmp_filename = dir_ccmp + syr + '/M' + smon.zfill(2) + fname_tem      \n",
    "            ds=xr.open_dataset(ccmp_filename,drop_variables=['nobs'])\n",
    "            ds_day = ds.mean(dim='time')     #take average across all 6 hourly data fields\n",
    "            ds_day = ds_day.rename({'longitude':'lon','latitude':'lat'}) #, inplace = True)            \n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if i==0:\n",
    "                ds_storm_ccmp = ds_storm\n",
    "            else:\n",
    "                ds_storm_ccmp = xr.concat([ds_storm_ccmp,ds_storm],dim='time')\n",
    "              \n",
    "#ocean mixed layer depth from monthly data GODAS NOAA, lon 0 to 360, monthly data so interp to day\n",
    "            #dir_godas='https://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/godas/'\n",
    "            dir_godas = 'f:/data/model_data/godas/'\n",
    "            if isave_mld_year != storm_date.year:\n",
    "                filename = dir_godas + 'dbss_obml.' + syr + '.nc'\n",
    "                ds_day_mld=xr.open_dataset(filename)\n",
    "                if iwrap==0:\n",
    "                    ds_day_mld.coords['lon'] = (ds_day_mld.coords['lon'] + 180) % 360 - 180\n",
    "                    ds_day_mld = ds_day_mld.sortby(ds_day_mld.lon)\n",
    "                ds_day_mld.close()\n",
    "                isave_mld_year = storm_date.year\n",
    "            ds_storm = ds_day_mld.interp(time = storm_date, lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm = ds_storm.assign_coords(time=storm_date)\n",
    "            if i==0:\n",
    "                ds_storm_mld = ds_storm\n",
    "            else:\n",
    "                ds_storm_mld = xr.concat([ds_storm_mld,ds_storm],dim='time')            \n",
    "            \n",
    "#latent heat flux data, masked already set to NaN                \n",
    "            filename = dir_flux + 'lh_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_lhf = ds_storm\n",
    "            else:\n",
    "                ds_storm_lhf = xr.concat([ds_storm_lhf,ds_storm],dim='time')\n",
    "\n",
    "#sensible heat flux data , masked already set to NaN                \n",
    "            filename = dir_flux + 'sh_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_shf = ds_storm\n",
    "            else:\n",
    "                ds_storm_shf = xr.concat([ds_storm_shf,ds_storm],dim='time')\n",
    "\n",
    "#surface humid flux data   , masked already set to NaN              \n",
    "            filename = dir_flux + 'qa_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_qa = ds_storm\n",
    "            else:\n",
    "                ds_storm_qa = xr.concat([ds_storm_qa,ds_storm],dim='time')\n",
    "\n",
    "#air temp flux data   , masked already set to NaN              \n",
    "            filename = dir_flux + 'ta_oaflux_' + syr + '.nc';\n",
    "            ds=xr.open_dataset(filename,drop_variables=['err'])\n",
    "            ds_day = ds.sel(time = storm_date.timetuple().tm_yday)  #select day of year from annual file\n",
    "            if iwrap==0:\n",
    "                ds_day.coords['lon'] = (ds_day.coords['lon'] + 180) % 360 - 180\n",
    "                ds_day = ds_day.sortby(ds_day.lon)\n",
    "            ds.close()\n",
    "            ds_storm = ds_day.interp(lat = new_lat_storm,lon = new_lon_storm)\n",
    "            if iwrap==1:\n",
    "                ds_storm.coords['lon'] = (ds_storm.coords['lon'] + 180) % 360 - 180\n",
    "            ds_storm['time']=storm_date\n",
    "            if i==0:\n",
    "                ds_storm_ta = ds_storm\n",
    "            else:\n",
    "                ds_storm_ta = xr.concat([ds_storm_ta,ds_storm],dim='time')\n",
    "                \n",
    "#        ds_all = xr.merge([ds_storm_ccmp, ds_storm_mld, ds_storm_lhf, ds_storm_shf, ds_storm_ta, ds_storm_qa, ds_storm_sst, ds_storm_sst_clim])\n",
    "        ds_all = xr.merge([ds_storm_ccmp, ds_storm_mld, ds_storm_lhf, ds_storm_shf, ds_storm_ta, ds_storm_qa, ds_storm_sst])\n",
    "\n",
    "        #calculate mask\n",
    "        print('caluculating mask')\n",
    "        ds_mask = calculate_storm_mask(ds_all,lats,lons)\n",
    "        ds_all['storm_mask']=ds_mask['storm_mask']\n",
    "        #dist to storm\n",
    "        print('calculating dist')\n",
    "        dist,index,stime = closest_dist(ds_all,ds_storm_info)\n",
    "        dtem=xr.DataArray(dist, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['dist_from_storm_km']=dtem\n",
    "        dtem=xr.DataArray(index, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['closest_storm_index']=dtem\n",
    "        dtem=xr.DataArray(stime, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_all['closest_storm_time']=dtem\n",
    "\n",
    "        \n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "        ds_all.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "     # filename = dir_out + str(iyr_storm) + '/' + 'str(inum_storm)' + '_other_data.nc'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all.closest_storm_time.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test distance from storm\n",
    "#read in some data from a storm that wraps\n",
    "iwrap=0\n",
    "iyr_storm,inum_storm=2002,1\n",
    "filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "ds = xr.open_dataset(filename)\n",
    "ds.close()\n",
    "ds_sst = ds.copy(deep=True)   #make a deep copy\n",
    "dsx = xr.open_dataset('F:/data/tc_wakes/database/info/'+str(iyr_storm)+'/'+ str(inum_storm).zfill(3) +'annual_storm_info.nc')\n",
    "dsx.close()\n",
    "#dist,index,stime = closest_dist(ds_sst,dsx)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.pcolormesh(dx_save[imin,:,:])\n",
    "print(imin)\n",
    "plt.plot(dx_save[:,i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this interpolater better than numpy interpolater because you don't have to force \n",
    "iyr_storm, inum_storm = 2002,2\n",
    "dsx = xr.open_dataset('F:/data/tc_wakes/database/info/'+str(iyr_storm)+'/'+ str(inum_storm).zfill(3) +'annual_storm_info.nc')\n",
    "dsx['lon'] = (dsx.lon + 180) % 360 - 180 #put -180 to 180\n",
    "dsx.close()\n",
    "dif_lat = dsx.lat[0,-1]-dsx.lat[0,0]\n",
    "dif_lon = dsx.lon[0,-1]-dsx.lon[0,0]\n",
    "print(dif_lat.values,dif_lon.values)\n",
    "if abs(dif_lat)>abs(dif_lon):\n",
    "    print('norm')\n",
    "    x,y,t = dsx.lat[0,:].values,dsx.lon[0,:].values,dsx.time[0,:].values\n",
    "    isign = np.sign(dif_lat)\n",
    "    xnew = np.arange(x[0], x[-1], isign.data*0.25)\n",
    "    f = interpolate.interp1d(x, y, assume_sorted=False)\n",
    "    ynew = f(xnew)\n",
    "    f = interpolate.interp1d(x, t, assume_sorted=False)\n",
    "    tnew = f(xnew)\n",
    "else:\n",
    "    print('test')\n",
    "    x,y,t = dsx.lat[0,:].values,dsx.lon[0,:].values,dsx.time[0,:].values\n",
    "    isign = np.sign(dif_lon)\n",
    "    ynew = np.arange(y[0], y[-1], isign*0.25)\n",
    "    f = interpolate.interp1d(y, x, assume_sorted=False)\n",
    "    xnew = f(ynew)\n",
    "    f = interpolate.interp1d(t, t, assume_sorted=False)\n",
    "    tnew = f(tnew)\n",
    "    \n",
    "print(x[0:5])\n",
    "print(y[0:5])\n",
    "print(xnew[0:5])\n",
    "print(ynew[0:5])\n",
    "print(t[0:5])\n",
    "print(tnew[0:5])\n",
    "plt.plot(x,y,'.-')\n",
    "plt.plot(xnew,ynew,'r.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this interpolater better than numpy interpolater because you don't have to force \n",
    "#test do step by step through storm\n",
    "            \n",
    "\n",
    "iyr_storm, inum_storm = 2002,13\n",
    "dsx = xr.open_dataset('F:/data/tc_wakes/database/info/'+str(iyr_storm)+'/'+ str(inum_storm).zfill(3) +'annual_storm_info.nc')\n",
    "dsx.close()\n",
    "dsx['lon'] = (dsx.lon-180) % 360 - 180 #put -180 to 180\n",
    "dsx_input = dsx.copy(deep=True)\n",
    "dsx_new = interpolate_storm_path(dsx_input)    \n",
    "print(dsx.lon[0,:])\n",
    "plt.plot(dsx.lon[0,:],dsx.lat[0,:],'b.-')\n",
    "plt.plot(dsx_new.lon[0,:],dsx_new.lat[0,:],'r.')\n",
    "#plt.plot(xnew,ynew,'r.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dsx)\n",
    "print(dsx_new)\n",
    "#dsx_new['lon']=xnew\n",
    "#plt.plot(dsx.lon[0,:],dsx.time[0,:],'b-.')\n",
    "#plt.plot(xnew,tnew,'r.')\n",
    "#print(dsx.lon[0,0:10])\n",
    "#print(xnew[10:25])\n",
    "#print(ynew[10:25])\n",
    "#print(tnew[10:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwrap=0\n",
    "iyr_storm,inum_storm=2002,5\n",
    "filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "ds = xr.open_dataset(filename)\n",
    "ds.close()\n",
    "ds_sst = ds.copy(deep=True)   #make a deep copy\n",
    "dsx = xr.open_dataset('F:/data/tc_wakes/database/info/'+str(iyr_storm)+'/'+ str(inum_storm).zfill(3) +'annual_storm_info.nc')\n",
    "dsx.close()\n",
    "\n",
    "ds_storm = dsx\n",
    "ds_in = ds_sst\n",
    "ds_storm['lon'] = (ds_storm.lon + 180) % 360 - 180\n",
    "ds_storm_new = interpolate_storm_path(ds_storm)       \n",
    "tdim,xdim,ydim=ds_storm_new.lat.shape[1], ds_in.analysed_sst[0,:,0].shape[0], ds_in.analysed_sst[0,0,:].shape[0]\n",
    "dx_save=np.zeros([tdim,xdim,ydim])\n",
    "print(ds_in.analysed_sst.shape)\n",
    "dx_grid,dy_grid = np.meshgrid(ds_in.lon.values,ds_in.lat.values)\n",
    "lon_grid,lat_grid = np.meshgrid(ds_in.lon.values,ds_in.lat.values)\n",
    "min_dist_save = np.zeros(ds_in.analysed_sst[0,:,:].shape)*np.nan\n",
    "min_index_save = np.zeros(ds_in.analysed_sst[0,:,:].shape)*np.nan\n",
    "#for each location of the storm calculate the difference for all values in box\n",
    "for ipt in range(0,ds_storm_new.lat.shape[1]):  # all storm values\n",
    "    dist_tem_grid = get_dist_grid(ds_storm_new.lat[0,ipt].values,ds_storm_new.lon[0,ipt].values,lat_grid,lon_grid)\n",
    "    dx_save[ipt,:,:]=dist_tem_grid\n",
    "#now go through each value in box and find minimum storm location/day\n",
    "for j in range(0,ds_in.lon.shape[0]):\n",
    "    for i in range(0,ds_in.lat.shape[0]):\n",
    "        imin = np.argmin(dx_save[:,i,j])\n",
    "        min_dist_save[i,j]=dx_save[imin,i,j]\n",
    "        min_index_save[i,j]=imin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.pcolormesh(dx_save[20,:,:],vmin = 0, vmax = 1000)\n",
    "#plt.pcolormesh(min_dist_save,vmin = 0, vmax = 1000)\n",
    "plt.pcolormesh(dist,vmin = 0, vmax = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_in = ds_sst\n",
    "ds_storm = dsx\n",
    "ds_storm['lon'] = (ds_storm.lon + 180) % 360 - 180\n",
    "tdim,xdim,ydim=ds_storm.lat.shape[1], ds_in.analysed_sst[0,:,0].shape[0], ds_in.analysed_sst[0,0,:].shape[0]\n",
    "dx_save=np.zeros([tdim,xdim,ydim])\n",
    "print(ds_in.analysed_sst.shape)\n",
    "lon_grid,lat_grid = np.meshgrid(ds_in.lon.values,ds_in.lat.values)\n",
    "\n",
    "ipt = 1\n",
    "lat_point,lon_point = ds_storm.lat[0,ipt],ds_storm.lon[0,ipt]\n",
    "dist_grid = np.empty(lat_grid.shape)    \n",
    "coords_1 = (lat_point, lon_point)  \n",
    "for i in range(0,lat_grid.shape[0]):\n",
    "    for j in range(0,lat_grid.shape[1]):\n",
    "        coords_2 = (lat_grid[i,j], lon_grid[i,j])  \n",
    "        arclen_temp = geopy.distance.geodesic(coords_1, coords_2).km  #distance in km       \n",
    "        dist_grid[i,j]=arclen_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(dist_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_storm_ccmp.vwnd[20,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename2 = 'f:/data/model_data/godas/dbss_obml.2002.nc'\n",
    "ds_day_mld=xr.open_dataset(filename2)\n",
    "ds_day_mld.close()\n",
    "ds_day_mld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isave_mld_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir_godas + 'dbss_obml.' + syr + '.nc')\n",
    "ds_day=xr.open_dataset(dir_godas + 'dbss_obml.' + syr + '.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#OLD CODE using netcdf and calculating location directly rather than using xarray\n",
    "###### dir_mur = 'F:/data/sst/jpl_mur/v4.1/'\n",
    "for root, dirs, files in os.walk(dir_storm_info, topdown=False):\n",
    "#    for ii in range(12,13): \n",
    "    for name in files:\n",
    "    #    name = files[ii]\n",
    "#    for name in files:\n",
    "        fname_in=os.path.join(root, name)\n",
    "        fname_out=dir_out + fname_in[31:39] + '_all_25km.nc'\n",
    "        inum_storm=int(fname_in[36:39])\n",
    "        iyr_storm=int(fname_in[31:35])\n",
    "        if iyr_storm>2003 or iyr_storm<2003:\n",
    "            continue\n",
    "#        if iyr_storm==2011 and inum_storm<15:\n",
    "#            continue\n",
    "        print(name,fname_in)\n",
    "        dsx = xr.open_dataset(fname_in)\n",
    "        lats = dsx.lat[0,:]\n",
    "        lons = dsx.lon[0,:]  #lons goes from 0 to 360\n",
    "        dysince = dsx.time\n",
    "        #minlon=min(lons[0,:].values)-10\n",
    "        #maxlon=max(lons[0,:].values)+10\n",
    "        #minlat=min(lats[0,:].values)-10\n",
    "        #maxlat=max(lats[0,:].values)+10\n",
    "        \n",
    "        iwrap=0\n",
    "        minlon=min(lons.values)-10\n",
    "        maxlon=max(lons.values)+10\n",
    "        minlat=min(lats.values)-10\n",
    "        maxlat=max(lats.values)+10\n",
    "        if minlon<10 and maxlon>350:  #wrapping around meridion need to cal new min/max lon\n",
    "            minlon=max(lons[lons<180].values)+10\n",
    "            maxlon=min(lons[lons>180].values)-10\n",
    "            iwrap=1 #set flag for wraparound\n",
    "        \n",
    "        #here is a fix for when a storm goes from 350 across 360 to 1 2 longitude\n",
    "#        iwrap=0\n",
    "#        print('first and last!',lons[0,1].values,lons[0,-1].values)\n",
    "#        if abs(min(lons[0,:].values)-max(lons[0,:].values))>180:\n",
    "#            lons1=lons[0,:].values-10>180\n",
    "#            lons2=lons[0,:].values+10<180\n",
    "#            maxlon=min(lons[0,lons1].values-10)\n",
    "#            minlon=max(lons[0,lons2].values+10)\n",
    "#            print('wrapped',minlon,maxlon)\n",
    "#            iwrap=1\n",
    "            #wrap_lons = ((lons+180) % 360) - 180        \n",
    "            #maxlon=max(wrap_lons[0,:].values)+10 #this will find the positive maximum\n",
    "            #minlon=min(wrap_lons[0,:].values)-10\n",
    "            #if minlon<0:\n",
    "            #    maxlon=min(wrap_lons[0,:].values)-10+360\n",
    "            #    minlon=max(wrap_lons[0,:].values)+10\n",
    "\n",
    "        print('min/max lon lat',minlon,maxlon,minlat,maxlat)\n",
    "\n",
    "        ix1=int(round((minlon-dx_offset)/dx))\n",
    "        ix2=int(round((maxlon-dx_offset)/dx))\n",
    "        iy1=int(round((minlat-dy_offset)/dy))\n",
    "        iy2=int(round((maxlat-dy_offset)/dy))\n",
    "        if iy2 > 628:\n",
    "            iy2=628\n",
    "        if iy1 < 1:\n",
    "            iy1=1    \n",
    "        if ix1 < 0:\n",
    "            ix1 = ix1 + 1440\n",
    "        if ix2 < 0:\n",
    "            ix2 = ix2 + 1440\n",
    "        print(minlon,maxlon,minlat,maxlat)\n",
    "        xdim=ix2-ix1\n",
    "        if iwrap==1:  #wraps around so make sure xdim reflects that\n",
    "            xdim=ix1-ix2+1440\n",
    "        ydim=iy2-iy1\n",
    "        \n",
    "        dims=lats.shape\n",
    "        tdim=dims[0]\n",
    "        tem_date=[0]*tdim #print(dysince.values)\n",
    "        for i in range(0,tdim):\n",
    "            tem_date[i]=date_1858+dt.timedelta(days=float(dysince[0,i].values))  #create new time array that can be queried for year etc\n",
    "        minjdy = min(tem_date).timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        minyear =min(tem_date).year #create new time array that can be queried for year etc\n",
    "        maxjdy = max(tem_date).timetuple().tm_yday  #create new time array that can be queried for year etc\n",
    "        maxyear =max(tem_date).year  #create new time array that can be queried for year etc\n",
    "        print(minyear,minjdy,maxyear,maxjdy)\n",
    "        \n",
    "        dif = max(tem_date)-min(tem_date)\n",
    "        tdim=int(dif.days)+30\n",
    "\n",
    "\n",
    "        print(tdim,ix1,ix2,iy1,iy2,xdim,ydim)      \n",
    "               \n",
    "        sst_out_sv= np.zeros([tdim,ydim,xdim], dtype=\"float\")\n",
    "        sst_clim_out_sv= np.zeros([tdim,ydim,xdim], dtype=\"float\")\n",
    "        wndu_out_sv= np.zeros([tdim,ydim,xdim], dtype=\"float\")\n",
    "        wndv_out_sv= np.zeros([tdim,ydim,xdim], dtype=\"float\")\n",
    "        wndu_clim_out_sv= np.zeros([tdim,ydim,xdim], dtype=\"float\")\n",
    "        wndv_clim_out_sv= np.zeros([tdim,ydim,xdim], dtype=\"float\")\n",
    "        \n",
    "        print('sst_out_sv',sst_out_sv.shape)\n",
    "        for i in range(0,tdim):\n",
    "            storm_date = tem_date[0]+dt.timedelta(days=i)\n",
    "            #print(storm_date)\n",
    "            \n",
    "            syr=str(storm_date.year)\n",
    "            smon=str(storm_date.month)\n",
    "            sdym=str(storm_date.day)\n",
    "            sjdy=str(storm_date.timetuple().tm_yday)\n",
    "            \n",
    "            fname_tem=syr + smon.zfill(2) + sdym.zfill(2) + '090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc'\n",
    "            mur_filename = dir_mur + syr + '/' + sjdy.zfill(3) + '/' + fname_tem\n",
    "            \n",
    "            fname_tem='/CCMP_Wind_Analysis_' + syr + smon.zfill(2) + sdym.zfill(2) + '_V02.0_L3.0_RSS.nc'\n",
    "            ccmp_filename = dir_ccmp + syr + '/M' + smon.zfill(2) + fname_tem      \n",
    "\n",
    "            #flux data\n",
    "            lh_flux_filename = dir_flux + 'lh_oaflux_' + syr + '.nc';\n",
    "            sh_flux_filename = dir_flux + 'sh_oaflux_' + syr + '.nc';\n",
    "            ta_flux_filename = dir_flux + 'ta_oaflux_' + syr + '.nc';\n",
    "            qa_flux_filename = dir_flux + 'qa_oaflux_' + syr + '.nc';\n",
    "  #          fname='F:\\data\\model_data\\oaflux\\data_v3\\daily\\radiation_1985-2009\\sw_isccp_2004.nc';\n",
    " #           [nswrs]=ncread(fname,'nswrs',[1 1 idy],[360 180 1]);\n",
    "            \n",
    "            if storm_date.timetuple().tm_yday==366:\n",
    "                sjdy = '365'\n",
    "            clim_filename='F:/data/sst/jpl_mur/v4.1/clim/clim2_' + sjdy.zfill(3) +'_2003_2013_MUR-GLOB-v02.0-fv04.1.nc'\n",
    "            ccmp_clim_filename='F:/data/sat_data/ccmp/v02.0/clim/ccmp_daily_clim_' + sjdy.zfill(3) +'.nc'\n",
    "#            print(ccmp_filename)\n",
    "#            print(ccmp_clim_filename)\n",
    "\n",
    "#ccmp wind            \n",
    "            nc_fid = Dataset(ccmp_filename, 'r')\n",
    "            nc_fid2 = Dataset(ccmp_clim_filename, 'r') \n",
    "            tem = nc_fid.variables['uwnd'][:,iy1:iy2,:]  #read in data all longitude, limited latitude\n",
    "            tem = np.mean(tem,axis=0)                     #take average across all 6 hourly data fields\n",
    "            wndu = np.append(tem[:,ydim:],tem[:,:ydim], axis=1) #switch from 0-360 to -180 to 180  ydim is half of xdim\n",
    "            tem = nc_fid.variables['vwnd'][:,iy1:iy2,:]\n",
    "            tem = np.mean(tem,axis=0)\n",
    "            wndv = np.append(tem[:,ydim:],tem[:,:ydim], axis=1)               \n",
    "            mlat_ccmp = nc_fid.variables['latitude'][iy1:iy2]\n",
    "            tem = nc_fid.variables['longitude'][:]\n",
    "            mlon_ccmp = np.append(tem[ydim:],tem[:ydim], axis=0)  \n",
    "            mlon_save = mlon_ccmp[:]\n",
    "            mlon_ccmp = ((mlon_ccmp - 180) % 360) - 180  #make -180 to 180 rather than 0 360\n",
    "            tem = nc_fid2.variables['av_u'][iy1:iy2,:]\n",
    "            wndu_clim = np.append(tem[:,ydim:],tem[:,:ydim], axis=1)               \n",
    "            tem = nc_fid2.variables['av_v'][iy1:iy2,:]\n",
    "            wndv_clim = np.append(tem[:,ydim:],tem[:,:ydim], axis=1)               \n",
    "            nc_fid.close()\n",
    "            nc_fid2.close()           \n",
    "\n",
    "\n",
    "            #flux data\n",
    "            ds = xr.open_dataset(lh_flux_filename)\n",
    "            ds_subset = ds.sel(time = idyjl)\n",
    "            ds_res = ds_subset.interp(latitude = new_lat,longitude = mlon_save)\n",
    "           # nc_fid4 = Dataset(sh_flux_filename, 'r')\n",
    "           # nc_fid5 = Dataset(ta_flux_filename, 'r')\n",
    "           # nc_fid6 = Dataset(qa_flux_filename, 'r')\n",
    "\n",
    "            #[lhf1]=ncread(fname,'lhtfl',[1 1 idy],[360 180 1]);\n",
    "            #[Tair1]=ncread(fname,'tmp2m',[1 1 idy],[360 180 1]);\n",
    "            #[Qair1]=ncread(fname,'hum2m',[1 1 idy],[360 180 1]);  \n",
    "            #[shf1]=ncread(fname,'shtfl',[1 1 idy],[360 180 1]);\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "#            if i==0:\n",
    "#                print('i=0',iy1,iy2,ix1,ix2,iy2-iy1,ix2-ix1)\n",
    "            if ix1<=1440 and ix2<=1440 and iwrap==0:\n",
    "#                if i==0:\n",
    "#                    print('inside1',iy1,iy2,ix1,ix2)\n",
    "                wndu_out = wndu[:,ix1:ix2]\n",
    "                wndu_clim_out = wndu_clim[:,ix1:ix2]\n",
    "                wndv_out = wndv[:,ix1:ix2]\n",
    "                wndv_clim_out = wndv_clim[:,ix1:ix2]\n",
    "            if ix1>1440 and ix2>1440 and iwrap==0:\n",
    " #               if i==0:\n",
    "#                    print('inside2',iy1,iy2,ix1,ix2)\n",
    "                wndu_out = wndu[:,ix1-1440:ix2-1440]\n",
    "                wndu_clim_out = wndu_clim[:,ix1-1440:ix2-1440]\n",
    "                wndv_out = wndv[:,ix1-1440:ix2-1440]\n",
    "                wndv_clim_out = wndv_clim[:,ix1-1440:ix2-1440]\n",
    "            if ix1<=1440 and ix2>1440 and iwrap==0:\n",
    "#                if i==0:\n",
    "#                    print('inside3',iy1,iy2,ix1,ix2)\n",
    "\n",
    "                tem1 = wndu[:,ix1:]\n",
    "                tem2 = wndu[:,:ix2-1440]\n",
    "                wndu_out = np.append(tem1,tem2, axis=1)\n",
    "                tem1 = wndv[:,ix1:]\n",
    "                tem2 = wndv[:,:ix2-1440]\n",
    "                wndv_out = np.append(tem1,tem2, axis=1)\n",
    "                tem1 = wndu_clim[:,ix1:]\n",
    "                tem2 = wndu_clim[:,:ix2-1440]\n",
    "                wndu_clim_out = np.append(tem1,tem2, axis=1)\n",
    "                tem1 = wndv_clim[:,ix1:]\n",
    "                tem2 = wndv_clim[:,:ix2-1440]\n",
    "                wndv_clim_out = np.append(tem1,tem2, axis=1)\n",
    "\n",
    "            if ix1<=1440 and ix2>1440 and iwrap==1:\n",
    "#                if i==0:\n",
    "#                    print('inside1',iy1,iy2,ix1,ix2)\n",
    "                wndu_out = wndu[:,ix2-1440:ix1]\n",
    "                wndu_clim_out = wndu_clim[:,ix2-1440:ix1]\n",
    "                wndv_out = wndv[:,ix2-1440:ix1]\n",
    "                wndv_clim_out = wndv_clim[:,ix2-1440:ix1]\n",
    "\n",
    "            wndu_out_sv[i,:,:]=wndu_out\n",
    "            wndv_out_sv[i,:,:]=wndv_out\n",
    "            wndu_clim_out_sv[i,:,:]=wndu_clim_out\n",
    "            wndv_clim_out_sv[i,:,:]=wndv_clim_out\n",
    "            \n",
    "#sst data   \n",
    "\n",
    "            nc_fid = Dataset(mur_filename, 'r')\n",
    "            mlat = nc_fid.variables['lat'][1149:16849]\n",
    "            ilat_mur1 = np.argmin(abs(mlat-mlat_ccmp.min()))-12\n",
    "            #print('mlat first point:', mlat[ilat_mur1_tem])\n",
    "            ilat_mur2 = np.argmin(abs(mlat-mlat_ccmp.max()))+13\n",
    "            mlat = mlat[ilat_mur1:ilat_mur2]\n",
    "            sst = nc_fid.variables['analysed_sst'][0,ilat_mur1:ilat_mur2,:]\n",
    "            mlon = nc_fid.variables['lon'][:]\n",
    "            nc_fid.close()\n",
    "\n",
    "            nc_fid2 = Dataset(clim_filename, 'r')                      \n",
    "            sst_clim = nc_fid2.variables['sst'][ilat_mur1:ilat_mur2,:]\n",
    "            nc_fid2.close()\n",
    "                       \n",
    "            coarseness = 25\n",
    "            temp = mlon.reshape((mlon.shape[0] // coarseness, coarseness))\n",
    "            coarse_mlon = np.mean(temp, axis=(1), dtype=np.float64)\n",
    "            temp = mlat.reshape((mlat.shape[0] // coarseness, coarseness))\n",
    "            coarse_mlat = np.mean(temp, axis=(1), dtype=np.float64)\n",
    "            temp = sst.reshape((sst.shape[0] // coarseness, coarseness, sst.shape[1] // coarseness, coarseness))\n",
    "            coarse_sst = np.mean(temp, axis=(1,3), dtype=np.float64)\n",
    "            temp = sst_clim.reshape((sst_clim.shape[0] // coarseness, coarseness, sst_clim.shape[1] // coarseness, coarseness))\n",
    "            coarse_sst_clim = np.mean(temp, axis=(1,3), dtype=np.float64)\n",
    "\n",
    "            #need to recalculate iy1 and iy2 because of offset made earlier to read less of file\n",
    "#            iy1=np.argmin(abs(coarse_mlat-minlat))\n",
    "#            iy2=np.argmin(abs(coarse_mlat-maxlat))\n",
    "#            ydim=iy2-iy1   \n",
    "#            print(coarse_mlat[0],coarse_mlat[-1])\n",
    "#            print(iy1,iy2,ydim,minlat,maxlat)\n",
    "\n",
    "#            if i==0:\n",
    "#                print('i=0',ix1,ix2,iy2-iy1,ix2-ix1)\n",
    "            if ix1<=1440 and ix2<=1440 and iwrap==0:\n",
    "#                if i==0:\n",
    "#                    print('inside1',ix1,ix2)\n",
    "                sst_out = coarse_sst[:,ix1:ix2]\n",
    "                mlat_out = coarse_mlat[:]\n",
    "                mlon_out = coarse_mlon[ix1:ix2]\n",
    "                sst_clim_out = coarse_sst_clim[:,ix1:ix2]\n",
    "            if ix1>1440 and ix2>1440 and iwrap==0:\n",
    "#                if i==0:\n",
    "#                    print('inside2',ix1,ix2)\n",
    "                sst_out = coarse_sst[:,ix1-1440:ix2-1440]\n",
    "                mlat_out = coarse_mlat[:]\n",
    "                mlon_out = coarse_mlon[ix1-1440:ix2-1440]\n",
    "                sst_clim_out = coarse_sst_clim[:,ix1-1440:ix2-1440]\n",
    "            if ix1<=1440 and ix2>1440 and iwrap==0:\n",
    "#                if i==0:\n",
    "#                    print('inside3',ix1,ix2)\n",
    "                tem1 = coarse_sst[:,ix1:]\n",
    "                tem2 = coarse_sst[:,:ix2-1440]\n",
    "                sst_out = np.append(tem1,tem2, axis=1)\n",
    "                mlat_out = coarse_mlat[:]\n",
    "                mlon1 = coarse_mlon[ix1:]\n",
    "                mlon2 = coarse_mlon[:ix2-1440]\n",
    "                print(mlon1.shape,mlon2.shape)\n",
    "                mlon_out = np.append(mlon1,mlon2, axis=0)               \n",
    "                tem1 = coarse_sst_clim[:,ix1:]\n",
    "                tem2 = coarse_sst_clim[:,:ix2-1440]\n",
    "                sst_clim_out = np.append(tem1,tem2, axis=1)\n",
    "\n",
    "            if ix1<=1440 and ix2>1440 and iwrap==1:\n",
    " #               if i==0:\n",
    " #                   print('inside1',ix1,ix2)\n",
    "                sst_out = coarse_sst[:,ix2-1440:ix1]\n",
    "                mlat_out = coarse_mlat[:]\n",
    "                mlon_out = coarse_mlon[ix2-1440:ix1]\n",
    "                sst_clim_out = coarse_sst_clim[:,ix2-1440:ix1]\n",
    "\n",
    "            if i==0:\n",
    "                print('sst',sst_out.shape,'coarse',coarse_sst.shape,wndu_out.shape)\n",
    "                print('sst',sst_out.shape,'sst_sv',sst_out_sv.shape)\n",
    "                print('mlon',mlon_out.shape,'mlat',mlat_out.shape)\n",
    "            #sst_sv[i,:,:]=sst-sst_clim\n",
    "            sst_out_sv[i,:,:]=sst_out\n",
    "            sst_clim_out_sv[i,:,:]=sst_clim_out\n",
    "\n",
    "\n",
    "  \n",
    " \n",
    "            \n",
    "            \n",
    "        ilen=len(fname_in)\n",
    "        \n",
    "        dif_dys=[0]*tdim\n",
    "        for i in range(0,tdim):\n",
    "            dif_dys[i] = i\n",
    "\n",
    "        print('file out:',fname_out)\n",
    "        #f.close()\n",
    "        f = Dataset(fname_out,'w', format='NETCDF4') \n",
    "        tempgrp = f.createGroup('data')\n",
    "        tempgrp.setncattr_string('start time',str(tem_date[0]))\n",
    "        tempgrp.createDimension('t', tdim)\n",
    "        tempgrp.createDimension('y', ydim)\n",
    "        tempgrp.createDimension('x', xdim)\n",
    "\n",
    "    #tem_date[i]\n",
    "        sst_netcdf = tempgrp.createVariable('sst', 'f4', ('t', 'y', 'x'))\n",
    "        sst_clim_netcdf = tempgrp.createVariable('sst_clim', 'f4', ('t', 'y', 'x'))\n",
    "        wndu_netcdf = tempgrp.createVariable('wndu', 'f4', ('t', 'y', 'x'))\n",
    "        wndv_netcdf = tempgrp.createVariable('wndv', 'f4', ('t', 'y', 'x'))\n",
    "        wndu_clim_netcdf = tempgrp.createVariable('wndu_clim', 'f4', ('t', 'y', 'x'))\n",
    "        wndv_clim_netcdf = tempgrp.createVariable('wndv_clim', 'f4', ('t', 'y', 'x'))\n",
    "        longitude = tempgrp.createVariable('lon', 'f4', 'x')\n",
    "        latitude = tempgrp.createVariable('lat', 'f4', 'y')  \n",
    "        time = tempgrp.createVariable('time', 'i4', 't')\n",
    "        \n",
    "        sst_netcdf[:] = sst_out_sv\n",
    "        sst_clim_netcdf[:] = sst_clim_out_sv\n",
    "        wndu_netcdf[:] = wndu_out_sv\n",
    "        wndv_netcdf[:] = wndv_out_sv\n",
    "        wndu_clim_netcdf[:] = wndu_clim_out_sv\n",
    "        wndv_clim_netcdf[:] = wndv_clim_out_sv\n",
    "        latitude[:] = mlat_out\n",
    "        longitude[:] = mlon_out\n",
    "        time[:]=dif_dys\n",
    "        f.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
