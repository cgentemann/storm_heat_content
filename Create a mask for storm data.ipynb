{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input files\n",
    "dir_in='f:/data/tc_wakes/database/info/'\n",
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "dir_mur = 'F:/data/sst/jpl_mur/v4.1/'\n",
    "dir_flux = 'F:/data/model_data/oaflux/data_v3/daily/turbulence/'\n",
    "dir_cmc = 'F:/data/sst/cmc/CMC0.2deg/v2/'\n",
    "#################################################################################\n",
    "#from math import cos, radians\n",
    "from math import sin, pi\n",
    "def closest_dist(ds_in,ds_storm): \n",
    "# calculate distance closest storm point\n",
    "# point given as tla,tlo.... storm is in the program\n",
    "# \n",
    "#point of data\n",
    "#print(tla,tlo)\n",
    "# initialize distances (in degrees)\n",
    "    tdim,xdim,ydim=ds_storm.lat.shape[1], ds_in.analysed_sst[0,:,0].shape[0], ds_in.analysed_sst[0,0,:].shape[0]\n",
    "    dx_save=np.zeros([tdim,xdim,ydim])\n",
    "    print(ds_in.analysed_sst.shape)\n",
    "    dx_grid,dy_grid = np.meshgrid(ds_in.lon.values,ds_in.lat.values)\n",
    "    min_dist_save = np.zeros(ds_in.analysed_sst[0,:,:].shape)*np.nan\n",
    "    min_index_save = np.zeros(ds_in.analysed_sst[0,:,:].shape)*np.nan\n",
    "    #for each location of the storm calculate the difference for all values in box\n",
    "    for i in range(0,ds_storm.lat.shape[1]):  # all storm values\n",
    "        dx = abs(dx_grid - ds_storm.lon[0,i].values)*sin(ds_storm.lat[0,i]*pi/180.)*111\n",
    "        dy = abs(dy_grid - ds_storm.lat[0,i].values)*111\n",
    "        dist_save=np.sqrt(dx*dx+dy*dy)\n",
    "        dx_save[i,:,:]=dist_save\n",
    "    #now go through each value in box and find minimum storm location/day\n",
    "    for j in range(0,ds_in.lon.shape[0]):\n",
    "        for i in range(0,ds_in.lat.shape[0]):\n",
    "            imin = np.argmin(dx_save[:,i,j])\n",
    "            min_dist_save[i,j]=dx_save[imin,i,j]\n",
    "            min_index_save[i,j]=imin\n",
    "    return min_dist_save,min_index_save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ccmp='F:/data/sat_data/ccmp/v02.0/Y'\n",
    "date_1858 = dt.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "dx=0.25\n",
    "dy=0.25\n",
    "dx_offset = -179.875\n",
    "dy_offset = -78.3750\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first read in 1 sst climatology just to get lat/lon\n",
    "#sst climatology\n",
    "filename='F:/data/sst/cmc/CMC0.2deg/v2/climatology/clim1993_2016300-CMC-L4_GHRSST-SSTfnd-CMC0.2deg-GLOB-v02.0-fv02.0.nc'\n",
    "ds_day=xr.open_dataset(filename,drop_variables=['analysis_error','sea_ice_fraction','sq_sst'])\n",
    "ds_day = ds_day.rename({'analysed_sst':'analysed_sst_clim','mask':'mask_clim'}) #, inplace = True)            \n",
    "ds_day = ds_day.where(ds_day['mask_clim'] == 1.) \n",
    "ds_day.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_storm_mask(ds_sst,lats,lons):\n",
    "    iwrap_mask = 0\n",
    "    if (ds_sst.lon.max().values>170) & (ds_sst.lon.min().values<-170):\n",
    "        iwrap_mask=1\n",
    "    print(ds_sst.lon.min().values,ds_sst.lon.max().values)\n",
    "#okay, now ds_storm is array with right lat/lon for storm so create mask now\n",
    "    ds_mask = ds_sst.copy(deep=True)\n",
    "    ds_mask['storm_mask']=ds_mask.analysed_sst*0\n",
    "    ds_mask = ds_mask.fillna(0)\n",
    "    ds_mask['storm_mask'] = ds_mask['storm_mask'].astype(int,copy=True)\n",
    "    for i in range(0,lats.shape[0]):\n",
    "        if lats[i]>0:   #northern hemi on right, southers on left\n",
    "            lons1,lons2=lons[i]-4,lons[i]+10\n",
    "        else:\n",
    "            lons1,lons2=lons[i]-10,lons[i]+4\n",
    "        lats1,lats2=lats[i]-10,lats[i]+10\n",
    "        if i==0:\n",
    "            print('lons1,lons2:',iwrap_mask,lons1.data,lons2.data)\n",
    "        if lons1<-180:\n",
    "            ds_mask['storm_mask'].loc[dict(lon=(ds_mask.lon < lons2) | (ds_mask.lon > lons1+360), lat=slice(lats1,lats2))] = -1\n",
    "        elif lons2>180:\n",
    "#                ds_sst['storm_mask'].loc[dict(lon=(ds_sst.lon < lon2) | (ds_sst.lon > lon1+360), lat=slice(lats1,lats2))] = -1\n",
    "            ds_mask['storm_mask'].loc[dict(lon=(ds_mask.lon < lons2-360) | (ds_mask.lon > lons1), lat=slice(lats1,lats2))] = -1\n",
    "#                ds_sst['storm_mask'].loc[dict(lon=slice(-180,lons2-360), lat=slice(lats1,lats2))] = -1\n",
    "        else:\n",
    "            if iwrap_mask==1:\n",
    "                ds_mask.coords['lon'] = np.mod(ds_mask['lon'], 360)\n",
    "                ds_mask = ds_mask.sortby(ds_mask.lon)\n",
    "                ds_mask['storm_mask'].loc[dict(lon=slice(lons1+360,lons2+360), lat=slice(lats1,lats2))] = -1\n",
    "                ds_mask.coords['lon'] = (ds_mask.coords['lon'] + 180) % 360 - 180\n",
    "            else:\n",
    "                ds_mask['storm_mask'].loc[dict(lon=slice(lons1,lons2), lat=slice(lats1,lats2))] = -1\n",
    "    return ds_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#code to create a mask for all storms\n",
    "icnt = 0\n",
    "for root, dirs, files in os.walk(dir_in, topdown=False):\n",
    "    if root[len(dir_in):len(dir_in)+1]=='.':\n",
    "        continue\n",
    "    for name in files:\n",
    "        if not name.endswith('.nc'):\n",
    "            continue\n",
    "        fname_in=os.path.join(root, name)\n",
    "        print(fname_in[36:39],fname_in[31:35])\n",
    "        inum_storm=int(fname_in[36:39])\n",
    "        iyr_storm=int(fname_in[31:35])\n",
    "        print(name,fname_in)\n",
    "        dsx = xr.open_dataset(fname_in)\n",
    "        lats = dsx.lat[0,:]\n",
    "        lons = dsx.lon[0,:]  #lons goes from 0 to 360\n",
    "        lons = (lons + 180) % 360 - 180\n",
    "        dysince = dsx.time\n",
    "        dsx.close()\n",
    "#read in combined data        \n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "        ds_sst = xr.open_dataset(filename)\n",
    "        ds_sst.close()\n",
    "\n",
    "        ds_mask = calculate_storm_mask(ds_sst,lats,lons)\n",
    "        \n",
    "        ds_mask = ds_mask.drop(['uwnd','vwnd','date','timePlot','dbss_obml','shtfl','lhtfl','tmp2m','hum2m','analysed_sst','mask','analysed_sst_clim','mask_clim'])               \n",
    "\n",
    "#calc distance / time\n",
    "        dist,index = closest_dist(ds_sst,dsx)\n",
    "        dtem=xr.DataArray(dist, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_mask['dist_from_storm_km']=dtem\n",
    "        dtem=xr.DataArray(index, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "        ds_mask['closest_storm_index']=dtem\n",
    "      \n",
    "\n",
    "        filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_mask_data.nc'\n",
    "        ds_mask.to_netcdf(filename)\n",
    "        print('out:',filename)\n",
    "        icnt +=1\n",
    "        if inum_storm==2:\n",
    "            break\n",
    "    break\n",
    "     # filename = dir_out + str(iyr_storm) + '/' + 'str(inum_storm)' + '_other_data.nc'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist,index = closest_dist(ds_sst,dsx)\n",
    "print(dist.shape)\n",
    "print(ds_mask)\n",
    "dtem=xr.DataArray(dist, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "ds_mask['dist_from_storm_km']=dtem\n",
    "dtem=xr.DataArray(index, coords={'lat': ds_mask.lat.values, 'lon':ds_mask.lon.values}, dims=('lat', 'lon'))\n",
    "ds_mask['closest_storm_index']=dtem\n",
    "ds_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mask\n",
    "#ds_mask.storm_mask[-1,:,:].plot()\n",
    "#ds_sst.analysed_sst[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check output data\n",
    "iwrap=0\n",
    "iyr_storm,inum_storm=2002,5\n",
    "filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_combined_data.nc'\n",
    "ds = xr.open_dataset(filename)\n",
    "ds.close()\n",
    "ds_sst = ds.copy(deep=True)   #make a deep copy\n",
    "dsx = xr.open_dataset('F:/data/tc_wakes/database/info/'+str(iyr_storm)+'/'+ str(inum_storm).zfill(3) +'annual_storm_info.nc')\n",
    "lats = dsx.lat[0,:]\n",
    "lons = dsx.lon[0,:]  #lons goes from 0 to 360\n",
    "lons = (lons + 180) % 360 - 180\n",
    "dsx.close()\n",
    "filename = dir_out + str(iyr_storm) + '/' + str(inum_storm).zfill(3) + '_mask_data.nc'\n",
    "ds_mask = xr.open_dataset(filename)\n",
    "ds_mask.close()\n",
    "print(ds_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsx.lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#md,mi,dx= closest_dist(ds,dsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dx_save.shape)\n",
    "plt.pcolormesh(min_index_save)\n",
    "#plt.pcolormesh(dx_grid[:,:])\n",
    "#print(md.shape,md[10,10],md[60,60],md[70,70],md[80,80])\n",
    "#plt.pcolormesh(md,vmin=0,vmax=5000)\n",
    "#print(dx_grid[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(ds.analysed_sst[10,:,:])\n",
    "#plt.pcolormesh(ds.lon,ds.lat,ds.analysed_sst[10,:,:])\n",
    "#plt.plot(lons,lats,'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(ds_mask.storm_mask[10,:,:])\n",
    "#plt.pcolormesh(ds_mask.lon,ds_mask.lat,ds_mask.storm_mask[10,:,:])\n",
    "#plt.plot(lons,lats,'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds.analysed_sst[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_subset.analysed_sst[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_sst['storm_mask']=ds_sst.analysed_sst*0\n",
    "ds_sst['storm_mask'] = ds_sst['storm_mask'].astype(int,copy=True)\n",
    "for i in range(0,lats.shape[0]):\n",
    "    if lats[i]>0:   #northern hemi on right, southers on left\n",
    "        ds_sst['storm_mask'].loc[dict(lon=slice(lons[i]-4,lons[i]+10), lat=slice(lats[i]-2,lats[i]+2))] = -1\n",
    "    else:\n",
    "        ds_sst['storm_mask'].loc[dict(lon=slice(lons[i]-10,lons[i]+4), lat=slice(lats[i]-2,lats[i]+2))] = -1        \n",
    "ds_sst.drop(['uwnd','vwnd','date','timePlot','dbss_obml','shtfl','lhtfl','tmp2m','hum2m','analysed_sst','mask','analysed_sst_clim','mask_clim'])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sst.drop(['uwnd','vwnd','date','timePlot','dbss_obml','shtfl','lhtfl','tmp2m','hum2m','analysed_sst','mask','analysed_sst_clim','mask_clim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.analysed_sst[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
