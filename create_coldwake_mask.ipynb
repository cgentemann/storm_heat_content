{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['PROJ_LIB']='/anaconda3/share/proj'\n",
    "sys.path.append('/Users/sol/Google Drive/professional/research/utilities/python_myfun')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from map_mymod import *\n",
    "import datetime\n",
    "import geopy.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storm to analyze\n",
    "stnyr, stn='2006055','055' \n",
    "#stnyr, stn='2007050','050' \n",
    "# storm track data\n",
    "fn2='/Users/sol/Documents/LargeData/tc_wakes/storms_Data/'+stnyr+'/'+stn+'annual_storm_info.nc'\n",
    "storm=xr.open_dataset(fn2)\n",
    "storm.close()\n",
    "# transform storm time to comparable time to data\n",
    "date_1858 = datetime.datetime(1858,11,17,0,0,0) # start date is 11/17/1958\n",
    "storm_date=[0]*len(storm.time.values[0]) \n",
    "for i in range(len(storm.time.values[0])):\n",
    "    storm_date[i]=date_1858+datetime.timedelta(days=float(storm.time.values[0][i]))  \n",
    "# transform storm lon to -180:180\n",
    "storm.lon[0]=[i-360 for i in storm.lon[0] if i>=180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather data from around the storm\n",
    "# open file\n",
    "fn='/Users/sol/Documents/LargeData/tc_wakes/storms_Data/'+stnyr+'/'+stn+'_combined_data.nc'\n",
    "dtd=xr.open_dataset(fn)\n",
    "dtd.close()\n",
    "#print(dt)\n",
    "#nmask=dt.storm_mask.where(dt.storm_mask<0)*-1\n",
    "nmask=dtd.dist_from_storm_km.where(dtd.dist_from_storm_km<600)\n",
    "#we also want the land mask\n",
    "nmask=nmask*0+1\n",
    "nmask=nmask*dtd.mask[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each point (of data within the storm mask) get data into two structures: sif (all points), adfs (array)\n",
    "tp=0\n",
    "rw=[] # extra info array\n",
    "ar=np.full([10000,len(dtd.time),4],np.nan)\n",
    "for i in range(len(dtd.lat.values)):\n",
    "    for j in range(len(dtd.lon.values)):\n",
    "        if not np.isnan(nmask[i,j]):\n",
    "            #print(i,j)\n",
    "            #plt.plot(dt.lon[j],dt.lat[i],'b.-')\n",
    "            \n",
    "            # lat lon for each point in the mask\n",
    "            tla=dtd.lat[i].values\n",
    "            tlo=dtd.lon[j].values\n",
    "            di=dtd.dist_from_storm_km[i,j].values\n",
    "            # timeseries data (3d: point, t, [sst, u, v])\n",
    "            # all time for data series\n",
    "            ar[tp,:,0]=dtd.analysed_sst[:,i,j].values-273.15\n",
    "            ar[tp,:,1]=dtd.uwnd[:,i,j].values\n",
    "            ar[tp,:,2]=dtd.vwnd[:,i,j].values\n",
    "            ar[tp,:,3]=dtd.dbss_obml[:,i,j].values # maybe redo for previous month\n",
    "            \n",
    "            rw.append([tla,tlo,di])\n",
    "            tp=tp+1\n",
    "\n",
    "#print(tp, len(rw))\n",
    "tp=tp-1 # last index\n",
    "# convert info to dataframe and save\n",
    "cols=['lat','lon','dist_km']\n",
    "sif=pd.DataFrame(rw,columns=cols)\n",
    "#sif.to_csv('../data/storm_sum_data/info_locdist_'+stnyr+'.csv')\n",
    "\n",
    "# convert data to xarray\n",
    "ar=np.delete(ar,range(tp+1,10000),axis=0)\n",
    "dt=xr.DataArray(ar,coords=[range(tp+1),dtd.time,['SST','uwnd','vwnd','mld']],dims=['location','time','var']) \n",
    "#adfs.to_netcdf('../data/storm_sum_data/stormdatamask_'+stnyr+'.nc')\n",
    "dttime=pd.to_datetime(dt.time.values) # datatime array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_grid(lat_point,lon_point,lat_list,lon_list):\n",
    "    # adapted from chelle's routine\n",
    "    #this routine takes a point and finds distance to all points in a grid of lat and lon\n",
    "    dist_grid = np.empty((len(lat_list)))    \n",
    "    coords_1 = (lat_point, lon_point)  \n",
    "    for i in range(len(lat_list)):\n",
    "        coords_2 = (lat_list[i], lon_list[i])  \n",
    "        arclen_temp = geopy.distance.geodesic(coords_1, coords_2).km  #distance in km   \n",
    "        #print(arclen_temp)\n",
    "        dist_grid[i]=arclen_temp\n",
    "    return dist_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_storm_beginend(pdt): # a point in sif\n",
    "    # find begining and end of storm influence\n",
    "    thwnd=5 # threshold of wind speed to consider significant storm\n",
    "    thdist=600 # threshold of distance strom traveled between two day to consider it close\n",
    "    \n",
    "    # calculate speed for all times of pdt\n",
    "    spd=np.sqrt(dt[pdt,:,1]*dt[pdt,:,1]+dt[pdt,:,2]*dt[pdt,:,2])\n",
    "    \n",
    "    # calculate distance from point pdt to each storm track point \n",
    "    nd=get_dist_grid(sif['lat'][pdt],sif['lon'][pdt],storm.lat.values[0],storm.lon.values[0])\n",
    "    \n",
    "    # find min distance from storm to point and time\n",
    "    clod=min(nd) # closest distance to storm\n",
    "    a=np.where(nd==clod)\n",
    "    b=a[0][0] # index of closest distance on storm track\n",
    "    ndi=np.abs(dttime - storm_date[b]).argmin() # find index of time on data where storm is closest\n",
    "    \n",
    "    # choose closest as first day of strom to begin\n",
    "    begs=ndi # index on data for closest point to storm\n",
    "    tb=b # Storm time index of closest\n",
    "    # look for the days before to see if storm arrived earlier (strong winds and close storm)\n",
    "    \n",
    "    # find distance to storm at day before  \n",
    "    if tb>2: # only days when storm closest distance is later than the first few storm points\n",
    "        if spd[begs]<=thwnd: # wind speed of closest is below threshold end routine\n",
    "            ok=0 # end search for beginning and do not search for end\n",
    "            ends=begs\n",
    "        else: # look for begining day before \n",
    "            ok=0\n",
    "            while ok==0:\n",
    "                if tb>2: # there is a previous storm day\n",
    "                    if spd[begs-1]<=thwnd: # if previous wind is below threshold end routine\n",
    "                        # the day begs is the real begining, but we have found the begining\n",
    "                        ok=1 \n",
    "                    else: # check distance \n",
    "                        # calculate distance to storm at begs-1\n",
    "                        # fist calculate storm time closest to begs-1\n",
    "                        #tb=np.abs(np.array(storm_date)-dttime[begs-1]).argmin() \n",
    "                        #if np.logical_and(b-tb>3,tb>2): # making sure that the begs-1 is still a storm day (and a real day before the closest)\n",
    "                        if tb-4>0:\n",
    "                            dts=nd[tb-4] # distance to storm at begs-1 time\n",
    "                            if dts>=thdist: # if storm was too far the day before, then wind is probably not due to the storm\n",
    "                                ok=1 # leave begs as the begining of storm\n",
    "                            else: # if the storm was close the day before, keep lookin\n",
    "                                begs=begs-1\n",
    "                                tb=tb-4\n",
    "                        else: # no room to go back\n",
    "                            ok=1\n",
    "                else: # there is no prveious strom date\n",
    "                    ok=1\n",
    "                    \n",
    "                \n",
    "    else: # point is to close to the begining of the strom - begin assumed the closest day\n",
    "        ok=1 # look for end of storm\n",
    "    \n",
    "    if ok==1: # check for the end of the storm \n",
    "        \n",
    "        # end of event when storm ends\n",
    "        endstorm=np.abs(dttime - storm_date[len(storm_date)-1]).argmin()\n",
    "        ok=0\n",
    "        ends=begs+1 # end of strom influence\n",
    "        while np.logical_and(ok==0,ends<=endstorm):\n",
    "            if nd[ends]>thdist: # Storm too far away\n",
    "                ok=1\n",
    "            else:\n",
    "                if spd[ends]<thwnd:\n",
    "                    ok=1\n",
    "                else:\n",
    "                    ends=ends+1\n",
    "        \n",
    "    #print(nd[tb],spd[begs])\n",
    " \n",
    "    return begs, ends, clod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cold_wake(pn,nidx, nfdx):\n",
    "    # find and quantify cold wake\n",
    "    # pass point, day (index) of storm arrival, and end of wind forcing (storm)\n",
    "    # returns: dcwi0 (beg of coldwake),lencw (lenght), sst0 (base temp), mxdT (max cooling), mxdTi (day of max cool, relative to cold wake beg)\n",
    "    # returns also: mld (mix layer depth at begining of storm)\n",
    "    \n",
    "    # within the 2 next days (realistic?)\n",
    "    # needs fixing to consider increases in wind\n",
    "    cow=0 # is there a cold wake? 0 = no\n",
    "    dcwi0=np.nan\n",
    "    lencw=nfdx-nidx\n",
    "    mxdT=np.nan\n",
    "    try: \n",
    "        sst0=np.mean(dt[pn,nidx-1-4:nidx-1,0])\n",
    "    except:\n",
    "        sst0=np.mean(dt[pn,:nidx-1,0])\n",
    "\n",
    "    \n",
    "    # test if lenght of wind event is longer than 0... if not, no coldwake to calculate\n",
    "    if nfdx-nidx>0:\n",
    "        # find if there is really a cold wake. look for 5 days\n",
    "        of=0\n",
    "        ok=0\n",
    "        while np.logical_and(ok==0, of<6):\n",
    "            # is there a change in temp before begining of storm and previous day\n",
    "            ct=dt[pn,nidx+of,0]-sst0 # nidx first day of \"cooling\" (the day the storm begins)\n",
    "            # is there a cold wake?\n",
    "            if ct > 0: # no decrease in temp, go for the next day, recalculate SST0\n",
    "                of=of+1\n",
    "                try: \n",
    "                    sst0=np.mean(dt[pn,nidx+of-1-4:nidx+of-1,0])\n",
    "                except:\n",
    "                    sst0=np.mean(dt[pn,:nidx+of-1,0])\n",
    "            else:  # decrease, so start counting... but only if change is low\n",
    "                #print('Cold wake') # cooling start the day after the storm begins (nidx+1)\n",
    "                cow=1\n",
    "                ok=1\n",
    "\n",
    "        # if coldwake\n",
    "        if cow==1:\n",
    "            #print(of)\n",
    "\n",
    "            # for the next days, calculate (#days), cumulative SSTa (from SST0), and max SSTa until the diff is < ?? 0.1\n",
    "            dcwi0=nidx+of # first day of cold wake (for now same as arriving o f storm)\n",
    "            dcwi=nidx+of # index to count days of cold wake\n",
    "            lencw=1 # length of cold wake\n",
    "            mxdT=dt[pn,dcwi,0]-sst0 # max change in temp during the wake\n",
    "\n",
    "            # first day doesn't get evaluated (already a change) \n",
    "            dcwi=dcwi+1\n",
    "            ok=0\n",
    "            mxok=0\n",
    "            while np.logical_and(ok==0,lencw<15):  \n",
    "                # \"anomaly stil larger than 0.1C, or the cw last less than 10days)\n",
    "                if dt[pn,dcwi,0]<sst0-0.1:\n",
    "                    # if temp keeps decreasing check for max change\n",
    "                    if np.logical_and(dt[pn,dcwi,0]<dt[pn,dcwi-1,0],mxok==0):\n",
    "                        if dt[pn,dcwi,0]-sst0<mxdT: # is today colder than the min SST from before?\n",
    "                            mxdT=dt[pn,dcwi,0]-sst0 # min SST \n",
    "                        else:\n",
    "                            mxok=1 # look for further for mx change\n",
    "                    dcwi=dcwi+1\n",
    "                    lencw=lencw+1\n",
    "                else: # end of cold wake\n",
    "                    ok=1\n",
    "                    # length remain the same\n",
    "        \n",
    "    return cow,dcwi0,lencw,sst0, mxdT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cold wake and create mask\n",
    "plat=[]\n",
    "plon=[]\n",
    "pxdT=[] # max cooling\n",
    "psst0=[] # initial (base) temperature\n",
    "\n",
    "for pn in range(len(sif)):\n",
    "    # calculate wind speed\n",
    "    spd=np.sqrt(dt[pn,:,1]*dt[pn,:,1]+dt[pn,:,2]*dt[pn,:,2])\n",
    "    # find actual arrival of storm date and wind at the time\n",
    "    tw0,twf, clod=find_storm_beginend(pn)\n",
    "    w0=spd[tw0]\n",
    "   \n",
    "    ## closest latitude in storm to point\n",
    "    clolat=np.abs(storm['lat'][0].values - sif['lat'][pn]).argmin()\n",
    "    # also get rid of points to close to the begining of the storm (8 points ~ 2 days)\n",
    "    if np.logical_or(storm['lon'][0].values[clolat]-sif['lon'][pn]>1,clolat<9): ## !!! review this for southern hemisphere\n",
    "        # no cold wake if more than 1 degree to the left of the storm \n",
    "        cow=0\n",
    "    else:\n",
    "        # is there a cold wake? and quantify\n",
    "        cow, tcw0, lcw, sst0, xdT = cold_wake(pn,tw0,twf)    \n",
    "    \n",
    "    if cow==1:\n",
    "        if np.logical_and(xdT<=-1,lcw>5): # threshold for cold wake\n",
    "            pxdT.append(xdT) # max cooling\n",
    "            psst0.append(sst0)\n",
    "            \n",
    "            # from storm info file\n",
    "            plat.append(sif['lat'][pn])\n",
    "            plon.append(sif['lon'][pn])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mask\n",
    "coldwake=np.full((len(dtd.lat.values),len(dtd.lon.values)),np.nan)\n",
    "coldwake_mask=np.full((len(dtd.lat.values),len(dtd.lon.values)),0.0)\n",
    "for i in range(len(plat)):\n",
    "    a=np.where(dtd.lat.values==plat[i])\n",
    "    b=np.where(dtd.lon.values==plon[i])\n",
    "    coldwake[a[0][0],b[0][0]]=pxdT[i]\n",
    "    coldwake_mask[a[0][0],b[0][0]]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=plt.cm.get_cmap('seismic')\n",
    "plt.figure(figsize=(14,6))\n",
    "x,y=np.meshgrid(dtd.lon,dtd.lat)\n",
    "plt.subplot(121)\n",
    "plt.scatter(x,y,c=coldwake,s=15, alpha=0.8, cmap=cm)\n",
    "cb=plt.colorbar()\n",
    "cb.set_label('Max Cooling')\n",
    "plt.xlabel('Lon')\n",
    "plt.ylabel('Lat')\n",
    "plt.scatter(storm.lon.values,storm.lat.values,c='k',s=20, marker='*')\n",
    "\n",
    "cm=plt.cm.get_cmap('seismic')\n",
    "plt.subplot(122)\n",
    "plt.scatter(x,y,c=coldwake_mask,s=15, alpha=0.8, cmap=cm)\n",
    "cb=plt.colorbar()\n",
    "cb.set_label('Mask')\n",
    "plt.xlabel('Lon')\n",
    "plt.ylabel('Lat')\n",
    "plt.scatter(storm.lon.values,storm.lat.values,c='k',s=20, marker='*')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
