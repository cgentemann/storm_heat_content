{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from netCDF4 import Dataset  # http://code.google.com/p/netcdf4-python/\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import xarray as xr\n",
    "#from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input files\n",
    "dir_in='f:/data/tc_wakes/database/info/'\n",
    "dir_out='f:/data/tc_wakes/database/sst/'\n",
    "dir_figs='f:/data/tc_wakes/database/coldwake_images/'\n",
    "#################################################################################\n",
    "#from math import cos, radians\n",
    "import geopy.distance\n",
    "from math import sin, pi\n",
    "from scipy import interpolate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_grid(lat_point,lon_point,lat_list,lon_list):\n",
    "    # adapted from chelle's routine\n",
    "    #this routine takes a point and finds distance to all points in a grid of lat and lon\n",
    "    dist_grid = np.empty((len(lat_list)))    \n",
    "    coords_1 = (lat_point, lon_point)  \n",
    "    for i in range(len(lat_list)):\n",
    "        coords_2 = (lat_list[i], lon_list[i])  \n",
    "        arclen_temp = geopy.distance.geodesic(coords_1, coords_2).km  #distance in km   \n",
    "        #print(arclen_temp)\n",
    "        dist_grid[i]=arclen_temp\n",
    "    return dist_grid\n",
    "\n",
    "\n",
    "def find_storm_beginend(pdt,dt,dttime,sif,storm,storm_date): # a point in sif\n",
    "    # find begining and end of storm influence\n",
    "    thwnd=5 # threshold of wind speed to consider significant storm\n",
    "    thdist=600 # threshold of distance strom traveled between two day to consider it close\n",
    "    \n",
    "    # calculate speed for all times of pdt\n",
    "    spd=np.sqrt(dt[pdt,:,1]*dt[pdt,:,1]+dt[pdt,:,2]*dt[pdt,:,2])\n",
    "    \n",
    "    # calculate distance from point pdt to each storm track point \n",
    "    nd=get_dist_grid(sif['lat'][pdt],sif['lon'][pdt],storm.lat.values[0],storm.lon.values[0])\n",
    "    \n",
    "    # find min distance from storm to point and time\n",
    "    clod=min(nd) # closest distance to storm\n",
    "    a=np.where(nd==clod)\n",
    "    b=a[0][0] # index of closest distance on storm track\n",
    "    ndi=np.abs(dttime - storm_date[b]).argmin() # find index of time on data where storm is closest\n",
    "    \n",
    "    # choose closest as first day of strom to begin\n",
    "    begs=ndi # index on data for closest point to storm\n",
    "    tb=b # Storm time index of closest\n",
    "    # look for the days before to see if storm arrived earlier (strong winds and close storm)\n",
    "    \n",
    "    # find distance to storm at day before  \n",
    "    if tb>2: # only days when storm closest distance is later than the first few storm points\n",
    "        if spd[begs]<=thwnd: # wind speed of closest is below threshold end routine\n",
    "            ok=0 # end search for beginning and do not search for end\n",
    "            ends=begs\n",
    "        else: # look for begining day before \n",
    "            ok=0\n",
    "            while ok==0:\n",
    "                if tb>2: # there is a previous storm day\n",
    "                    if spd[begs-1]<=thwnd: # if previous wind is below threshold end routine\n",
    "                        # the day begs is the real begining, but we have found the begining\n",
    "                        ok=1 \n",
    "                    else: # check distance \n",
    "                        # calculate distance to storm at begs-1\n",
    "                        # fist calculate storm time closest to begs-1\n",
    "                        #tb=np.abs(np.array(storm_date)-dttime[begs-1]).argmin() \n",
    "                        #if np.logical_and(b-tb>3,tb>2): # making sure that the begs-1 is still a storm day (and a real day before the closest)\n",
    "                        if tb-4>0:\n",
    "                            dts=nd[tb-4] # distance to storm at begs-1 time\n",
    "                            if dts>=thdist: # if storm was too far the day before, then wind is probably not due to the storm\n",
    "                                ok=1 # leave begs as the begining of storm\n",
    "                            else: # if the storm was close the day before, keep lookin\n",
    "                                begs=begs-1\n",
    "                                tb=tb-4\n",
    "                        else: # no room to go back\n",
    "                            ok=1\n",
    "                else: # there is no prveious strom date\n",
    "                    ok=1\n",
    "                    \n",
    "                \n",
    "    else: # point is to close to the begining of the strom - begin assumed the closest day\n",
    "        ok=1 # look for end of storm\n",
    "    \n",
    "    if ok==1: # check for the end of the storm \n",
    "        \n",
    "        # end of event when storm ends\n",
    "        endstorm=np.abs(dttime - storm_date[len(storm_date)-1]).argmin()\n",
    "        ok=0\n",
    "        ends=begs+1 # end of strom influence\n",
    "        while np.logical_and(ok==0,ends<=endstorm):\n",
    "            if nd[ends]>thdist: # Storm too far away\n",
    "                ok=1\n",
    "            else:\n",
    "                if spd[ends]<thwnd:\n",
    "                    ok=1\n",
    "                else:\n",
    "                    ends=ends+1\n",
    "        \n",
    "    #print(nd[tb],spd[begs])\n",
    " \n",
    "    return begs, ends, clod\n",
    "\n",
    "\n",
    "def cold_wake(pn,dt,nidx,nfdx):\n",
    "    # find and quantify cold wake\n",
    "    # pass point, day (index) of storm arrival, and end of wind forcing (storm)\n",
    "    # returns: dcwi0 (beg of coldwake),lencw (lenght), sst0 (base temp), mxdT (max cooling), mxdTi (day of max cool, relative to cold wake beg)\n",
    "    # returns also: mld (mix layer depth at begining of storm)\n",
    "    \n",
    "    # within the 2 next days (realistic?)\n",
    "    # needs fixing to consider increases in wind\n",
    "    cow=0 # is there a cold wake? 0 = no\n",
    "    dcwi0=np.nan\n",
    "    lencw=nfdx-nidx\n",
    "    mxdT=np.nan\n",
    "    try: \n",
    "        sst0=np.mean(dt[pn,nidx-1-4:nidx-1,0])\n",
    "    except:\n",
    "        sst0=np.mean(dt[pn,:nidx-1,0])\n",
    "\n",
    "    \n",
    "    # test if lenght of wind event is longer than 0... if not, no coldwake to calculate\n",
    "    if nfdx-nidx>0:\n",
    "        # find if there is really a cold wake. look for 5 days\n",
    "        of=0\n",
    "        ok=0\n",
    "        while np.logical_and(ok==0, of<6):\n",
    "            # is there a change in temp before begining of storm and previous day\n",
    "            ct=dt[pn,nidx+of,0]-sst0 # nidx first day of \"cooling\" (the day the storm begins)\n",
    "            # is there a cold wake?\n",
    "            if ct > 0: # no decrease in temp, go for the next day, recalculate SST0\n",
    "                of=of+1\n",
    "                try: \n",
    "                    sst0=np.mean(dt[pn,nidx+of-1-4:nidx+of-1,0])\n",
    "                except:\n",
    "                    sst0=np.mean(dt[pn,:nidx+of-1,0])\n",
    "            else:  # decrease, so start counting... but only if change is low\n",
    "                #print('Cold wake') # cooling start the day after the storm begins (nidx+1)\n",
    "                cow=1\n",
    "                ok=1\n",
    "\n",
    "        # if coldwake\n",
    "        if cow==1:\n",
    "            #print(of)\n",
    "\n",
    "            # for the next days, calculate (#days), cumulative SSTa (from SST0), and max SSTa until the diff is < ?? 0.1\n",
    "            dcwi0=nidx+of # first day of cold wake (for now same as arriving o f storm)\n",
    "            dcwi=nidx+of # index to count days of cold wake\n",
    "            lencw=1 # length of cold wake\n",
    "#            print('1:',pn,dcwi,dt[pn,dcwi,0])\n",
    "            mxdT=dt[pn,dcwi,0]-sst0 # max change in temp during the wake\n",
    "\n",
    "            # first day doesn't get evaluated (already a change) \n",
    "            dcwi=dcwi+1\n",
    "            ok=0\n",
    "            mxok=0\n",
    "            while np.logical_and(ok==0,lencw<15):  \n",
    "                # \"anomaly stil larger than 0.1C, or the cw last less than 10days)\n",
    "#                print(dt.dims)\n",
    "#                print(dt.shape)\n",
    "#                print(pn)\n",
    "#                print(dcwi)\n",
    "#                print(sst0)\n",
    "#                print(sst0.values)\n",
    "#                print(dt[pn,dcwi,0])\n",
    "                if dt[pn,dcwi,0]<(sst0.values-0.1):\n",
    "#                    print('here')\n",
    "                    # if temp keeps decreasing check for max change\n",
    "                    if np.logical_and(dt[pn,dcwi,0]<dt[pn,dcwi-1,0],mxok==0):\n",
    "                        if dt[pn,dcwi,0]-sst0<mxdT: # is today colder than the min SST from before?\n",
    "                            mxdT=dt[pn,dcwi,0]-sst0 # min SST \n",
    "                        else:\n",
    "                            mxok=1 # look for further for mx change\n",
    "                    dcwi=dcwi+1\n",
    "                    lencw=lencw+1\n",
    "                else: # end of cold wake\n",
    "                    ok=1\n",
    "                    # length remain the same\n",
    "        \n",
    "    return cow,dcwi0,lencw,sst0, mxdT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entire_storm_wake(iyr,inum_storm):\n",
    "    stnyr = str(iyr)\n",
    "    stn = str(inum_storm).zfill(3)\n",
    "    fn2=dir_in+stnyr+'/'+stn+'annual_storm_info.nc'\n",
    "    storm=xr.open_dataset(fn2)\n",
    "    storm.close()\n",
    "    # transform storm time to comparable time to data\n",
    "    date_1858 = datetime.datetime(1858,11,17) # start date is 11/17/1958\n",
    "    storm_date=[0]*len(storm.time.values[0]) \n",
    "    for i in range(len(storm.time.values[0])):\n",
    "        storm_date[i]=date_1858+datetime.timedelta(days=float(storm.time.values[0][i]))  \n",
    "    # transform storm lon to -180:180\n",
    "    storm['lon'] = (storm['lon'] + 180) % 360 - 180\n",
    "#CG    storm.lon[0]=[i-360 for i in storm.lon[0] if i>=180]\n",
    "    fn=dir_out +stnyr+'/'+stn+'_combined_data.nc'\n",
    "    dtd=xr.open_dataset(fn)\n",
    "    dtd.close()\n",
    "    #print(dt)\n",
    "    #nmask=dt.storm_mask.where(dt.storm_mask<0)*-1\n",
    "    nmask=dtd.dist_from_storm_km.where(dtd.dist_from_storm_km<600)\n",
    "    #we also want the land mask\n",
    "    nmask=nmask*0+1\n",
    "    nmask=nmask*dtd.mask[0,:,:]\n",
    "    # for each point (of data within the storm mask) get data into two structures: sif (all points), adfs (array)\n",
    "    tp=0\n",
    "    rw=[] # extra info array\n",
    "    ar=np.full([50000,len(dtd.time),4],np.nan)\n",
    "    for i in range(len(dtd.lat.values)):\n",
    "        for j in range(len(dtd.lon.values)):\n",
    "            if not np.isnan(nmask[i,j]):\n",
    "                #print(i,j)\n",
    "                #plt.plot(dt.lon[j],dt.lat[i],'b.-')\n",
    "\n",
    "                # lat lon for each point in the mask\n",
    "                tla=dtd.lat[i].values\n",
    "                tlo=dtd.lon[j].values\n",
    "                di=dtd.dist_from_storm_km[i,j].values\n",
    "                # timeseries data (3d: point, t, [sst, u, v])\n",
    "                # all time for data series\n",
    "                ar[tp,:,0]=dtd.analysed_sst[:,i,j].values-273.15\n",
    "                ar[tp,:,1]=dtd.uwnd[:,i,j].values\n",
    "                ar[tp,:,2]=dtd.vwnd[:,i,j].values\n",
    "                ar[tp,:,3]=dtd.dbss_obml[:,i,j].values # maybe redo for previous month\n",
    "\n",
    "                rw.append([tla,tlo,di])\n",
    "                tp=tp+1\n",
    "\n",
    "    #print(tp, len(rw))\n",
    "    tp=tp-1 # last index\n",
    "    # convert info to dataframe and save\n",
    "    cols=['lat','lon','dist_km']\n",
    "    sif=pd.DataFrame(rw,columns=cols)\n",
    "    #sif.to_csv('../data/storm_sum_data/info_locdist_'+stnyr+'.csv')\n",
    "\n",
    "    # convert data to xarray\n",
    "    ar=np.delete(ar,range(tp+1,50000),axis=0)\n",
    "    dt=xr.DataArray(ar,coords=[range(tp+1),dtd.time,['SST','uwnd','vwnd','mld']],dims=['location','time','var']) \n",
    "    #adfs.to_netcdf('../data/storm_sum_data/stormdatamask_'+stnyr+'.nc')\n",
    "    dttime=pd.to_datetime(dt.time.values) # datatime array\n",
    "    # calculate cold wake and create mask\n",
    "    plat=[]\n",
    "    plon=[]\n",
    "    pxdT=[] # max cooling\n",
    "    psst0=[] # initial (base) temperature\n",
    "\n",
    "    for pn in range(len(sif)):\n",
    "        # calculate wind speed\n",
    "        spd=np.sqrt(dt[pn,:,1]*dt[pn,:,1]+dt[pn,:,2]*dt[pn,:,2])\n",
    "        # find actual arrival of storm date and wind at the time\n",
    "        tw0,twf, clod=find_storm_beginend(pn,dt,dttime,sif,storm,storm_date)\n",
    "        w0=spd[tw0]\n",
    "\n",
    "        ## closest latitude in storm to point\n",
    "        clolat=np.abs(storm['lat'][0].values - sif['lat'][pn]).argmin()\n",
    "        # also get rid of points to close to the begining of the storm (8 points ~ 2 days)\n",
    "        if (storm.lat[0,0].data>0) and (np.logical_or(storm['lon'][0].values[clolat]-sif['lon'][pn]>1,clolat<9)): ## !!! review this for southern hemisphere\n",
    "            # no cold wake if more than 1 degree to the left of the storm \n",
    "            cow=0\n",
    "        elif (storm.lat[0,0].data<0) and (np.logical_or(storm['lon'][0].values[clolat]-sif['lon'][pn]>1,clolat<9)): ## !!! review this for southern hemisphere\n",
    "            # no cold wake if more than 1 degree to the left of the storm \n",
    "            cow=0\n",
    "        else:\n",
    "            # is there a cold wake? and quantify\n",
    "            cow, tcw0, lcw, sst0, xdT = cold_wake(pn,dt,tw0,twf)    \n",
    "\n",
    "        if cow==1:\n",
    "            if np.logical_and(xdT<=-1,lcw>5): # threshold for cold wake\n",
    "                pxdT.append(xdT) # max cooling\n",
    "                psst0.append(sst0)\n",
    "\n",
    "                # from storm info file\n",
    "                plat.append(sif['lat'][pn])\n",
    "                plon.append(sif['lon'][pn])\n",
    "    # create a mask\n",
    "    coldwake=np.full((len(dtd.lat.values),len(dtd.lon.values)),np.nan)\n",
    "    coldwake_mask=np.full((len(dtd.lat.values),len(dtd.lon.values)),0.0)\n",
    "    prestorm_sst=np.full((len(dtd.lat.values),len(dtd.lon.values)),nan)\n",
    "    for i in range(len(plat)):\n",
    "        a=np.where(dtd.lat.values==plat[i])\n",
    "        b=np.where(dtd.lon.values==plon[i])\n",
    "        coldwake[a[0][0],b[0][0]]=pxdT[i]\n",
    "        coldwake_mask[a[0][0],b[0][0]]=1\n",
    "        prestorm_sst[a[0][0],b[0][0]]=psst0[i]\n",
    "    return dtd,storm,coldwake,coldwake_mask,prestorm_sst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for root, dirs, files in os.walk(dir_in, topdown=False):\n",
    "    if root[len(dir_in):len(dir_in)+1]=='.':\n",
    "        continue\n",
    "    for name in files:\n",
    "        if not name.endswith('.nc'):\n",
    "            continue\n",
    "        fname_in=os.path.join(root, name)\n",
    "        print(fname_in[36:39],fname_in[31:35])\n",
    "        inum_storm=int(fname_in[36:39])\n",
    "        iyr_storm=int(fname_in[31:35])\n",
    "\n",
    "        print('processing:',iyr_storm,inum_storm)\n",
    "        dtd,storm,coldwake,coldwake_mask,prestorm_sst = entire_storm_wake(iyr_storm,inum_storm)\n",
    "        print('make figure')\n",
    "        cm=plt.cm.get_cmap('seismic')\n",
    "        plt.figure(figsize=(14,4))\n",
    "        x,y=np.meshgrid(dtd.lon,dtd.lat)\n",
    "        plt.subplot(131)\n",
    "        plt.scatter(x,y,c=coldwake,s=15, alpha=0.8, cmap=cm,vmin=-3,vmax=3)\n",
    "        cb=plt.colorbar()\n",
    "        cb.set_label('Max Cooling')\n",
    "        plt.xlabel('Lon')\n",
    "        plt.ylabel('Lat')\n",
    "        plt.scatter(storm.lon.values,storm.lat.values,c='k',s=20, marker='*')\n",
    "\n",
    "        cm=plt.cm.get_cmap('seismic')\n",
    "        plt.subplot(132)\n",
    "        plt.scatter(x,y,c=coldwake_mask,s=15, alpha=0.8, cmap=cm)\n",
    "        cb=plt.colorbar()\n",
    "        cb.set_label('Mask')\n",
    "        plt.xlabel('Lon')\n",
    "        plt.ylabel('Lat')\n",
    "        plt.scatter(storm.lon.values,storm.lat.values,c='k',s=20, marker='*')\n",
    "\n",
    "        cm=plt.cm.get_cmap('seismic')\n",
    "        plt.subplot(133)\n",
    "        plt.pcolormesh(dtd.lon,dtd.lat,(dtd.analysed_sst[10,:,:]-dtd.analysed_sst[0,:,:]),vmin=-2,vmax=2,cmap='seismic')\n",
    "        plt.plot(storm.lon[0,:],storm.lat[0,:],'k')\n",
    "        cb=plt.colorbar()\n",
    "        cb.set_label('Mask')\n",
    "        plt.xlabel('Lon')\n",
    "        plt.ylabel('Lat')\n",
    "        plt.scatter(storm.lon.values,storm.lat.values,c='k',s=20, marker='*')\n",
    "        fname = dir_figs+str(iyr_storm)+str(inum_storm)+'_coldwake.png'\n",
    "        plt.savefig(fname, dpi=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iyr,inum_storm = 2002,7\n",
    "dtd,storm,coldwake,coldwake_mask = entire_storm_wake(iyr,inum_storm)\n",
    "\n",
    "cm=plt.cm.get_cmap('seismic')\n",
    "plt.figure(figsize=(14,4))\n",
    "x,y=np.meshgrid(dtd.lon,dtd.lat)\n",
    "plt.subplot(131)\n",
    "plt.pcolormesh(x,y,c=coldwake,s=15, alpha=0.8, cmap=cm,vmin=-3,vmax=3)\n",
    "cb=plt.colorbar()\n",
    "cb.set_label('Max Cooling')\n",
    "plt.xlabel('Lon')\n",
    "plt.ylabel('Lat')\n",
    "plt.scatter(storm.lon.values,storm.lat.values,c='k',s=20, marker='*')\n",
    "\n",
    "cm=plt.cm.get_cmap('seismic')\n",
    "plt.subplot(132)\n",
    "plt.scatter(x,y,c=coldwake_mask,s=15, alpha=0.8, cmap=cm)\n",
    "cb=plt.colorbar()\n",
    "cb.set_label('Mask')\n",
    "plt.xlabel('Lon')\n",
    "plt.ylabel('Lat')\n",
    "plt.scatter(storm.lon.values,storm.lat.values,c='k',s=20, marker='*')\n",
    "\n",
    "cm=plt.cm.get_cmap('seismic')\n",
    "plt.subplot(133)\n",
    "plt.pcolormesh(dtd.lon,dtd.lat,(dtd.analysed_sst[10,:,:]-dtd.analysed_sst[0,:,:]),vmin=-2,vmax=2,cmap='seismic')\n",
    "plt.plot(storm.lon[0,:],storm.lat[0,:],'k')\n",
    "cb=plt.colorbar()\n",
    "cb.set_label('Mask')\n",
    "plt.xlabel('Lon')\n",
    "plt.ylabel('Lat')\n",
    "plt.scatter(storm.lon.values,storm.lat.values,c='k',s=20, marker='*')\n",
    "fname = dir_figs+str(iyr)+str(inum_storm)+'_coldwake.png'\n",
    "plt.savefig(fname, dpi=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=plt.cm.get_cmap('seismic')\n",
    "plt.figure(figsize=(14,4))\n",
    "x,y=np.meshgrid(dtd.lon,dtd.lat)\n",
    "plt.subplot(131)\n",
    "plt.pcolormesh(x,y,coldwake, cmap=cm,vmin=-3,vmax=3)\n",
    "cb=plt.colorbar()\n",
    "cb.set_label('Max Cooling')\n",
    "plt.xlabel('Lon')\n",
    "plt.ylabel('Lat')\n",
    "plt.scatter(storm.lon.values,storm.lat.values,c='k',s=20, marker='*')\n",
    "\n",
    "cm=plt.cm.get_cmap('seismic')\n",
    "plt.subplot(132)\n",
    "plt.scatter(x,y,c=coldwake_mask,s=15, alpha=0.8, cmap=cm)\n",
    "cb=plt.colorbar()\n",
    "cb.set_label('Mask')\n",
    "plt.xlabel('Lon')\n",
    "plt.ylabel('Lat')\n",
    "plt.scatter(storm.lon.values,storm.lat.values,c='k',s=20, marker='*')\n",
    "\n",
    "cm=plt.cm.get_cmap('seismic')\n",
    "plt.subplot(133)\n",
    "plt.pcolormesh(dtd.lon,dtd.lat,(dtd.analysed_sst[15,:,:]-dtd.analysed_sst[0,:,:]),vmin=-2,vmax=2,cmap='seismic')\n",
    "plt.plot(storm.lon[0,:],storm.lat[0,:],'k')\n",
    "cb=plt.colorbar()\n",
    "cb.set_label('Mask')\n",
    "plt.xlabel('Lon')\n",
    "plt.ylabel('Lat')\n",
    "plt.scatter(storm.lon.values,storm.lat.values,c='k',s=20, marker='*')\n",
    "fname = dir_figs+str(iyr)+str(inum_storm)+'_coldwake.png'\n",
    "plt.savefig(fname, dpi=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=plt.cm.get_cmap('seismic')\n",
    "#plt.subplot(1)\n",
    "plt.pcolormesh(dtd.lon,dtd.lat,(dtd.analysed_sst[15,:,:]-dtd.analysed_sst[0,:,:]),vmin=-2,vmax=2,cmap='seismic')\n",
    "plt.plot(storm.lon[0,:],storm.lat[0,:],'y')\n",
    "cb=plt.colorbar()\n",
    "cb.set_label('Mask')\n",
    "plt.xlabel('Lon')\n",
    "plt.ylabel('Lat')\n",
    "plt.scatter(storm.lon.values,storm.lat.values,c='k',s=20, marker='*')\n",
    "plt.plot(dtd.lon[47],dtd.lat[62],'y*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_1858 = datetime.datetime(1858,11,17) # start date is 11/17/1958\n",
    "#    for i in range(len(storm.time.values[0])):\n",
    "#        storm_date[i]=date_1858+datetime.timedelta(days=float(storm.time.values[0][i]))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ilat,ilon = 62,47\n",
    "print(coldwake_mask[ilat,ilon])\n",
    "print(dtd.closest_storm_index[ilat,ilon].values)\n",
    "print(date_1858+datetime.timedelta(days=float(dtd.closest_storm_time[ilat,ilon].values)))\n",
    "plt.plot(dtd.time,2*(dtd.analysed_sst[0,ilat,ilon]-dtd.analysed_sst[:,ilat,ilon]))\n",
    "plt.plot(dtd.time,np.sqrt(dtd.uwnd[:,ilat,ilon]**2+dtd.vwnd[:,ilat,ilon]**2))\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ilat,ilon = 55,45\n",
    "print(coldwake_mask[ilat,ilon])\n",
    "print(dtd.closest_storm_index[ilat,ilon].values)\n",
    "print(date_1858+datetime.timedelta(days=float(dtd.closest_storm_time[ilat,ilon].values)))\n",
    "plt.plot(dtd.time,2*(dtd.analysed_sst[0,ilat,ilon]-dtd.analysed_sst[:,ilat,ilon]))\n",
    "plt.plot(dtd.time,np.sqrt(dtd.uwnd[:,ilat,ilon]**2+dtd.vwnd[:,ilat,ilon]**2))\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime \n",
    "iyr,inum_storm=2002,1\n",
    "stnyr = str(iyr)\n",
    "stn = str(inum_storm).zfill(3)\n",
    "fn2=dir_in+stnyr+'/'+stn+'annual_storm_info.nc'\n",
    "storm=xr.open_dataset(fn2)\n",
    "#storm.drop('j2')\n",
    "storm.close()\n",
    "# transform storm time to comparable time to data\n",
    "date_1858 = datetime.datetime(1858,11,17) # start date is 11/17/1958\n",
    "storm_date=[0]*len(storm.time.values[0]) \n",
    "for i in range(len(storm.time.values[0])):\n",
    "    storm_date[i]=date_1858+datetime.timedelta(days=float(storm.time.values[0][i]))  \n",
    "# transform storm lon to -180:180\n",
    "#storm['lon'] = (storm['lon'] + 180) % 360 - 180\n",
    "#print(pd.to_datetime(storm.time[0,0]) - pd.datetime(1858,11,17))\n",
    "print(storm.time[0,0])\n",
    "print(datetime.timedelta(days=float(storm.time.values[0,0])))\n",
    "#print((storm.time[0] - datetime.datetime(1858,11,17).days))\n",
    "#print(storm.time)\n",
    "#print(np.datetime64('1858-11-17'),np.timedelta64(1,'D'))\n",
    "#print(np.datetime64('1858-11-17')/np.timedelta64(1,'D'))\n",
    "#sdate=(storm.time[0,:].values)-np.datetime64('1858-11-17')/np.timedelta64(1,'D')\n",
    "#sdate2=datetime.utcfromtimestamp(tem2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.timedelta64(52207, 'D')-np.datetime64('1858-11-17'))\n",
    "print(np.datetime64('1858-11-17'))\n",
    "print(np.timedelta64(storm.time[0,0].values,'D'))\n",
    "x = np.timedelta64(storm.time[0,0].values*(24*60*60*1e-9), 'ns')\n",
    "days = x.astype('timedelta64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inum_storm,iyr_storm = 9,2015\n",
    "#read in data\n",
    "stnyr = str(iyr_storm)\n",
    "stn = str(inum_storm).zfill(3)\n",
    "filename=dir_in+stnyr+'/'+stn+'annual_storm_info.nc'\n",
    "storm=xr.open_dataset(filename)\n",
    "storm.close()\n",
    "# transform storm time to comparable time to data\n",
    "date_1858 = datetime.datetime(1858,11,17) # start date is 11/17/1958\n",
    "storm_date=[0]*len(storm.time.values[0]) \n",
    "for i in range(len(storm.time.values[0])):\n",
    "    storm_date[i]=date_1858+datetime.timedelta(days=float(storm.time.values[0][i]))  \n",
    "# transform storm lon to -180:180\n",
    "storm['lon'] = (storm['lon'] + 180) % 360 - 180\n",
    "#CG    storm.lon[0]=[i-360 for i in storm.lon[0] if i>=180]\n",
    "filename=dir_out +stnyr+'/'+stn+'_combined_data.nc'\n",
    "ds_data=xr.open_dataset(filename)\n",
    "ds_data.close()\n",
    "\n",
    "#mask data\n",
    "tdim,xdim,ydim = len(ds_data.time),len(ds_data.lon),len(ds_data.lat)\n",
    "#create dummy variable dist to make distance from storm into same array size as all others in ds_data\n",
    "dist=ds_data.uwnd.copy(deep=True)\n",
    "for i in range(0,tdim):\n",
    "    dist[i,:,:]=ds_data.dist_from_storm_km\n",
    "ds_data['dist_from_storm_km_array']=dist\n",
    "\n",
    "#find max sst 5 days before storm location\n",
    "#first create an array with the storm crossover time (from nearest point) as an array\n",
    "sdate = np.empty([ydim,xdim], dtype=datetime.datetime)    \n",
    "for i in range(0,xdim):\n",
    "    for j in range(0,ydim):\n",
    "        tem=date_1858+datetime.timedelta(days=float(ds_data.closest_storm_time[j,i])) \n",
    "        sdate[j,i]=np.datetime64(tem)\n",
    "xsdate=xr.DataArray(sdate, coords={'lat': ds_data.lat.values, 'lon':ds_data.lon.values}, dims=('lat', 'lon'))        \n",
    "#now use array of storm time to calculate prestorm sst\n",
    "sst0 = ds_data.dist_from_storm_km.copy(deep=True)\n",
    "for i in range(0,xdim):\n",
    "    for j in range(0,ydim):\n",
    "        #sst0[j,i] = ds_data.analysed_sst[:,j,i].interp(time=xsdate[j,i])\n",
    "        sst0[j,i] = ds_data.analysed_sst[:,j,i].sel(time=slice(xsdate[j,i]-np.timedelta64(5,'D'),xsdate[j,i])).max()\n",
    "ds_data['sst_prestorm']=sst0\n",
    "\n",
    "#is storm in southern or northen hemisphere?  ihem = 0 north ihem = 1 south\n",
    "ihemisphere = 0\n",
    "if ds_data.lat.max()<0:\n",
    "    ihemisphere=1\n",
    "if (storm.lat.max()-storm.lat.min())<(storm.lon.max()-storm.lon.min()):\n",
    "    #look at each longitude, go from minimum to 0 to find dist < 100km\n",
    "    dsmin = ds_data.dist_from_storm_km_array.argmin(dim='lat', skipna=True)\n",
    "    for ilon in range(0,xdim):\n",
    "        if ihemisphere==1:\n",
    "            for j in range(dsmin[0,ilon].data,0,-1):\n",
    "                if ds_data.dist_from_storm_km_array[0,j,ilon]>100:  #only keep 100km on one side of storm\n",
    "                    ds_data.mask[:,:j,ilon]=2\n",
    "                    break\n",
    "        else:\n",
    "            for j in range(dsmin[0,ilon].data,ydim):\n",
    "                if ds_data.dist_from_storm_km_array[0,j,ilon]>100:  #only keep 100km on one side of storm\n",
    "                    ds_data.mask[:,j:,ilon]=2\n",
    "                    break\n",
    "else:\n",
    "    #look at each longitude, go from minimum to 0 to find dist < 100km\n",
    "    dsmin = ds_data.dist_from_storm_km_array.argmin(dim='lon', skipna=True)\n",
    "    for ilat in range(0,ydim):\n",
    "        if ihemisphere==0:\n",
    "            for i in range(dsmin[0,ilat].data,0,-1):\n",
    "                if ds_data.dist_from_storm_km_array[0,ilat,i]>100:  #only keep 100km on one side of storm\n",
    "                    ds_data.mask[:,ilat,:i]=2\n",
    "                    break\n",
    "        else:\n",
    "            for i in range(dsmin[0,ilat].data,xdim):\n",
    "                if ds_data.dist_from_storm_km_array[0,ilat,i]>100:  #only keep 100km on one side of storm\n",
    "                    ds_data.mask[:,ilat,i:]=2\n",
    "                    break\n",
    "#now mask storm by land and distance from storm\n",
    "ds_masked = ds_data.where((np.round(ds_data.mask)==1) & (ds_data.dist_from_storm_km_array<600))\n",
    "ds_masked['spd']=np.sqrt(ds_masked.uwnd**2+ds_masked.vwnd**2)\n",
    "ds_data['spd']=np.sqrt(ds_data.uwnd**2+ds_data.vwnd**2)\n",
    "ds_data['coldwake_sst']=ds_data.sst_prestorm-ds_data.analysed_sst\n",
    "ds_masked['coldwake_sst']=ds_masked.sst_prestorm-ds_masked.analysed_sst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(131)\n",
    "ds_masked.coldwake_sst.min(dim='time').plot(vmin=-2,vmax=2,cmap='seismic')\n",
    "plt.plot(storm.lon[0,:],storm.lat[0,:],'k')\n",
    "plt.subplot(132)\n",
    "ds_masked.spd.max(dim='time').plot(vmin=0,vmax=15,cmap='jet')\n",
    "plt.plot(storm.lon[0,:],storm.lat[0,:],'k')\n",
    "plt.subplot(133)\n",
    "ds_data.spd.max(dim='time').plot(vmin=0,vmax=15,cmap='jet')\n",
    "plt.plot(storm.lon[0,:],storm.lat[0,:],'k')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_storm_path(dsx):\n",
    "    #after calculating the distance from the storm it became clear that the storm data is every 6 hours, no matter \n",
    "    #how much it may have moved.  So if the storm moved 300 km in 6 hr, when calculating the distance to the storm\n",
    "    #there were points on the storm track that showed large distances because of the separation to the 6hrly storm points\n",
    "    #this subroutine interpolates the storm path onto a higher spatial resolution\n",
    "    #the new storm dataset is carefully put into an identical format with i2 and j2 as dims to match the old format\n",
    "    ynew = []\n",
    "    tnew = []\n",
    "    xnew = []\n",
    "    dsx['lon'] = (dsx.lon-180) % 360 - 180 #put -180 to 180\n",
    "    for istep in range(1,dsx.lon.shape[1]):\n",
    "        dif_lat = dsx.lat[0,istep]-dsx.lat[0,istep-1]\n",
    "        dif_lon = dsx.lon[0,istep]-dsx.lon[0,istep-1]\n",
    "        x,y,t = dsx.lon[0,istep-1:istep+1].values,dsx.lat[0,istep-1:istep+1].values,dsx.time[0,istep-1:istep+1].values\n",
    "        x1,y1,t1 = dsx.lon[0,istep-1:istep].values,dsx.lat[0,istep-1:istep].values,dsx.time[0,istep-1:istep].values\n",
    "        if abs(dif_lat)>abs(dif_lon):\n",
    "            isign = np.sign(dif_lat)\n",
    "            if abs(dif_lat)>0.75:\n",
    "                ynew1 = np.arange(y[0], y[-1], isign.data*0.75)\n",
    "                f = interpolate.interp1d(y, x, assume_sorted=False)\n",
    "                xnew1 = f(ynew1)\n",
    "                f = interpolate.interp1d(y, t, assume_sorted=False)\n",
    "                tnew1 = f(ynew1)\n",
    "            else:\n",
    "                xnew1,ynew1,tnew1 = x1,y1,t1\n",
    "            xnew,ynew,tnew = np.append(xnew,xnew1),np.append(ynew,ynew1),np.append(tnew,tnew1) \n",
    "        else:\n",
    "            isign = np.sign(dif_lon)\n",
    "            if abs(dif_lon)>0.75:\n",
    "                iwrap_interp = 1\n",
    "                if (x[0]<-90) & (x[-1]>90):\n",
    "                    iwrap_interp = -1\n",
    "                    x[0]=x[0]+360\n",
    "                if (x[0]>90) & (x[-1]<-90):\n",
    "                    iwrap_interp = -1\n",
    "                    x[-1]=x[-1]+360\n",
    "                xnew1 = np.arange(x[0], x[-1], iwrap_interp*isign.data*0.75)\n",
    "                f = interpolate.interp1d(x, y, assume_sorted=False)\n",
    "                ynew1 = f(xnew1)\n",
    "                f = interpolate.interp1d(x, t, assume_sorted=False)\n",
    "                tnew1 = f(xnew1)\n",
    "                xnew1 = (xnew1 - 180) % 360 - 180 #put -180 to 180\n",
    "            else:\n",
    "                xnew1,ynew1,tnew1 = x1,y1,t1\n",
    "            xnew,ynew,tnew = np.append(xnew,xnew1),np.append(ynew,ynew1),np.append(tnew,tnew1) \n",
    "#remove any repeated points\n",
    "    ilen=xnew.size\n",
    "    outputx,outputy,outputt=[],[],[]\n",
    "    for i in range(ilen-1):\n",
    "        if (xnew[i]==xnew[i+1]) and (ynew[i]==ynew[i+1]):\n",
    "            continue\n",
    "        else:\n",
    "            outputx,outputy,outputt = np.append(outputx,xnew[i]),np.append(outputy,ynew[i]),np.append(outputt,tnew[i])\n",
    "    xnew,ynew,tnew=outputx,outputy,outputt\n",
    "#put into xarray\n",
    "    i2,j2=xnew.shape[0],1\n",
    "    tem = np.expand_dims(xnew, axis=0)\n",
    "    xx = xr.DataArray(tem.T,dims=['i2','j2'])\n",
    "    tem = np.expand_dims(ynew, axis=0)\n",
    "    yy = xr.DataArray(tem.T,dims=['i2','j2'])\n",
    "    tem = np.expand_dims(tnew, axis=0)\n",
    "    tt = xr.DataArray(tem.T,dims=['i2','j2'])\n",
    "    dsx_new = xr.Dataset({'lon':xx.T,'lat':yy.T,'time':tt.T})\n",
    "    return dsx_new\n",
    "\n",
    "def closest_dist(ds_in,ds_storm): \n",
    "# m.garcia-reyes 2.4.2019, edited c.gentemann 2.4.2019\n",
    "# calculate distance closest storm point\n",
    "# point given as tla,tlo.... storm is in the program\n",
    "# initialize distances (in km)\n",
    "    ds_storm['lon'] = (ds_storm.lon + 180) % 360 - 180\n",
    "    dsx_input = ds_storm.copy(deep=True)\n",
    "    ds_storm_new = interpolate_storm_path(dsx_input)       \n",
    "    tdim,xdim,ydim=ds_storm_new.lat.shape[1], ds_in.analysed_sst[0,:,0].shape[0], ds_in.analysed_sst[0,0,:].shape[0]\n",
    "    dx_save=np.zeros([tdim,xdim,ydim])\n",
    "    dx_grid,dy_grid = np.meshgrid(ds_in.lon.values,ds_in.lat.values)\n",
    "    lon_grid,lat_grid = np.meshgrid(ds_in.lon.values,ds_in.lat.values)\n",
    "    min_dist_save = np.zeros([xdim,ydim])*np.nan\n",
    "    min_index_save = np.zeros([xdim,ydim])*np.nan\n",
    "    min_time_save = np.zeros([xdim,ydim])*np.nan\n",
    "\n",
    "    position = np.zeros([xdim,ydim])*np.nan\n",
    "    #for each location of the storm calculate the difference for all values in box\n",
    "    for ipt in range(0,ds_storm_new.lat.shape[1]):  # all storm values\n",
    "        dist_tem_grid = get_dist_grid(ds_storm_new.lat[0,ipt].values,ds_storm_new.lon[0,ipt].values,lat_grid,lon_grid)\n",
    "        dx_save[ipt,:,:]=dist_tem_grid       \n",
    "    #now go through each value in box and find minimum storm location/day\n",
    "    for j in range(0,ds_in.lon.shape[0]):\n",
    "        for i in range(0,ds_in.lat.shape[0]):\n",
    "            imin = np.argmin(dx_save[:,i,j])\n",
    "            min_dist_save[i,j]=dx_save[imin,i,j]\n",
    "            min_index_save[i,j]=imin\n",
    "            min_time_save[i,j]=ds_storm_new.time[0,imin]\n",
    "            i1,i2=imin,imin+1\n",
    "            if i2>=ds_storm_new.lat.shape[1]:\n",
    "                i1,i2=imin-1,imin\n",
    "            lonx,laty=ds_in.lon[j],ds_in.lat[i]\n",
    "    #                sign((Bx - Ax) * (Y - Ay) - (By - Ay) * (X - Ax))\n",
    "            position[i,j] = np.sign((ds_storm_new.lon[0,i2] - ds_storm_new.lon[0,i1]) * (laty - ds_storm_new.lat[0,i1]) \n",
    "                                 - (ds_storm_new.lat[0,i2] - ds_storm_new.lat[0,i1]) * (lonx - ds_storm_new.lon[0,i1]))\n",
    "\n",
    "    return min_dist_save,min_index_save,min_time_save,position\n",
    "\n",
    "min_dist_save,min_index_save,min_time_save,position = closest_dist(ds_data,storm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_storm = storm.copy(deep=True)\n",
    "ds_in = ds_data.copy(deep=True)\n",
    "\n",
    "ds_storm['lon'] = (ds_storm.lon + 180) % 360 - 180\n",
    "dsx_input = ds_storm.copy(deep=True)\n",
    "ds_storm_new = interpolate_storm_path(dsx_input)       \n",
    "tdim,xdim,ydim=ds_storm_new.lat.shape[1], ds_in.analysed_sst[0,:,0].shape[0], ds_in.analysed_sst[0,0,:].shape[0]\n",
    "dx_save=np.zeros([tdim,xdim,ydim])\n",
    "dx_grid,dy_grid = np.meshgrid(ds_in.lon.values,ds_in.lat.values)\n",
    "lon_grid,lat_grid = np.meshgrid(ds_in.lon.values,ds_in.lat.values)\n",
    "min_dist_save = np.zeros([xdim,ydim])*np.nan\n",
    "min_index_save = np.zeros([xdim,ydim])*np.nan\n",
    "min_time_save = np.zeros([xdim,ydim])*np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1,i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(ds_in.lon,ds_in.lat,position,vmin=-1,vmax=1)\n",
    "plt.colorbar()\n",
    "plt.plot(ds_storm_new.lon[0,:],ds_storm_new.lat[0,:],'r',linewidth=2)\n",
    "plt.plot(ds_storm_new.lon[0,0],ds_storm_new.lat[0,0],'w*')\n",
    "i,j=50,50\n",
    "plt.plot(ds_in.lon[j],ds_in.lat[i],'w*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j=50,50\n",
    "imin = np.argmin(dx_save[:,i,j])\n",
    "min_dist_save[i,j]=dx_save[imin,i,j]\n",
    "min_index_save[i,j]=imin\n",
    "min_time_save[i,j]=ds_storm_new.time[0,imin]\n",
    "i1,i2=imin,imin+1\n",
    "if i2>=ds_storm_new.lat.shape[1]:\n",
    "    i1,i2=imin-1,imin\n",
    "lonx,laty=ds_in.lon[j],ds_in.lat[i]\n",
    "#                sign((Bx - Ax) * (Y - Ay) - (By - Ay) * (X - Ax))\n",
    "position[i,j] = np.sign((ds_storm_new.lon[0,i2] - ds_storm_new.lon[0,i1]) * (laty - ds_storm_new.lat[0,i1]) \n",
    "                     - (ds_storm_new.lat[0,i2] - ds_storm_new.lat[0,i1]) * (lonx - ds_storm_new.lon[0,i1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dx_save.shape)\n",
    "i,j=50,50\n",
    "imin = np.argmin(dx_save[:,i,j])\n",
    "min_dist_save[i,j]=dx_save[imin,i,j]\n",
    "min_index_save[i,j]=imin\n",
    "print(imin)\n",
    "min_time_save[i,j]=ds_storm_new.time[0,imin]\n",
    "i1,i2=imin,imin+1\n",
    "if i2>ds_storm_new.lat.shape[1]:\n",
    "    i1,i2=imin-1,imin\n",
    "lonx,laty=ds_in.lon[j],ds_in.lat[i]\n",
    "#                sign((Bx - Ax) * (Y - Ay) - (By - Ay) * (X - Ax))\n",
    "print(lonx.values,laty.values)\n",
    "position[i,j] = np.sign((ds_storm_new.lon[0,i2] - ds_storm_new.lon[0,i1]) * (laty - ds_storm_new.lat[0,i1]) \n",
    "                     - (ds_storm_new.lat[0,i2] - ds_storm_new.lat[0,i1]) * (lonx - ds_storm_new.lon[0,i1]))\n",
    "print(position[i,j])\n",
    "print(i1,i2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_storm_new.lon[0,i1-1:i2+2].values)\n",
    "print(ds_storm_new.time[0,i1-1:i2+2].values)\n",
    "print(ds_storm_new.lat[0,i1-1:i2+2].values)\n",
    "plt.plot(ds_storm_new.lon[0,i1-1:i2+2],ds_storm_new.lat[0,i1-1:i2+2],'k.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_storm_new.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_storm.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " \n",
    "dsx = ds_storm.copy(deep=True)\n",
    "ynew = []\n",
    "tnew = []\n",
    "xnew = []\n",
    "dsx['lon'] = (dsx.lon-180) % 360 - 180 #put -180 to 180\n",
    "for istep in range(1,dsx.lon.shape[1]):\n",
    "    dif_lat = dsx.lat[0,istep]-dsx.lat[0,istep-1]\n",
    "    dif_lon = dsx.lon[0,istep]-dsx.lon[0,istep-1]\n",
    "    x,y,t = dsx.lon[0,istep-1:istep+1].values,dsx.lat[0,istep-1:istep+1].values,dsx.time[0,istep-1:istep+1].values\n",
    "    x1,y1,t1 = dsx.lon[0,istep-1:istep].values,dsx.lat[0,istep-1:istep].values,dsx.time[0,istep-1:istep].values\n",
    "    if abs(dif_lat)>abs(dif_lon):\n",
    "        isign = np.sign(dif_lat)\n",
    "        if abs(dif_lat)>0.75:\n",
    "            ynew1 = np.arange(y[0], y[-1], isign.data*0.75)\n",
    "            f = interpolate.interp1d(y, x, assume_sorted=False)\n",
    "            xnew1 = f(ynew1)\n",
    "            f = interpolate.interp1d(y, t, assume_sorted=False)\n",
    "            tnew1 = f(ynew1)\n",
    "#            print('1:',tnew1,xnew1,ynew1)\n",
    "        else:\n",
    "            xnew1,ynew1,tnew1 = x,y,t\n",
    "#            xnew1,ynew1,tnew1 = x1,y1,t1\n",
    "#            print('2:',tnew1,xnew1,ynew1)\n",
    "        xnew,ynew,tnew = np.append(xnew,xnew1),np.append(ynew,ynew1),np.append(tnew,tnew1) \n",
    "    else:\n",
    "        isign = np.sign(dif_lon)\n",
    "        if abs(dif_lon)>0.75:\n",
    "            iwrap_interp = 1\n",
    "            if (x[0]<-90) & (x[-1]>90):\n",
    "                iwrap_interp = -1\n",
    "                x[0]=x[0]+360\n",
    "            if (x[0]>90) & (x[-1]<-90):\n",
    "                iwrap_interp = -1\n",
    "                x[-1]=x[-1]+360\n",
    "            xnew1 = np.arange(x[0], x[-1], iwrap_interp*isign.data*0.75)\n",
    "            f = interpolate.interp1d(x, y, assume_sorted=False)\n",
    "            ynew1 = f(xnew1)\n",
    "            f = interpolate.interp1d(x, t, assume_sorted=False)\n",
    "            tnew1 = f(xnew1)\n",
    "            xnew1 = (xnew1 - 180) % 360 - 180 #put -180 to 180\n",
    "#            print('3:',tnew1,xnew1,ynew1)\n",
    "        else:\n",
    "#            xnew1,ynew1,tnew1 = x1,y1,t1\n",
    "            xnew1,ynew1,tnew1 = x,y,t\n",
    "#            print('4:',tnew1,xnew1,ynew1)\n",
    "        xnew,ynew,tnew = np.append(xnew,xnew1),np.append(ynew,ynew1),np.append(tnew,tnew1) \n",
    "print('1',tnew)\n",
    "\n",
    "print('2',tnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#################################################################################\n",
    "\n",
    "inum_storm,iyr_storm = 9,2015\n",
    "#read in data\n",
    "stnyr = str(iyr_storm)\n",
    "stn = str(inum_storm).zfill(3)\n",
    "filename=dir_in+stnyr+'/'+stn+'annual_storm_info.nc'\n",
    "storm=xr.open_dataset(filename)\n",
    "storm.close()\n",
    "# transform storm time to comparable time to data\n",
    "date_1858 = datetime.datetime(1858,11,17) # start date is 11/17/1958\n",
    "storm_date=[0]*len(storm.time.values[0]) \n",
    "for i in range(len(storm.time.values[0])):\n",
    "    storm_date[i]=date_1858+datetime.timedelta(days=float(storm.time.values[0][i]))  \n",
    "# transform storm lon to -180:180\n",
    "storm['lon'] = (storm['lon'] + 180) % 360 - 180\n",
    "#CG    storm.lon[0]=[i-360 for i in storm.lon[0] if i>=180]\n",
    "filename=dir_out +stnyr+'/'+stn+'_combined_data.nc'\n",
    "ds_data=xr.open_dataset(filename)\n",
    "ds_data.close()\n",
    "ds_data['spd']=np.sqrt(ds_data.uwnd**2+ds_data.vwnd**2)\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(131)\n",
    "(ds_data.analysed_sst_clim[0,:,:]-ds_data.analysed_sst.min(dim='time')).plot(vmin=-2,vmax=2,cmap='seismic')\n",
    "plt.plot(storm.lon[0,:],storm.lat[0,:],'k')\n",
    "plt.subplot(132)\n",
    "ds_data.spd.max(dim='time').plot(vmin=0,vmax=15,cmap='jet')\n",
    "plt.plot(storm.lon[0,:],storm.lat[0,:],'k')\n",
    "plt.subplot(133)\n",
    "#ds_data.spd.max(dim='time').plot(vmin=0,vmax=15,cmap='jet')\n",
    "#plt.plot(storm.lon[0,:],storm.lat[0,:],'k')\n",
    "\n",
    "#how to calculate if a point to the right or left of a line\n",
    "#M(X,Y) query point\n",
    "#A first point B second point along line\n",
    "#for i in range(0,len(storm.time.values[0])-1):\n",
    "#    position = sign((storm.lon[0,i+1] - storm.lon[0,i]) * (Y - storm.lat[0,i]) - (storm.lat[0,i+1] - storm.lat[0,i]) * (X - storm.lon[0,i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsdate\n",
    "#ds_data\n",
    "#sdate\n",
    "#ds_data.interp(time=xsdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ds_data.analysed_sst.dims)\n",
    "#print(xsdate.dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ds_masked.coldwake_sst.min(dim='time'),ds_masked.spd.max(dim='time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_data.analysed_sst_clim[0,:,:]-ds_data.analysed_sst[15,:,:]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #ds_masked2[:,:jsv,ilon]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_data.mask[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_data.uwnd[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
